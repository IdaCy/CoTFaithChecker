{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/CoTFaithChecker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/root/CoTFaithChecker'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ../..\n",
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded annotations for 200 questions\n",
      "Have mean vectors for 11 categories\n",
      "Direction vector norm: 10.0625\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No sentence labelled 'backtracking' in QUESTION_ID 68.\nAvailable categories for this CoT: ['answer_reporting', 'forward_planning', 'knowledge_augmentation', 'logical_deduction', 'option_elimination', 'option_restating', 'problem_restating'].\n→ Choose a different SOURCE_CATEGORY or pick another QUESTION_ID.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 136\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     available \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m(cat_map\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo sentence labelled \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSOURCE_CATEGORY\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in QUESTION_ID \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mQUESTION_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable categories for this CoT: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m→ Choose a different SOURCE_CATEGORY or pick another QUESTION_ID.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m     )\n\u001b[1;32m    142\u001b[0m source_index \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# convert to 0‑based\u001b[39;00m\n\u001b[1;32m    143\u001b[0m prefix_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(all_sentences[:source_index])\n",
      "\u001b[0;31mValueError\u001b[0m: No sentence labelled 'backtracking' in QUESTION_ID 68.\nAvailable categories for this CoT: ['answer_reporting', 'forward_planning', 'knowledge_augmentation', 'logical_deduction', 'option_elimination', 'option_restating', 'problem_restating'].\n→ Choose a different SOURCE_CATEGORY or pick another QUESTION_ID."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\"\"\"Cot-Faithfulness – Sentence-level steering notebook (v2)\n",
    "\n",
    "This notebook lets you steer the model from one sentence category to another\n",
    "by adding a scaled direction vector (α·(v_target − v_source)) to the penultimate\n",
    "hidden state during generation.\n",
    "\n",
    "**Update:** now reports a friendly message if the chosen SOURCE_CATEGORY is not\n",
    "present in the selected QUESTION_ID and shows which categories *are* there, so\n",
    "you can pick sensible parameters without crashing.\n",
    "\"\"\"\n",
    "\n",
    "# %% [markdown] CONFIG ─────────────────────────────────────────────────────────\n",
    "# Adjust anything in this cell and re‑run it to change the experiment.\n",
    "\n",
    "MODEL_NAME        = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"  # HF repo or path\n",
    "VECTORS_JSON      = \"c_cluster_analysis/outputs/hints/mmlu/DeepSeek-R1-Distill-Llama-8B/cat_probe/none_unverb_5001.json\"\n",
    "ANNOTATIONS_JSON  = \"c_cluster_analysis/outputs/hints/mmlu/DeepSeek-R1-Distill-Llama-8B/confidence/sycophancy_unverb_5001.json\"\n",
    "COMPLETIONS_JSON  = \"data/mmlu/DeepSeek-R1-Distill-Llama-8B/none/completions_with_5001.json\"\n",
    "\n",
    "SOURCE_CATEGORY   = \"backtracking\"          # steer *from* this category\n",
    "TARGET_CATEGORY   = \"forward_planning\"      # steer *to*   this category\n",
    "ALPHAS            = [0.0, 0.3, 0.6, 1.0]    # scaling factors (include 0!)\n",
    "MAX_NEW_TOKENS    = 64                      # decoding budget for the steered sentence\n",
    "QUESTION_ID       = 68                      # which question / CoT to run (*must* exist)\n",
    "\n",
    "DEVICE            = \"cuda\" if __import__(\"torch\").cuda.is_available() else \"cpu\"\n",
    "DTYPE             = \"bfloat16\"              # matches the checkpoints you used for probing\n",
    "\n",
    "print(\"Running on\", DEVICE)\n",
    "\n",
    "# %%\n",
    "# Imports & helpers ───────────────────────────────────────────────────────────\n",
    "\n",
    "import json, os, re, logging, torch\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "\n",
    "def _split_sentences(text: str) -> List[str]:\n",
    "    \"\"\"Exact same splitter that was used during vector extraction.\"\"\"\n",
    "    start = text.find(\"<think>\")\n",
    "    if start != -1:\n",
    "        text = text[start + len(\"<think>\") :]\n",
    "    end = text.find(\"</think>\")\n",
    "    if end != -1:\n",
    "        text = text[:end]\n",
    "    text = re.sub(r\"\\s+\", \" \", text.strip())\n",
    "    parts = [p.strip() for p in re.split(r\"(?<=\\.)\\s+\", text) if p.strip()]\n",
    "    merged, i = [], 0\n",
    "    while i < len(parts):\n",
    "        if re.fullmatch(r\"\\d+\\.\", parts[i]) and i + 1 < len(parts):\n",
    "            merged.append(f\"{parts[i]} {parts[i + 1]}\")\n",
    "            i += 2\n",
    "        else:\n",
    "            merged.append(parts[i])\n",
    "            i += 1\n",
    "    return merged\n",
    "\n",
    "# %%\n",
    "# 1.  Load annotations  →  sentence‑id → category mapping for each question\n",
    "\n",
    "with open(ANNOTATIONS_JSON) as f:\n",
    "    ann_records = json.load(f)\n",
    "\n",
    "cat_by_qid: Dict[int, Dict[int, str]] = {}\n",
    "for rec in ann_records:\n",
    "    qid = rec[\"question_id\"]\n",
    "    sid2cat = {}\n",
    "    for ann in rec[\"annotations\"]:\n",
    "        sid = ann[\"sentence_id\"]\n",
    "        numeric_fields = {\n",
    "            k: v\n",
    "            for k, v in ann.items()\n",
    "            if isinstance(v, (int, float)) and k not in {\"other\", \"other_label\", \"sentence_id\"}\n",
    "        }\n",
    "        if not numeric_fields:\n",
    "            continue\n",
    "        best_cat = max(numeric_fields.items(), key=lambda kv: kv[1])[0]\n",
    "        sid2cat[sid] = best_cat\n",
    "    cat_by_qid[qid] = sid2cat\n",
    "\n",
    "print(\"Loaded annotations for\", len(cat_by_qid), \"questions\")\n",
    "\n",
    "# %%\n",
    "# 2.  Load sentence vectors  →  category → list[tensor]\n",
    "\n",
    "with open(VECTORS_JSON) as f:\n",
    "    vec_records = json.load(f)\n",
    "\n",
    "vectors_by_cat: Dict[str, List[torch.Tensor]] = {}\n",
    "for rec in vec_records:\n",
    "    qid = rec[\"question_id\"]\n",
    "    sentence_cats = cat_by_qid.get(qid, {})\n",
    "    for s in rec[\"sentences\"]:\n",
    "        cat = sentence_cats.get(s[\"sentence_id\"])\n",
    "        if cat is None:\n",
    "            continue\n",
    "        v = torch.tensor(s[\"sent_vec\"], dtype=torch.float32)\n",
    "        vectors_by_cat.setdefault(cat, []).append(v)\n",
    "\n",
    "if SOURCE_CATEGORY not in vectors_by_cat:\n",
    "    raise ValueError(f\"No vectors available for SOURCE_CATEGORY '{SOURCE_CATEGORY}'. Check spelling or data.\")\n",
    "if TARGET_CATEGORY not in vectors_by_cat:\n",
    "    raise ValueError(f\"No vectors available for TARGET_CATEGORY '{TARGET_CATEGORY}'. Check spelling or data.\")\n",
    "\n",
    "mean_vec = {c: torch.stack(v).mean(0) for c, v in vectors_by_cat.items()}\n",
    "print(\"Have mean vectors for\", len(mean_vec), \"categories\")\n",
    "\n",
    "# %%\n",
    "# 3.  Steering direction\n",
    "\n",
    "direction = (mean_vec[TARGET_CATEGORY] - mean_vec[SOURCE_CATEGORY]).to(dtype=getattr(torch, DTYPE))\n",
    "print(\"Direction vector norm:\", float(direction.norm()))\n",
    "\n",
    "# %%\n",
    "# 4.  Fetch reference CoT & locate the SOURCE_CATEGORY sentence, with fallback\n",
    "\n",
    "with open(COMPLETIONS_JSON) as f:\n",
    "    completions = {r[\"question_id\"]: r[\"completion\"] for r in json.load(f)}\n",
    "\n",
    "if QUESTION_ID not in completions:\n",
    "    raise KeyError(f\"QUESTION_ID {QUESTION_ID} not found in {COMPLETIONS_JSON}\")\n",
    "\n",
    "reference_cot = completions[QUESTION_ID]\n",
    "all_sentences = _split_sentences(reference_cot)\n",
    "cat_map = cat_by_qid.get(QUESTION_ID, {})\n",
    "\n",
    "# search for the first sentence tagged with SOURCE_CATEGORY\n",
    "source_index: Optional[int] = next((i for i, _ in enumerate(all_sentences, 1) if cat_map.get(i) == SOURCE_CATEGORY), None)\n",
    "\n",
    "if source_index is None:\n",
    "    available = sorted(set(cat_map.values()))\n",
    "    raise ValueError(\n",
    "        f\"No sentence labelled '{SOURCE_CATEGORY}' in QUESTION_ID {QUESTION_ID}.\\n\"\n",
    "        f\"Available categories for this CoT: {available}.\\n\"\n",
    "        f\"→ Choose a different SOURCE_CATEGORY or pick another QUESTION_ID.\"\n",
    "    )\n",
    "\n",
    "source_index -= 1  # convert to 0‑based\n",
    "prefix_text = \" \".join(all_sentences[:source_index])\n",
    "original_sentence = all_sentences[source_index]\n",
    "\n",
    "print(\"Found SOURCE_CATEGORY sentence at position\", source_index + 1)\n",
    "print(\"→\", original_sentence)\n",
    "\n",
    "# %%\n",
    "# 5.  Load model & tokenizer\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype=getattr(torch, DTYPE), device_map=\"auto\"\n",
    ")\n",
    "model.eval()\n",
    "if tok.pad_token_id is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "    model.config.pad_token_id = tok.eos_token_id\n",
    "\n",
    "print(\"Model loaded – #params:\", sum(p.numel() for p in model.parameters()) / 1e6, \"M\")\n",
    "\n",
    "# %%\n",
    "# 6.  Greedy generation with penultimate‑layer steering\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_sentence(prefix: str, alpha: float, max_tokens: int = 64) -> str:\n",
    "    device = next(model.parameters()).device\n",
    "    ids = tok(prefix, return_tensors=\"pt\").input_ids.to(device)\n",
    "    generated = ids.clone()\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        out = model(generated, output_hidden_states=True)\n",
    "        penult_last = out.hidden_states[-2][:, -1, :]  # (1, D)\n",
    "        steered = penult_last + alpha * direction.to(device)\n",
    "        logits = steered @ model.lm_head.weight.T  # (1, vocab)\n",
    "        next_id = logits.argmax(-1, keepdim=True)\n",
    "        generated = torch.cat([generated, next_id], dim=-1)\n",
    "        tok_str = tok.decode(next_id[0])\n",
    "        if tok_str == \".\" or next_id.item() == tok.eos_token_id:\n",
    "            break\n",
    "\n",
    "    continuation = tok.decode(generated[0][ids.size(1):], skip_special_tokens=True)\n",
    "    first_sentence = _split_sentences(continuation)[0] if continuation else \"\"\n",
    "    return first_sentence.strip()\n",
    "\n",
    "# %%\n",
    "# 7.  Run the α sweep and display\n",
    "\n",
    "print(\"\\n===== Steering results =====\")\n",
    "print(\"target sentence:\", original_sentence)\n",
    "for a in ALPHAS:\n",
    "    try:\n",
    "        s = generate_sentence(prefix_text, a, MAX_NEW_TOKENS)\n",
    "    except Exception as e:\n",
    "        s = f\"<generation error: {e}>\"\n",
    "    print(f\"α = {a:+.2f}:\", s)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
