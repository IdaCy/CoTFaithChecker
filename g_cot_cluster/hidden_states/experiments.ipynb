{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a51ea47",
   "metadata": {},
   "source": [
    "\n",
    "# Chain‑of‑Thought Faithfulness Experiments\n",
    "\n",
    "This notebook reproduces the three experiments described in our research plan:\n",
    "\n",
    "1. **Semantic space of phrase categories** via clustering + UMAP.  \n",
    "2. **Contrastive components** (cPCA/CKA) contrasting hidden states with/without hints.  \n",
    "3. **Linear probes** layer‑by‑layer to decode the phrase category.\n",
    "\n",
    "Everything relies on reusable helpers in `cot_analysis/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b9a96a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cot_analysis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcot_analysis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_segmented_completions, flat_segments\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcot_analysis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhidden_state_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_model, segment_vector, layerwise_segment_vectors\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcot_analysis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvisualization_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m umap_projection, plot_umap, run_kmeans, run_hdbscan, plot_linear_probe_results\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cot_analysis'"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, random, itertools, json, math, collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from cot_analysis.data_utils import load_segmented_completions, flat_segments\n",
    "from cot_analysis.hidden_state_utils import load_model, segment_vector, layerwise_segment_vectors\n",
    "from cot_analysis.visualization_utils import umap_projection, plot_umap, run_kmeans, run_hdbscan, plot_linear_probe_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c22e9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_NAME = \"gpt2\"          # adjust as needed (e.g. path to your fine‑tuned checkpoint)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer, model = load_model(MODEL_NAME, DEVICE)\n",
    "\n",
    "CATEGORIES = [\n",
    "  \"problem_restating\",\n",
    "  \"knowledge_recall\",\n",
    "  \"concept_definition\",\n",
    "  \"quantitative_calculation\",\n",
    "  \"logical_deduction\",\n",
    "  \"option_elimination\",\n",
    "  \"assumption_validation\",\n",
    "  \"uncertainty_expression\",\n",
    "  \"self_questioning\",\n",
    "  \"backtracking_revision\",\n",
    "  \"decision_confirmation\",\n",
    "  \"answer_reporting\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d10d630",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load segmented CoTs\n",
    "bg_segments  = list(flat_segments(load_segmented_completions(\"none\")))\n",
    "\n",
    "hint_types = [\"sycophancy\", \"induced_urgency\", \"unethical_information\"]\n",
    "hint_segments = {\n",
    "    h : list(flat_segments(load_segmented_completions(h))) for h in hint_types\n",
    "}\n",
    "\n",
    "print(f\"Background segments: {len(bg_segments):,}\")\n",
    "for h, segs in hint_segments.items():\n",
    "    print(f\"{h:<22}: {len(segs):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9354b6a",
   "metadata": {},
   "source": [
    "## 1 · Segment clustering and UMAP visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b7c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample (to keep runtime reasonable) and embed\n",
    "SAMPLE_SIZE = 3000\n",
    "random.seed(42)\n",
    "\n",
    "sample_bg = random.sample(bg_segments, SAMPLE_SIZE)\n",
    "embeddings = []\n",
    "labels     = []\n",
    "\n",
    "for seg in sample_bg:\n",
    "    vec = segment_vector(seg[\"text\"], tokenizer, model, DEVICE)\n",
    "    embeddings.append(vec)\n",
    "    labels.append(seg[\"phrase_category\"])\n",
    "embeddings = np.stack(embeddings)\n",
    "\n",
    "# K‑means (can swap to HDBSCAN)\n",
    "kmeans = run_kmeans(embeddings, n_clusters=len(CATEGORIES))\n",
    "\n",
    "# UMAP projection\n",
    "proj2d = umap_projection(embeddings)\n",
    "plot_umap(proj2d, labels, CATEGORIES, title=\"UMAP of segment embeddings (background)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc08f2d",
   "metadata": {},
   "source": [
    "## 2 · Contrastive PCA (no‑hint vs hint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e8dbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install cpca if missing\n",
    "import importlib, subprocess, sys, pkg_resources\n",
    "if importlib.util.find_spec(\"cpca\") is None:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"cpca\"])\n",
    "\n",
    "from cpca import CPCA\n",
    "\n",
    "# Collect matrices\n",
    "def stack_embeddings(segments, max_items=2000):\n",
    "    sample = random.sample(segments, min(len(segments), max_items))\n",
    "    return np.stack([segment_vector(s['text'], tokenizer, model, DEVICE) for s in sample])\n",
    "\n",
    "X_background = stack_embeddings(bg_segments)\n",
    "contrasts    = {}\n",
    "for h in hint_types:\n",
    "    contrasts[h] = stack_embeddings(hint_segments[h])\n",
    "\n",
    "# Run cPCA for each hint type\n",
    "for h, X_target in contrasts.items():\n",
    "    cpca = CPCA(n_components=2, alpha=None, scale=False)\n",
    "    Xt, Xc = cpca.fit_transform(X_target, X_background, plot=False)\n",
    "    # Plot first 2 contrastive components\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.scatter(Xt[:,0], Xt[:,1], s=10, alpha=0.6)\n",
    "    plt.title(f\"cPCA – contrast {h} vs background\")\n",
    "    plt.xlabel(\"cPC‑1\")\n",
    "    plt.ylabel(\"cPC‑2\")\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78167ad7",
   "metadata": {},
   "source": [
    "## 3 · Linear probes per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8030908",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample segments across all datasets for balanced probe training\n",
    "ALL_SEGMENTS = bg_segments + sum(hint_segments.values(), [])\n",
    "random.shuffle(ALL_SEGMENTS)\n",
    "SAMPLE_FOR_PROBE = 4000   # make sure it's not too heavy\n",
    "\n",
    "sample_probe = random.sample(ALL_SEGMENTS, SAMPLE_FOR_PROBE)\n",
    "\n",
    "layer_vectors = []\n",
    "y             = []\n",
    "\n",
    "# First pass to get n_layers\n",
    "_, _, hidden0 = load_model.cache_info().keys()  # ignore\n",
    "n_layers = len(layerwise_segment_vectors(sample_probe[0][\"text\"], tokenizer, model, DEVICE))\n",
    "\n",
    "layer_vectors = [ [] for _ in range(n_layers) ]\n",
    "\n",
    "for seg in sample_probe:\n",
    "    layerwise = layerwise_segment_vectors(seg[\"text\"], tokenizer, model, DEVICE)\n",
    "    for i, vec in enumerate(layerwise):\n",
    "        layer_vectors[i].append(vec)\n",
    "    y.append(CATEGORIES.index(seg[\"phrase_category\"]))\n",
    "\n",
    "# Train/test split *once* to be fair across layers\n",
    "y = np.array(y)\n",
    "idx_train, idx_test = train_test_split(np.arange(len(y)), test_size=0.3, random_state=1, stratify=y)\n",
    "\n",
    "accs = []\n",
    "for layer, X in enumerate(layer_vectors):\n",
    "    X = np.stack(X)\n",
    "    clf = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "    clf.fit(X[idx_train], y[idx_train])\n",
    "    y_pred = clf.predict(X[idx_test])\n",
    "    accs.append(accuracy_score(y[idx_test], y_pred))\n",
    "    print(f\"Layer {layer:2d}: {accs[-1]:.3f}\")\n",
    "\n",
    "plot_linear_probe_results(list(range(n_layers)), accs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
