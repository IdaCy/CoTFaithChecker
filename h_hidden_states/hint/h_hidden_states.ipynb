{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/CoTFaithChecker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/CoTFaithChecker/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/CoTFaithChecker'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ..\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "\n",
    "from a_confirm_posthoc.hint.pipeline import generate_dataset_completions\n",
    "from a_confirm_posthoc.utils.model_handler import load_model_and_tokenizer\n",
    "from a_confirm_posthoc.eval.llm_verificator import run_verification\n",
    "from a_confirm_posthoc.eval.switch_check import run_switch_check\n",
    "from a_confirm_posthoc.eval.llm_hint_verificator import run_hint_verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 13:10:35,910 - INFO - CUDA is available. Using GPU.\n",
      "2025-04-25 13:10:35,911 - INFO - Loading model and tokenizer: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B onto cuda\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "2025-04-25 13:10:54,526 - INFO - Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "# model_path = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "#model_path = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "model, tokenizer, model_name, device = load_model_and_tokenizer(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"mmlu\"\n",
    "#hint_types = [\"none\", \"sycophancy\", \"unethical_information\", \"induced_urgency\"]\n",
    "hint_types = [\"none\", \"sycophancy\"]\n",
    "n_questions = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 13:20:24,157 - INFO - Using chat template: User: {instruction}\n",
      "Assistant:\n",
      "2025-04-25 13:20:24,158 - INFO - --- Processing dataset for hint type: none ---\n",
      "2025-04-25 13:20:24,173 - ERROR - Data file not found: data/mmlu/hints_none.json\n",
      "2025-04-25 13:20:24,174 - INFO - Generating completions for none...\n",
      "2025-04-25 13:20:24,175 - INFO - Using max_new_tokens: 2048\n",
      "2025-04-25 13:20:24,175 - INFO - Processing batch 1/5 (Size: 32, QIDs: 0-31)\n",
      "/home/ubuntu/CoTFaithChecker/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/CoTFaithChecker/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-25 13:21:40,794 - INFO - Processing batch 2/5 (Size: 32, QIDs: 32-63)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-25 13:22:59,026 - INFO - Processing batch 3/5 (Size: 32, QIDs: 64-95)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-25 13:24:18,914 - INFO - Processing batch 4/5 (Size: 32, QIDs: 96-127)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-25 13:25:34,673 - INFO - Processing batch 5/5 (Size: 22, QIDs: 128-149)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-25 13:26:43,222 - INFO - Results saved to data/mmlu/DeepSeek-R1-Distill-Qwen-1.5B/none/completions_with_150.json\n",
      "2025-04-25 13:26:43,223 - INFO - Total processing time: 379.07 seconds\n"
     ]
    }
   ],
   "source": [
    "generate_dataset_completions(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    model_name = model_name,\n",
    "    device = device,\n",
    "    dataset_name = dataset_name,\n",
    "    hint_types = hint_types,\n",
    "    batch_size = 32,\n",
    "    max_new_tokens = None, \n",
    "    n_questions = n_questions\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
