{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/CoTFaithChecker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/root/CoTFaithChecker'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ../..\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️  running on cuda\n",
      "✓ steering vectors: problem_restating·4096, uncertainty_or_certainty_expression·4096, logical_deduction·4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ probing: orig\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "orig:   0%|          | 0/3 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "orig: 100%|██████████| 3/3 [00:25<00:00,  8.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ probing: problem_restating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "problem_restating: 100%|██████████| 3/3 [00:25<00:00,  8.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ probing: uncertainty_or_certainty_expression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "uncertainty_or_certainty_expression:  67%|██████▋   | 2/3 [00:21<00:11, 11.85s/it]"
     ]
    }
   ],
   "source": [
    "# ╔════════════════════════════════════════════════════════════════════╗\n",
    "# ║  Multi-steer CoT probe · 4-way violin plots                        ║\n",
    "# ╚════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "# %% ── 1. Imports & config ───────────────────────────────────────────\n",
    "import json, re, logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import torch, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 110\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "print(\"⚙️  running on\", DEVICE)\n",
    "\n",
    "# %% ── 2. Paths & high-level parameters ──────────────────────────────\n",
    "MODEL_PATH      = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "COT_JSON        = (\"data/mmlu/DeepSeek-R1-Distill-Llama-8B/sycophancy/\"\n",
    "                   \"completions_with_5001.json\")\n",
    "SEGMENT_CAT_JSON= (\"c_cluster_analysis/outputs/hints/mmlu/\"\n",
    "                   \"DeepSeek-R1-Distill-Llama-8B/confidence/\"\n",
    "                   \"sycophancy_unverb_5001.json\")\n",
    "HIDDEN_JSON     = (\"c_cluster_analysis/outputs/hints/mmlu/\"\n",
    "                   \"DeepSeek-R1-Distill-Llama-8B/cat_probe/\"\n",
    "                   \"sycophancy_unverb_5001.json\")\n",
    "WHITELIST_JSON  = (\"c_cluster_analysis/outputs/hints/mmlu/\"\n",
    "                   \"DeepSeek-R1-Distill-Llama-8B/filter/\"\n",
    "                   \"unverb_ids_5001_sycophancy.json\")\n",
    "\n",
    "ALPHA           = 1.0\n",
    "MAX_QUESTIONS   = 3                 # None → use all whitelist\n",
    "OPTION_LABELS   = [\"A\", \"B\", \"C\", \"D\"]\n",
    "INTV_PROMPT     = \"... The final answer is [\"\n",
    "\n",
    "# ---- categories -----------------------------------------------------\n",
    "GENERAL_CATS = [\"problem_restating\",\n",
    "                \"uncertainty_or_certainty_expression\",\n",
    "                \"backtracking\",\n",
    "                \"forward_planning\",\n",
    "                \"decision_confirmation\"]\n",
    "\n",
    "STEER_CATS   = [\"problem_restating\",\n",
    "                \"uncertainty_or_certainty_expression\",\n",
    "                \"logical_deduction\"]   # vectors used for steering\n",
    "\n",
    "# %% ── 3. Helper: sentence split & category map ──────────────────────\n",
    "whitelist_ids = {int(x) for x in json.loads(Path(WHITELIST_JSON).read_text())}\n",
    "\n",
    "_SENT_RGX = re.compile(r\"(?<=\\.)\\s+\")\n",
    "def split_sents(txt:str)->List[str]:\n",
    "    start,end=txt.find(\"<think>\"),txt.find(\"</think>\")\n",
    "    if start!=-1: txt=txt[start+7:]\n",
    "    if end  !=-1: txt=txt[:end]\n",
    "    txt=re.sub(r\"\\s+\",\" \",txt.strip())\n",
    "    parts=[p.strip() for p in _SENT_RGX.split(txt) if p.strip()]\n",
    "    merged,i=[],0\n",
    "    while i<len(parts):\n",
    "        if re.fullmatch(r\"\\d+\\.\",parts[i])and i+1<len(parts):\n",
    "            merged.append(f\"{parts[i]} {parts[i+1]}\");i+=2\n",
    "        else: merged.append(parts[i]);i+=1\n",
    "    return merged\n",
    "\n",
    "def load_cat_map(path)->Dict[Tuple[int,int],str]:\n",
    "    out={}\n",
    "    for blk in json.loads(Path(path).read_text()):\n",
    "        qid=int(blk[\"question_id\"])\n",
    "        for row in blk[\"annotations\"]:\n",
    "            sid=int(row[\"sentence_id\"])\n",
    "            scores={k:v for k,v in row.items()\n",
    "                    if k not in(\"sentence_id\",\"other\",\"other_label\")}\n",
    "            out[(qid,sid)]=max(scores,key=scores.get)\n",
    "    return out\n",
    "cat_map=load_cat_map(SEGMENT_CAT_JSON)\n",
    "\n",
    "# %% ── 4. Build steering vectors ------------------------------------\n",
    "hidden_data=json.loads(Path(HIDDEN_JSON).read_text())\n",
    "def mean_vec(cat)->torch.Tensor:\n",
    "    vecs=[]\n",
    "    for q in hidden_data:\n",
    "        qid=int(q[\"question_id\"])\n",
    "        if qid not in whitelist_ids: continue\n",
    "        for s in q[\"sentences\"]:\n",
    "            if cat_map.get((qid,int(s[\"sentence_id\"])))==cat:\n",
    "                vecs.append(np.array(s[\"sent_vec\"],dtype=np.float32))\n",
    "    if not vecs:\n",
    "        raise ValueError(f\"No vectors for category '{cat}'\")\n",
    "    return torch.tensor(np.mean(np.stack(vecs,0),0),device=DEVICE)\n",
    "\n",
    "STEER_VECS={c:mean_vec(c) for c in STEER_CATS}\n",
    "print(\"✓ steering vectors:\",\", \".join(f\"{k}·{v.shape[-1]}\"for k,v in STEER_VECS.items()))\n",
    "\n",
    "# %% ── 5. Model, tokenizer, hook ------------------------------------\n",
    "tok=AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model=AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,torch_dtype=torch.bfloat16).to(DEVICE).eval()\n",
    "if tok.pad_token_id is None: tok.pad_token=tok.eos_token\n",
    "model.config.pad_token_id=tok.eos_token_id\n",
    "penult_layer=model.model.layers[-2]\n",
    "\n",
    "def make_hook(alpha,vec):\n",
    "    def _hook(_m,_i,out):\n",
    "        h=out[0] if isinstance(out,tuple) else out\n",
    "        q=h[:,-1,:]\n",
    "        n=q.norm(p=2,dim=-1,keepdim=True)\n",
    "        q2=q+alpha*vec\n",
    "        q2=q2*(n/q2.norm(p=2,dim=-1,keepdim=True))\n",
    "        h[:,-1,:]=q2\n",
    "        return out\n",
    "    return _hook\n",
    "\n",
    "INTV_IDS=tok.encode(INTV_PROMPT,add_special_tokens=False)\n",
    "OPT_IDS ={lbl: tok.encode(f\" {lbl}\", add_special_tokens=False)[0]\n",
    "          for lbl in OPTION_LABELS}\n",
    "def softmax_subset(d:Dict[str,float])->Dict[str,float]:\n",
    "    vals=torch.tensor(list(d.values())); pr=torch.softmax(vals,0).tolist()\n",
    "    return dict(zip(d.keys(),pr))\n",
    "\n",
    "# %% ── 6. Probe function --------------------------------------------\n",
    "def probe(completion:str, steer_vec:torch.Tensor|None):\n",
    "    think_pos=completion.find(\"<think>\")\n",
    "    prompt_ids=tok.encode(completion[:think_pos],add_special_tokens=False)\n",
    "\n",
    "    sentences=split_sents(completion)\n",
    "    ctx_ids,res=[],[]\n",
    "    hook=penult_layer.register_forward_hook(\n",
    "            make_hook(ALPHA,steer_vec)) if steer_vec is not None else None\n",
    "    try:\n",
    "        for sid,s in enumerate(sentences,1):\n",
    "            ctx_ids.extend(tok.encode(s,add_special_tokens=False))\n",
    "            seq=torch.tensor([prompt_ids+ctx_ids],device=DEVICE)\n",
    "            with torch.no_grad():\n",
    "                out1=model(seq,output_attentions=True)\n",
    "            att=out1.attentions[-1][0]\n",
    "            #share=float(att[:,-1,:len(prompt_ids)].sum(-1).mean().item())\n",
    "            share=float(att[:,-1,:len(prompt_ids)].mean().item())\n",
    "\n",
    "            seq2=torch.tensor([seq[0].tolist()+INTV_IDS],device=DEVICE)\n",
    "            with torch.no_grad():\n",
    "                logits=model(seq2).logits[0,-1]\n",
    "            logit_dict={lbl:float(logits[tid]) for lbl,tid in OPT_IDS.items()}\n",
    "            prob=max(softmax_subset(logit_dict).values())\n",
    "            res.append(dict(sentence_id=sid,\n",
    "                            share=share,\n",
    "                            top_logit=max(logit_dict.values()),\n",
    "                            top_prob =prob))\n",
    "    finally:\n",
    "        if hook: hook.remove(); torch.cuda.empty_cache()\n",
    "    return res\n",
    "\n",
    "# %% ── 7. Run probe for orig + each steer ---------------------------\n",
    "all_cots=[c for c in json.loads(Path(COT_JSON).read_text())\n",
    "          if int(c[\"question_id\"]) in whitelist_ids]\n",
    "if MAX_QUESTIONS: all_cots=all_cots[:MAX_QUESTIONS]\n",
    "\n",
    "runs={\"orig\":None}|{c:v for c,v in STEER_VECS.items()}\n",
    "records=[]\n",
    "for label,vec in runs.items():\n",
    "    print(f\"▶ probing: {label}\")\n",
    "    for rec in tqdm(all_cots,desc=label):\n",
    "        qid=int(rec[\"question_id\"])\n",
    "        rows=probe(rec[\"completion\"], vec)\n",
    "        for r in rows:\n",
    "            r.update(dict(run=label,\n",
    "                          question_id=qid,\n",
    "                          category=cat_map.get((qid,r[\"sentence_id\"]),\"UNK\")))\n",
    "            records.append(r)\n",
    "\n",
    "df=pd.DataFrame(records)\n",
    "\n",
    "# focus on our five plotted categories\n",
    "df=df[df.category.isin(GENERAL_CATS)]\n",
    "\n",
    "# %% ── 8. 4-way violin plots ----------------------------------------\n",
    "palette={\"orig\":\"#1f77b4\",\n",
    "         STEER_CATS[0]:\"#ff7f0e\",\n",
    "         STEER_CATS[1]:\"#2ca02c\",\n",
    "         STEER_CATS[2]:\"#d62728\"}\n",
    "\n",
    "def violin(metric,title,ylabel):\n",
    "    cats=GENERAL_CATS\n",
    "    fig,ax=plt.subplots(figsize=(1.4*len(cats),5))\n",
    "    pos,clr,data=[],[],[]\n",
    "    for i,cat in enumerate(cats):\n",
    "        for j,run in enumerate([\"orig\"]+STEER_CATS):\n",
    "            vals=df[(df.category==cat)&(df.run==run)][metric].tolist()\n",
    "            if not vals: continue\n",
    "            data.append(vals); pos.append(i*4+j*0.9); clr.append(palette[run])\n",
    "    vp=ax.violinplot(data,positions=pos,widths=0.8,showmeans=False)\n",
    "    for body,c in zip(vp['bodies'],clr):\n",
    "        body.set_facecolor(c); body.set_alpha(.6)\n",
    "    ax.set_xticks([i*4+1.35 for i in range(len(cats))])\n",
    "    ax.set_xticklabels(cats,rotation=45,ha=\"right\")\n",
    "    ax.set_ylabel(ylabel); ax.set_title(title); plt.tight_layout(); plt.show()\n",
    "\n",
    "violin(\"share\",\"Prompt-attention share (0-1)\",\"share\")\n",
    "violin(\"top_logit\",\"Top answer-logit\",\"logit\")\n",
    "violin(\"top_prob\",\"Top answer-probability\",\"probability\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
