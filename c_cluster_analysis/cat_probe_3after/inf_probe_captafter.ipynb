{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/CoTFaithChecker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ../..\n",
    "%pwd\n",
    "import os, json, math, itertools, random, logging, pathlib, re, time, sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, classification_report, precision_recall_fscore_support\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# === Paths & constants =======================================================\n",
    "MODEL_NAME        = \"DeepSeek-R1-Distill-Llama-8B\"\n",
    "MODEL_PATH        = \"deepseek-ai/\" + MODEL_NAME\n",
    "INCOUNT   = 2001                   # keep consistent with earlier run\n",
    "DATA_DIR          = Path(\"data/mmlu\")\n",
    "#DATA_DIR       = Path(\"c_cluster_analysis/outputs/hints/mmlu\") / MODEL_NAME / \"cat_probe\"\n",
    "OUT_GEN       = Path(\"c_cluster_analysis/outputs/hints/mmlu/DeepSeek-R1-Distill-Llama-8B\")\n",
    "OUT_DIR       = OUT_GEN / \"catprob\"\n",
    "QUESTIONS_JSON    = DATA_DIR / \"input_mcq_data.json\"\n",
    "FULL_COT_JSON     = DATA_DIR / MODEL_NAME / \"none\" / f\"completions_with_{INCOUNT}.json\"\n",
    "\n",
    "TARGET_CAT        = \"backtracking\"         # ⬅ pick any of the 12 categories\n",
    "LAYER_STRIDE      = 5                      # every ~5 layers\n",
    "CAPTURE_OUTFILE   = OUT_DIR / f\"hidden_layers_stride{LAYER_STRIDE}.json\"\n",
    "ATTRIBUTE_VEC_OUT = OUT_DIR / f\"attrvec_{TARGET_CAT}.pt\"\n",
    "\n",
    "CATEGORY_NAMES = [\n",
    "    \"problem_restating\",\"knowledge_augmentation\",\"assumption_validation\",\n",
    "    \"logical_deduction\",\"option_elimination\",\"uncertainty_or_certainty_expression\",\n",
    "    \"backtracking\",\"forward_planning\",\"decision_confirmation\",\n",
    "    \"answer_reporting\",\"option_restating\",\"other\",\n",
    "]\n",
    "target_id = CATEGORY_NAMES.index(TARGET_CAT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached capture at c_cluster_analysis/outputs/hints/mmlu/DeepSeek-R1-Distill-Llama-8B/catprob/hidden_layers_stride5.json\n"
     ]
    }
   ],
   "source": [
    "# 1. Hidden-state capture (only runs once)\n",
    "from c_cluster_analysis.cat_probe_3after.probe_capture_general import (\n",
    "    run_probe_capture\n",
    ")\n",
    "\n",
    "if not CAPTURE_OUTFILE.exists():\n",
    "    n_layers = 32   # DeepSeek-R1-Distill-Llama-8B\n",
    "    layers = list(range(1, n_layers + 1, LAYER_STRIDE))  # 1-based indices\n",
    "    print(\"Capturing layers:\", layers)\n",
    "\n",
    "    _ = run_probe_capture(\n",
    "        model_path     = MODEL_PATH,\n",
    "        questions_file = str(QUESTIONS_JSON),\n",
    "        full_cot_file  = str(FULL_COT_JSON),\n",
    "        output_file    = str(CAPTURE_OUTFILE),\n",
    "        layers         = layers,                  # <-- key change\n",
    "        keep_attn_and_logits = True,              # so we can inspect later\n",
    "        max_questions = 2,\n",
    "    )\n",
    "else:\n",
    "    print(\"Using cached capture at\", CAPTURE_OUTFILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 109 sentences\n",
      "After merge: (0, 24)\n"
     ]
    }
   ],
   "source": [
    "# 2. Load capture → DataFrame\n",
    "def load_capture(path: Path) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for obj in json.loads(path.read_text()):\n",
    "        qid = obj[\"question_id\"]\n",
    "        for sent in obj[\"sentences\"]:\n",
    "            base = {\n",
    "                \"question_id\": qid,\n",
    "                \"sentence_id\": sent[\"sentence_id\"],\n",
    "                \"sentence\": sent[\"sentence\"],\n",
    "            }\n",
    "            # one column per captured layer\n",
    "            for k, v in sent[\"pooled_hs\"].items():   # k==\"layer_5\", …\n",
    "                base[k] = np.array(v, dtype=np.float32)\n",
    "            # keep extra diagnostics if present\n",
    "            for extra in (\"answer_logits\",\"avg_att_prompt\"):\n",
    "                if extra in sent: base[extra] = sent[extra]\n",
    "            rows.append(base)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "capt_df = load_capture(CAPTURE_OUTFILE)\n",
    "print(\"Loaded\", len(capt_df), \"sentences\")\n",
    "\n",
    "# merge category annotations ---------------------------------------------------\n",
    "CAT_ANN_FILE = OUT_GEN / \"confidence\" / \"sycophancy_unverb_2001.json\"\n",
    "def load_categories(path: Path) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for obj in json.loads(path.read_text()):\n",
    "        qid = obj[\"question_id\"]\n",
    "        for ann in obj[\"annotations\"]:\n",
    "            rec = {\"question_id\": qid, \"sentence_id\": ann[\"sentence_id\"]}\n",
    "            rec.update({c: ann[c] for c in CATEGORY_NAMES})\n",
    "            rows.append(rec)\n",
    "    return pd.DataFrame(rows)\n",
    "cats_df = load_categories(CAT_ANN_FILE)\n",
    "df = capt_df.merge(cats_df, on=[\"question_id\",\"sentence_id\"])\n",
    "print(\"After merge:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: ['layer_1', 'layer_6', 'layer_11', 'layer_16', 'layer_21', 'layer_26', 'layer_31']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to stack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m metrics \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m layer_cols:\n\u001b[0;32m----> 8\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     y \u001b[38;5;241m=\u001b[39m (df[TARGET_CAT]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     11\u001b[0m     idx_train, idx_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m     12\u001b[0m         np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(y)), test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/shape_base.py:445\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[1;32m    443\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [asanyarray(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arrays:\n\u001b[0;32m--> 445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneed at least one array to stack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    447\u001b[0m shapes \u001b[38;5;241m=\u001b[39m {arr\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays}\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shapes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to stack"
     ]
    }
   ],
   "source": [
    "# 3. Probe per layer (binary: TARGET_CAT vs rest)\n",
    "layer_cols = sorted([c for c in df.columns if c.startswith(\"layer_\")],\n",
    "                    key=lambda x: int(x.split(\"_\")[1]))\n",
    "print(\"Layers:\", layer_cols)\n",
    "\n",
    "metrics = []\n",
    "for col in layer_cols:\n",
    "    X = np.stack(df[col].values)\n",
    "    y = (df[TARGET_CAT]==1).astype(int).values\n",
    "\n",
    "    idx_train, idx_test = train_test_split(\n",
    "        np.arange(len(y)), test_size=0.2, stratify=y, random_state=42)\n",
    "    clf = LogisticRegression(max_iter=500, solver=\"lbfgs\")\n",
    "    clf.fit(X[idx_train], y[idx_train])\n",
    "\n",
    "    y_pred = clf.predict(X[idx_test])\n",
    "    acc  = accuracy_score(y[idx_test], y_pred)\n",
    "    f1   = f1_score     (y[idx_test], y_pred)\n",
    "    metrics.append((col, acc, f1, clf))\n",
    "\n",
    "# Plot F1 vs layer\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot([int(c.split(\"_\")[1]) for c,_,_,_ in metrics],\n",
    "         [m[2] for m in metrics], marker=\"o\")\n",
    "plt.xlabel(\"Layer\"); plt.ylabel(\"F1\"); plt.title(f\"{TARGET_CAT} probe\")\n",
    "plt.ylim(0,1); plt.grid(); plt.show()\n",
    "\n",
    "best_layer, best_acc, best_f1, best_clf = max(metrics, key=lambda t: t[2])\n",
    "print(f\"▶ Best: {best_layer} – acc {best_acc:.3f}  F1 {best_f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Build & store attribute / steering vector\n",
    "best_vecs      = np.stack(df[best_layer].values)\n",
    "mask_target    = (df[TARGET_CAT]==1).values\n",
    "attr_vec       = best_vecs[mask_target].mean(0) - best_vecs[~mask_target].mean(0)\n",
    "\n",
    "torch.save(torch.tensor(attr_vec, dtype=torch.float32), ATTRIBUTE_VEC_OUT)\n",
    "print(\"Saved attribute vector →\", ATTRIBUTE_VEC_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Steering demo: inject at generation time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "mdl = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "mdl.eval()\n",
    "\n",
    "layer_idx = int(best_layer.split(\"_\")[1])   # 1-based\n",
    "alpha = 2.5                                 # steering strength (try 0-5)\n",
    "\n",
    "attr_t = torch.tensor(attr_vec, device=mdl.device, dtype=mdl.dtype)\n",
    "\n",
    "def add_attrvec_hook(module, input, output):\n",
    "    # output: (batch, seq, hidden)\n",
    "    return output + alpha * attr_t\n",
    "\n",
    "handle = mdl.model.layers[layer_idx-1].register_forward_hook(add_attrvec_hook)\n",
    "\n",
    "prompt = \"Explain why the sky appears blue during the day.\"\n",
    "ids = tok.encode(prompt, return_tensors=\"pt\").to(mdl.device)\n",
    "out = mdl.generate(ids, max_new_tokens=120, temperature=0.7)\n",
    "print(tok.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "handle.remove()   # clean up\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
