{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/root'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ..\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "\n",
    "from a_confirm_posthoc.src.main.pipeline import load_model_and_tokenizer, generate_dataset_completions\n",
    "from a_confirm_posthoc.src.eval.llm_verificator import run_verification\n",
    "from a_confirm_posthoc.src.eval.switch_check import run_switch_check\n",
    "from a_confirm_posthoc.src.eval.llm_hint_verificator import run_hint_verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 14:41:51,768 - INFO - CUDA is available. Using GPU.\n",
      "2025-04-23 14:41:51,769 - INFO - Loading model and tokenizer: deepseek-ai/DeepSeek-R1-Distill-Qwen-14B onto cuda\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# model_path = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "#model_path = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "model_path = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "model, tokenizer, model_name, device = load_model_and_tokenizer(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"mmlu\"\n",
    "hint_types = [\"unethical_information\", \"induced_urgency\"]\n",
    "n_questions = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 12:34:28,933 - INFO - Using chat template: User: {instruction}\n",
      "Assistant:\n",
      "2025-04-23 12:34:28,933 - INFO - --- Processing dataset for hint type: unethical_information ---\n",
      "2025-04-23 12:34:28,942 - INFO - Generating completions for unethical_information...\n",
      "2025-04-23 12:34:28,943 - INFO - Using max_new_tokens: 2048\n",
      "2025-04-23 12:34:28,943 - INFO - Processing batch 1/32 (Size: 16, QIDs: 0-15)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 12:38:04,044 - INFO - Processing batch 2/32 (Size: 16, QIDs: 16-31)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 12:41:56,391 - INFO - Processing batch 3/32 (Size: 16, QIDs: 32-47)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 12:45:16,741 - INFO - Processing batch 4/32 (Size: 16, QIDs: 48-63)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 12:47:31,913 - INFO - Processing batch 5/32 (Size: 16, QIDs: 64-79)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 12:50:22,283 - INFO - Processing batch 6/32 (Size: 16, QIDs: 80-95)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 12:51:45,548 - INFO - Processing batch 7/32 (Size: 16, QIDs: 96-111)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 12:55:33,196 - INFO - Processing batch 8/32 (Size: 16, QIDs: 112-127)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 12:56:33,952 - INFO - Processing batch 9/32 (Size: 16, QIDs: 128-143)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 13:00:34,392 - INFO - Processing batch 10/32 (Size: 16, QIDs: 144-159)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 13:04:24,147 - INFO - Processing batch 11/32 (Size: 16, QIDs: 160-175)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 13:08:14,926 - INFO - Processing batch 12/32 (Size: 16, QIDs: 176-191)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 13:11:45,852 - INFO - Processing batch 13/32 (Size: 16, QIDs: 192-207)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 13:15:18,134 - INFO - Processing batch 14/32 (Size: 16, QIDs: 208-223)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 13:18:41,470 - INFO - Processing batch 15/32 (Size: 16, QIDs: 224-239)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 13:22:15,501 - INFO - Processing batch 16/32 (Size: 16, QIDs: 240-255)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 13:26:14,412 - INFO - Processing batch 17/32 (Size: 16, QIDs: 256-271)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 13:27:12,344 - INFO - Processing batch 18/32 (Size: 16, QIDs: 272-287)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 13:27:55,569 - INFO - Processing batch 19/32 (Size: 16, QIDs: 288-303)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 13:29:33,253 - INFO - Processing batch 20/32 (Size: 16, QIDs: 304-319)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 13:33:01,665 - INFO - Processing batch 21/32 (Size: 16, QIDs: 320-335)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 13:36:45,323 - INFO - Processing batch 22/32 (Size: 16, QIDs: 336-351)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 13:38:04,678 - INFO - Processing batch 23/32 (Size: 16, QIDs: 352-367)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 13:40:21,033 - INFO - Processing batch 24/32 (Size: 16, QIDs: 368-383)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 13:43:43,894 - INFO - Processing batch 25/32 (Size: 16, QIDs: 384-399)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 13:46:23,550 - INFO - Processing batch 26/32 (Size: 16, QIDs: 400-415)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 13:48:11,183 - INFO - Processing batch 27/32 (Size: 16, QIDs: 416-431)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 13:49:42,802 - INFO - Processing batch 28/32 (Size: 16, QIDs: 432-447)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 13:53:42,379 - INFO - Processing batch 29/32 (Size: 16, QIDs: 448-463)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 13:57:51,114 - INFO - Processing batch 30/32 (Size: 16, QIDs: 464-479)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 14:02:03,428 - INFO - Processing batch 31/32 (Size: 16, QIDs: 480-495)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 14:04:51,564 - INFO - Processing batch 32/32 (Size: 4, QIDs: 496-499)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 14:05:28,861 - INFO - Results saved to data/mmlu/DeepSeek-R1-Distill-Qwen-14B/unethical_information/completions_with_500.json\n",
      "2025-04-23 14:05:28,862 - INFO - --- Processing dataset for hint type: induced_urgency ---\n",
      "2025-04-23 14:05:28,870 - INFO - Generating completions for induced_urgency...\n",
      "2025-04-23 14:05:28,871 - INFO - Using max_new_tokens: 2048\n",
      "2025-04-23 14:05:28,871 - INFO - Processing batch 1/32 (Size: 16, QIDs: 0-15)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 14:09:05,871 - INFO - Processing batch 2/32 (Size: 16, QIDs: 16-31)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 14:12:58,707 - INFO - Processing batch 3/32 (Size: 16, QIDs: 32-47)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 14:15:54,788 - INFO - Processing batch 4/32 (Size: 16, QIDs: 48-63)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 14:19:30,889 - INFO - Processing batch 5/32 (Size: 16, QIDs: 64-79)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 14:21:23,725 - INFO - Processing batch 6/32 (Size: 16, QIDs: 80-95)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 14:25:06,273 - INFO - Processing batch 7/32 (Size: 16, QIDs: 96-111)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-23 14:28:53,671 - INFO - Processing batch 8/32 (Size: 16, QIDs: 112-127)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "generate_dataset_completions(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    model_name = model_name,\n",
    "    device = device,\n",
    "    dataset_name = dataset_name,\n",
    "    hint_types = hint_types,\n",
    "    batch_size = 16,\n",
    "    max_new_tokens = None, \n",
    "    n_questions = n_questions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run llm verification to get the final model answers\n",
    "# Note that this will drop the results that are N/A (eg the model never stopped reasoning)\n",
    "run_verification(dataset_name, hint_types, model_name, n_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model switches between none and the other hint types\n",
    "# [1:] because we don't want to check the none hint type as it's the baseline\n",
    "run_switch_check(dataset_name, hint_types[1:], model_name, n_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if the model verbalizes the hint\n",
    "# [1:] because we don't want to check the none hint type as it's the baseline\n",
    "run_hint_verification(dataset_name, hint_types[1:], model_name, n_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from a_confirm_posthoc.src.eval.faithfulness_metric import run_faithfulness_metric\n",
    "base_path = \"data/mmlu/DeepSeek-R1-Distill-Llama-8B/induced_urgency/\"\n",
    "\n",
    "hint_verification_path = base_path + \"hint_verification_with_500.json\"\n",
    "switch_analysis_path = base_path + \"switch_analysis_with_500.json\"\n",
    "\n",
    "results = run_faithfulness_metric(\n",
    "    hint_verification_path=hint_verification_path,\n",
    "    switch_analysis_path=switch_analysis_path,\n",
    "    out_filename=base_path+\"/faithfulness_results.json\"\n",
    ")\n",
    "\n",
    "print(\"=== Faithfulness Results ===\")\n",
    "print(\"Raw faithfulness:       \", results[\"raw_faithfulness\"])\n",
    "print(\"Corrected faithfulness: \", results[\"corrected_faithfulness\"])\n",
    "print(\"Alpha:                  \", results[\"alpha\"])\n",
    "print(\"p (switch-to-hint):     \", results[\"p\"])\n",
    "print(\"q (switch-other):       \", results[\"q\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from a_confirm_posthoc.src.eval.faithfulness_metric import compute_faithfulness_metric\n",
    "\n",
    "unhinted_path = \"data/gsm8k/DeepSeek-R1-Distill-Llama-8B_old/none/completions_with_150.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sycophancy\n",
    "\n",
    "verification_path = \"data/gsm8k/DeepSeek-R1-Distill-Llama-8B_old/sycophancy/hint_verification_with_150.json\"\n",
    "hinted_path   = \"data/gsm8k/DeepSeek-R1-Distill-Llama-8B_old/sycophancy/completions_with_150.json\"\n",
    "hints_path    = \"data/gsm8k/hints_sycophancy.json\"\n",
    "\n",
    "\n",
    "results = compute_faithfulness_metric(\n",
    "    unhinted_completions_path=unhinted_path,\n",
    "    hinted_completions_path=hinted_path,\n",
    "    hint_verification_path=verification_path,\n",
    "    hints_path=hints_path,\n",
    ")\n",
    "\n",
    "print(\"Faithfulness Metric Results:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Induced Urgency\n",
    "\n",
    "verification_path = \"data/induced_urgency/hint_verification_DeepSeek-R1-Distill-Llama-8B_with_150.json\"\n",
    "hinted_path   = \"data/induced_urgency/completions_DeepSeek-R1-Distill-Llama-8B_with_150.json\"\n",
    "hints_path    = \"data/induced_urgency/hints.json\"\n",
    "\n",
    "results = compute_faithfulness_metric(\n",
    "    unhinted_completions_path=unhinted_path,\n",
    "    hinted_completions_path=hinted_path,\n",
    "    hint_verification_path=verification_path,\n",
    "    hints_path=hints_path,\n",
    ")\n",
    "\n",
    "print(\"Faithfulness Metric Results:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unethical Info\n",
    "\n",
    "verification_path = \"data/unethical_information/hint_verification_DeepSeek-R1-Distill-Llama-8B_with_150.json\"\n",
    "hinted_path   = \"data/unethical_information/completions_DeepSeek-R1-Distill-Llama-8B_with_150.json\"\n",
    "hints_path    = \"data/unethical_information/hints.json\"\n",
    "\n",
    "results = compute_faithfulness_metric(\n",
    "    unhinted_completions_path=unhinted_path,\n",
    "    hinted_completions_path=hinted_path,\n",
    "    hint_verification_path=verification_path,\n",
    "    hints_path=hints_path,\n",
    ")\n",
    "\n",
    "print(\"Faithfulness Metric Results:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: can the model solve the questions without CoT?\n",
    "\n",
    "from a_confirm_posthoc.src.eval.sanity_check import sanity_check\n",
    "\n",
    "sanity_results = sanity_check(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_name=model_name,\n",
    "    device=device,\n",
    "    dataset_name=dataset_name,\n",
    "    batch_size=16,\n",
    "    max_new_tokens=4,   # we only need the letter\n",
    "    n_questions=n_questions,\n",
    ")\n",
    "\n",
    "print(\"\\n=== No-CoT Sanity Check ===\")\n",
    "print(f\"Accuracy: {sanity_results['accuracy']*100:.2f}% \"\n",
    "      f\"({sanity_results['correct']}/{sanity_results['total']})\")\n",
    "if sanity_results[\"accuracy\"] > 0.35:      # > 35 % is suspicious for 4-way MCQ\n",
    "    print(\"Warning: accuracy is well above chance; the dataset may \"\n",
    "          \"not require explicit chain-of-thought.\")\n",
    "else:\n",
    "    print(\"Passed: model performs at or near chance without CoT.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint-attention analysis\n",
    "from b_attention_hint.src.attention_checker import attention_check\n",
    "\n",
    "attention_results = attention_check(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_name=model_name,\n",
    "    device=device,\n",
    "    dataset_name=dataset_name,    # ← reuse earlier variable\n",
    "    hint_type=\"induced_urgency\",\n",
    "    baseline_hint_type=\"none\",    # completions without hints\n",
    "    n_questions=n_questions,      # keep it in sync with generation\n",
    "    probe_first_k=10,             # look at first 10 generated tokens\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"=== Attention & Probe results ===\")\n",
    "for k, v in attention_results.items():\n",
    "    if not isinstance(v, list):   # don't flood the output with full curves\n",
    "        print(f\"{k}: {v}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
