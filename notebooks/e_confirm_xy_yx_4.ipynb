{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/CoTFaithChecker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-26 18:40:50,509 - INFO - CUDA is available. Using GPU.\n",
      "2025-04-26 18:40:50,509 - INFO - Loading model and tokenizer: deepseek-ai/DeepSeek-R1-Distill-Llama-8B onto cuda\n",
      "Fetching 2 files: 100%|██████████| 2/2 [00:15<00:00,  7.76s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.81it/s]\n",
      "2025-04-26 18:41:10,803 - INFO - Model and tokenizer loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ───────────────────────────────────────────────\n",
    "# 0. Manual configuration\n",
    "# ───────────────────────────────────────────────\n",
    "%cd ..\n",
    "%pwd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "DATA_ROOT = Path(\"data/chainscope/questions_json\")\n",
    "TEMPLATE_PATH = Path(\"data/chainscope/templates/instructions.json\")\n",
    "LOG_DIR = Path(\"logs\")\n",
    "OUT_DIR = Path(\"e_confirm_xy_yx/outputs\")          # completions, verification, matches\n",
    "MODEL_PATH = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# choose folder subsets\n",
    "MAIN_TYPE = \"gt\"\n",
    "DATASETS   = [f\"{MAIN_TYPE}_NO_1\", f\"{MAIN_TYPE}_YES_1\"]\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "MAX_NEW_TOKENS = None\n",
    "\n",
    "# ─── multi-run & sampling ────────────────────────────────────────\n",
    "N_RUNS      = 10      # generate 10 reasoning chains per question\n",
    "TEMPERATURE = 0.7     # sampling temperature\n",
    "TOP_P       = 0.9     # nucleus-sampling top-p\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "\n",
    "OUT_GEN = Path(\"e_confirm_xy_yx/outputs/\" + MAIN_TYPE + \"_completions_\" + str(N_RUNS))\n",
    "\n",
    "SAVE_HIDDEN, SAVE_ATTN = False, False\n",
    "HIDDEN_LAYERS, ATTN_LAYERS = [0, -1], [0, -1]   # ignored unless above switches True\n",
    "N_VERIFY = 0   # 0 == verify all\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 1. Load model & tokenizer  (your helper)\n",
    "# ───────────────────────────────────────────────\n",
    "from a_confirm_posthoc.utils.model_handler import load_model_and_tokenizer\n",
    "\n",
    "model, tokenizer, model_name, device = load_model_and_tokenizer(MODEL_PATH)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "data_loader — INFO — → kept 0 after cluster filter ['human']\n",
      "2025-04-26 18:42:36,335 - INFO - → kept 0 after cluster filter ['human']\n",
      "data_loader — INFO — Found 0 files in data/gt_NO_1\n",
      "2025-04-26 18:42:36,336 - INFO - Found 0 files in data/gt_NO_1\n",
      "data_loader — INFO — → kept 0 after cluster filter ['human']\n",
      "2025-04-26 18:42:36,338 - INFO - → kept 0 after cluster filter ['human']\n",
      "data_loader — INFO — Found 0 files in data/gt_YES_1\n",
      "2025-04-26 18:42:36,338 - INFO - Found 0 files in data/gt_YES_1\n",
      "data_loader — INFO — Total files collected: 0\n",
      "2025-04-26 18:42:36,339 - INFO - Total files collected: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "get_dataset_files() source:\n",
      "================================================================================\n",
      "    def get_dataset_files(\n",
      "        questions_root: Path,\n",
      "        dataset_folders: List[str],\n",
      "        clusters: List[str] | None = None,\n",
      "    ) -> List[Path]:\n",
      "        \"\"\"\n",
      "        Return every *.json file inside the requested folder names.\n",
      "        Example: dataset_folders = [\"gt_NO_1\", \"gt_YES_1\"].\n",
      "        \"\"\"\n",
      "        files: List[Path] = []\n",
      "        for folder in dataset_folders:\n",
      "            folder_path = questions_root / folder\n",
      "            current_files = sorted(folder_path.glob(\"*.json\"))\n",
      "            # optional cluster-level filtering\n",
      "            if clusters is not None:\n",
      "                current_files = [\n",
      "                    f for f in current_files\n",
      "                    if detect_cluster(f.stem) in clusters\n",
      "                ]\n",
      "                logger.info(\n",
      "                    f\"→ kept {len(current_files)} after cluster filter {clusters}\"\n",
      "                )\n",
      "            logger.info(f\"Found {len(current_files)} files in {folder_path}\")\n",
      "            files.extend(current_files)\n",
      "        logger.info(f\"Total files collected: {len(files)}\")\n",
      "        return files\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Returned 0 file(s):\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Candidates found per dataset (before loader filters):\n",
      "================================================================================\n",
      "\n",
      "gt_NO_1 (6 files):\n",
      "   wm-nyc-place-lat_gt_NO_1_089018ac.json\n",
      "   wm-nyc-place-long_gt_NO_1_34dd4404.json\n",
      "   wm-person-age_gt_NO_1_91b16500.json\n",
      "   wm-person-birth_gt_NO_1_5e3000c2.json\n",
      "   wm-person-death_gt_NO_1_824c23ea.json\n",
      "   wm-song-release_gt_NO_1_c41fc992.json\n",
      "\n",
      "gt_YES_1 (6 files):\n",
      "   wm-nyc-place-lat_gt_YES_1_5ad9993c.json\n",
      "   wm-nyc-place-long_gt_YES_1_b29e920d.json\n",
      "   wm-person-age_gt_YES_1_d3ebac3a.json\n",
      "   wm-person-birth_gt_YES_1_658fde65.json\n",
      "   wm-person-death_gt_YES_1_99ea4ed8.json\n",
      "   wm-song-release_gt_YES_1_c122e832.json\n"
     ]
    }
   ],
   "source": [
    "# ─── debug_get_dataset_files.py ────────────────────────────────────────────────\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import inspect\n",
    "import textwrap\n",
    "\n",
    "# --- project import -----------------------------------------------------------\n",
    "from e_confirm_xy_yx.main import data_loader          # adjust if module path differs\n",
    "\n",
    "# --- config -------------------------------------------------------------------\n",
    "DATA_ROOT = Path(\"data\")              # change if needed\n",
    "DATASETS  = [\"gt_NO_1\", \"gt_YES_1\"]   # the same two you pass in your script\n",
    "CLUSTERS  = [\"human\"]                 # only the 'human' cluster\n",
    "\n",
    "# ---- 1. show the loader’s source code ----------------------------------------\n",
    "print(\"\\n\" + \"=\"*80 + \"\\nget_dataset_files() source:\\n\" + \"=\"*80)\n",
    "print(textwrap.indent(inspect.getsource(data_loader.get_dataset_files), \"    \"))\n",
    "\n",
    "# ---- 2. call the loader ------------------------------------------------------\n",
    "files = data_loader.get_dataset_files(DATA_ROOT, DATASETS, clusters=CLUSTERS)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + f\"\\nReturned {len(files)} file(s):\\n\" + \"=\"*80)\n",
    "for p in files:\n",
    "    # show path relative to project root for brevity\n",
    "    print(\"  \", p.relative_to(DATA_ROOT))\n",
    "\n",
    "# ---- 3. show every candidate in each dataset dir -----------------------------\n",
    "import re\n",
    "\n",
    "# re-use the same cluster regex the loader will use\n",
    "CLUSTER_PATTERNS = {\n",
    "    \"thing\":   re.compile(r\"^wm-(book|movie|nyt)\"),\n",
    "    \"human\":   re.compile(r\"^wm-(nyc|person|song)\"),\n",
    "    \"us\":      re.compile(r\"^wm-us-\"),\n",
    "    \"world\":   re.compile(r\"^wm-world-\"),\n",
    "    \"us-and-world\": re.compile(r\"^wm-(us|world)-\"),\n",
    "}\n",
    "pattern = CLUSTER_PATTERNS[\"human\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\nCandidates found per dataset (before loader filters):\\n\" + \"=\"*80)\n",
    "for ds in DATASETS:\n",
    "    ds_dir = DATA_ROOT / \"chainscope\" / \"questions_json\" / ds\n",
    "    cand   = sorted([p for p in ds_dir.glob(\"wm-*\") if pattern.match(p.name)])\n",
    "    print(f\"\\n{ds} ({len(cand)} files):\")\n",
    "    for p in cand:\n",
    "        print(\"  \", p.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "data_loader — INFO — → kept 0 after cluster filter ['human']\n",
      "2025-04-26 18:43:05,583 - INFO - → kept 0 after cluster filter ['human']\n",
      "data_loader — INFO — Found 0 files in data/gt_NO_1\n",
      "2025-04-26 18:43:05,585 - INFO - Found 0 files in data/gt_NO_1\n",
      "data_loader — INFO — → kept 0 after cluster filter ['human']\n",
      "2025-04-26 18:43:05,586 - INFO - → kept 0 after cluster filter ['human']\n",
      "data_loader — INFO — Found 0 files in data/gt_YES_1\n",
      "2025-04-26 18:43:05,588 - INFO - Found 0 files in data/gt_YES_1\n",
      "data_loader — INFO — Total files collected: 0\n",
      "2025-04-26 18:43:05,589 - INFO - Total files collected: 0\n"
     ]
    }
   ],
   "source": [
    "from e_confirm_xy_yx.main.data_loader import get_dataset_files\n",
    "\n",
    "# 0. Extra toggle\n",
    "CLUSTERS = [\"human\"]   # no \"no_wm\"\n",
    "\n",
    "# 2. Collect dataset files\n",
    "dataset_files = get_dataset_files(\n",
    "    DATA_ROOT,\n",
    "    DATASETS,\n",
    "    clusters=CLUSTERS,          # ← NEW ARG\n",
    ")\n",
    "\n",
    "# 5. Verify – point to aggregated cluster outputs\n",
    "completion_files = sorted(\n",
    "    (OUT_DIR / \"completions\" / \"clusters\").glob(\"*_completions.json\")\n",
    ")\n",
    "\n",
    "# 6. Match YES vs NO on cluster files\n",
    "verified_files = sorted((OUT_DIR / \"verified\").glob(\"*_verified.json\"))\n",
    "\n",
    "pairs = [\n",
    "    (vf, vf.parent / vf.name.replace(\"_NO_\", \"_YES_\"))\n",
    "    for vf in verified_files\n",
    "    if \"_NO_\" in vf.name\n",
    "    and (vf.parent / vf.name.replace(\"_NO_\", \"_YES_\")).exists()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "prompt_builder — INFO — Logger initialised; log file = /root/CoTFaithChecker/logs/prompt_builder_20250426_181833.log\n",
      "2025-04-26 18:18:33,460 - INFO - Logger initialised; log file = /root/CoTFaithChecker/logs/prompt_builder_20250426_181833.log\n",
      "prompt_builder — INFO — PromptBuilder initialised with style=instr-v0, mode=cot\n",
      "2025-04-26 18:18:33,463 - INFO - PromptBuilder initialised with style=instr-v0, mode=cot\n",
      "inference — INFO — Logger initialised; log file = /root/CoTFaithChecker/logs/inference_20250426_181833.log\n",
      "2025-04-26 18:18:33,468 - INFO - Logger initialised; log file = /root/CoTFaithChecker/logs/inference_20250426_181833.log\n",
      "inference — INFO — Processing wm-book-length_lt_NO_1_f0d02256.json\n",
      "2025-04-26 18:18:33,470 - INFO - Processing wm-book-length_lt_NO_1_f0d02256.json\n",
      "2025-04-26 18:18:33,471 - DEBUG - Loading data/chainscope/questions_json/lt_NO_1/wm-book-length_lt_NO_1_f0d02256.json\n",
      "2025-04-26 18:18:33,473 - DEBUG -   ↳ run 1/10\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 2. Collect dataset files\n",
    "# ───────────────────────────────────────────────\n",
    "#from e_confirm_xy_yx.main.data_loader import get_dataset_files\n",
    "#dataset_files = get_dataset_files(DATA_ROOT, DATASETS)\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 3. Prepare prompt builder\n",
    "# ───────────────────────────────────────────────\n",
    "from e_confirm_xy_yx.main.prompt_builder import PromptBuilder\n",
    "pb = PromptBuilder(template_path=TEMPLATE_PATH, style=\"instr-v0\", mode=\"cot\")\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 4. Run inference\n",
    "# ───────────────────────────────────────────────\n",
    "from e_confirm_xy_yx.main.inference import run_inference\n",
    "\n",
    "run_inference(\n",
    "    dataset_files=dataset_files,\n",
    "    prompt_builder=pb,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_name=model_name,\n",
    "    device=device,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    save_hidden=SAVE_HIDDEN,\n",
    "    hidden_layers=HIDDEN_LAYERS,\n",
    "    save_attention=SAVE_ATTN,\n",
    "    attn_layers=ATTN_LAYERS,\n",
    "    output_dir=OUT_GEN,\n",
    "    n_runs=N_RUNS,\n",
    "    temperature=TEMPERATURE,\n",
    "    top_p=TOP_P,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# ───────────────────────────────────────────────\\n# 5. Verify model answers\\n# ───────────────────────────────────────────────\\nfrom e_confirm_xy_yx.main.verifier import run_verification\\ncompletion_files = sorted((OUT_DIR / \"completions\").glob(\"*_completions.json\"))\\n\\nrun_verification(\\n    completion_files=completion_files,\\n    n_questions=N_VERIFY,\\n    output_dir=OUT_DIR / \"verified\",\\n)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# ───────────────────────────────────────────────\n",
    "# 5. Verify model answers\n",
    "# ───────────────────────────────────────────────\n",
    "from e_confirm_xy_yx.main.verifier import run_verification\n",
    "completion_files = sorted((OUT_DIR / \"completions\").glob(\"*_completions.json\"))\n",
    "\n",
    "run_verification(\n",
    "    completion_files=completion_files,\n",
    "    n_questions=N_VERIFY,\n",
    "    output_dir=OUT_DIR / \"verified\",\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# ───────────────────────────────────────────────\\n# 6. Cross-match YES vs NO answers\\n# ───────────────────────────────────────────────\\nfrom e_confirm_xy_yx.main.match_checker import check_matches\\nverified_files = sorted((OUT_DIR / \"verified\").glob(\"*_verified.json\"))\\n\\n# pair them: every gt_NO_X file with its matching gt_YES_X (adapt if lt)\\npairs = [\\n    (\\n        vf,\\n        vf.parent\\n        / vf.name.replace(\"gt_NO\", \"gt_YES\")\\n    )\\n    for vf in verified_files\\n    if \"_NO_\" in vf.name\\n]\\n\\nfor no_file, yes_file in pairs:\\n    out_match = (\\n        OUT_DIR\\n        / \"matches\"\\n        / f\"{no_file.stem.replace(\\'_verified\\',\\'\\')}_match.json\"\\n    )\\n    out_match.parent.mkdir(parents=True, exist_ok=True)\\n    check_matches(no_file, yes_file, out_match)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "# ───────────────────────────────────────────────\n",
    "# 6. Cross-match YES vs NO answers\n",
    "# ───────────────────────────────────────────────\n",
    "from e_confirm_xy_yx.main.match_checker import check_matches\n",
    "verified_files = sorted((OUT_DIR / \"verified\").glob(\"*_verified.json\"))\n",
    "\n",
    "# pair them: every gt_NO_X file with its matching gt_YES_X (adapt if lt)\n",
    "pairs = [\n",
    "    (\n",
    "        vf,\n",
    "        vf.parent\n",
    "        / vf.name.replace(\"gt_NO\", \"gt_YES\")\n",
    "    )\n",
    "    for vf in verified_files\n",
    "    if \"_NO_\" in vf.name\n",
    "]\n",
    "\n",
    "for no_file, yes_file in pairs:\n",
    "    out_match = (\n",
    "        OUT_DIR\n",
    "        / \"matches\"\n",
    "        / f\"{no_file.stem.replace('_verified','')}_match.json\"\n",
    "    )\n",
    "    out_match.parent.mkdir(parents=True, exist_ok=True)\n",
    "    check_matches(no_file, yes_file, out_match)\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
