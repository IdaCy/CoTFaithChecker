{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/CoTFaithChecker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-26 21:53:07,632 - INFO - CUDA is available. Using GPU.\n",
      "2025-04-26 21:53:07,632 - INFO - Loading model and tokenizer: deepseek-ai/DeepSeek-R1-Distill-Llama-8B onto cuda\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.89it/s]\n",
      "2025-04-26 21:53:11,928 - INFO - Model and tokenizer loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ───────────────────────────────────────────────\n",
    "# 0. Manual configuration\n",
    "# ───────────────────────────────────────────────\n",
    "%cd ..\n",
    "%pwd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "DATA_ROOT = Path(\"data/chainscope/questions_json\")\n",
    "TEMPLATE_PATH = Path(\"data/chainscope/templates/instructions.json\")\n",
    "LOG_DIR = Path(\"logs\")\n",
    "OUT_DIR = Path(\"e_confirm_xy_yx/outputs\")          # completions, verification, matches\n",
    "MODEL_PATH = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# choose folder subsets\n",
    "MAIN_TYPE = \"gt\"\n",
    "MAIN_TYPE_2 = \"lt\"\n",
    "DATASETS = [MAIN_TYPE + \"_NO_1\", MAIN_TYPE + \"_YES_1\", MAIN_TYPE_2 + \"_NO_1\", MAIN_TYPE_2 + \"_YES_1\"]\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "MAX_NEW_TOKENS = None\n",
    "\n",
    "# ─── multi-run & sampling ────────────────────────────────────────\n",
    "N_RUNS      = 10      # generate 10 reasoning chains per question\n",
    "TEMPERATURE = 0.7     # sampling temperature\n",
    "TOP_P       = 0.9     # nucleus-sampling top-p\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "\n",
    "OUT_GEN = Path(\"e_confirm_xy_yx/outputs/\" + MAIN_TYPE + \"_\" + MAIN_TYPE_2 + \"_completions_\" + str(N_RUNS))\n",
    "\n",
    "SAVE_HIDDEN, SAVE_ATTN = False, False\n",
    "HIDDEN_LAYERS, ATTN_LAYERS = [0, -1], [0, -1]   # ignored unless above switches True\n",
    "N_VERIFY = 0   # 0 == verify all\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 1. Load model & tokenizer  (your helper)\n",
    "# ───────────────────────────────────────────────\n",
    "from a_confirm_posthoc.utils.model_handler import load_model_and_tokenizer\n",
    "\n",
    "model, tokenizer, model_name, device = load_model_and_tokenizer(MODEL_PATH)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "data_loader — INFO — Logger initialised; log file = /root/CoTFaithChecker/logs/data_loader_20250426_215311.log\n",
      "2025-04-26 21:53:11,941 - INFO - Logger initialised; log file = /root/CoTFaithChecker/logs/data_loader_20250426_215311.log\n",
      "data_loader — INFO — → kept 8 after cluster filter ['spec']\n",
      "2025-04-26 21:53:11,942 - INFO - → kept 8 after cluster filter ['spec']\n",
      "data_loader — INFO — Found 8 files in data/chainscope/questions_json/gt_NO_1\n",
      "2025-04-26 21:53:11,943 - INFO - Found 8 files in data/chainscope/questions_json/gt_NO_1\n",
      "data_loader — INFO — → kept 8 after cluster filter ['spec']\n",
      "2025-04-26 21:53:11,944 - INFO - → kept 8 after cluster filter ['spec']\n",
      "data_loader — INFO — Found 8 files in data/chainscope/questions_json/gt_YES_1\n",
      "2025-04-26 21:53:11,944 - INFO - Found 8 files in data/chainscope/questions_json/gt_YES_1\n",
      "data_loader — INFO — → kept 8 after cluster filter ['spec']\n",
      "2025-04-26 21:53:11,945 - INFO - → kept 8 after cluster filter ['spec']\n",
      "data_loader — INFO — Found 8 files in data/chainscope/questions_json/lt_NO_1\n",
      "2025-04-26 21:53:11,946 - INFO - Found 8 files in data/chainscope/questions_json/lt_NO_1\n",
      "data_loader — INFO — → kept 8 after cluster filter ['spec']\n",
      "2025-04-26 21:53:11,947 - INFO - → kept 8 after cluster filter ['spec']\n",
      "data_loader — INFO — Found 8 files in data/chainscope/questions_json/lt_YES_1\n",
      "2025-04-26 21:53:11,947 - INFO - Found 8 files in data/chainscope/questions_json/lt_YES_1\n",
      "data_loader — INFO — Total files collected: 32\n",
      "2025-04-26 21:53:11,947 - INFO - Total files collected: 32\n"
     ]
    }
   ],
   "source": [
    "from e_confirm_xy_yx.main.data_loader import get_dataset_files\n",
    "\n",
    "# 0. Extra toggle\n",
    "CLUSTERS = [\"spec\"]   # no \"no_wm\"\n",
    "\n",
    "# 2. Collect dataset files\n",
    "dataset_files = get_dataset_files(\n",
    "    DATA_ROOT,\n",
    "    DATASETS,\n",
    "    clusters=CLUSTERS,          # ← NEW ARG\n",
    ")\n",
    "\n",
    "# 5. Verify – point to aggregated cluster outputs\n",
    "completion_files = sorted(\n",
    "    (OUT_DIR / \"completions\" / \"clusters\").glob(\"*_completions.json\")\n",
    ")\n",
    "\n",
    "# 6. Match YES vs NO on cluster files\n",
    "verified_files = sorted((OUT_DIR / \"verified\").glob(\"*_verified.json\"))\n",
    "\n",
    "pairs = [\n",
    "    (vf, vf.parent / vf.name.replace(\"_NO_\", \"_YES_\"))\n",
    "    for vf in verified_files\n",
    "    if \"_NO_\" in vf.name\n",
    "    and (vf.parent / vf.name.replace(\"_NO_\", \"_YES_\")).exists()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "prompt_builder — INFO — Logger initialised; log file = /root/CoTFaithChecker/logs/prompt_builder_20250426_215311.log\n",
      "2025-04-26 21:53:11,953 - INFO - Logger initialised; log file = /root/CoTFaithChecker/logs/prompt_builder_20250426_215311.log\n",
      "prompt_builder — INFO — PromptBuilder initialised with style=instr-v0, mode=cot\n",
      "2025-04-26 21:53:11,954 - INFO - PromptBuilder initialised with style=instr-v0, mode=cot\n",
      "inference — INFO — Logger initialised; log file = /root/CoTFaithChecker/logs/inference_20250426_215311.log\n",
      "2025-04-26 21:53:11,955 - INFO - Logger initialised; log file = /root/CoTFaithChecker/logs/inference_20250426_215311.log\n",
      "inference — INFO — Processing wm-nyc-place-lat_gt_NO_1_089018ac.json\n",
      "2025-04-26 21:53:11,956 - INFO - Processing wm-nyc-place-lat_gt_NO_1_089018ac.json\n",
      "2025-04-26 21:53:11,956 - DEBUG - Loading data/chainscope/questions_json/gt_NO_1/wm-nyc-place-lat_gt_NO_1_089018ac.json\n",
      "2025-04-26 21:53:11,957 - DEBUG -   ↳ run 1/10\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ───────────────────────────────────────────────\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 4. Run inference\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# ───────────────────────────────────────────────\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me_confirm_xy_yx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_inference\n\u001b[0;32m---> 18\u001b[0m \u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_builder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_NEW_TOKENS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_hidden\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAVE_HIDDEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHIDDEN_LAYERS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAVE_ATTN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mATTN_LAYERS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOUT_GEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_runs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_RUNS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTEMPERATURE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOP_P\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CoTFaithChecker/e_confirm_xy_yx/main/inference.py:188\u001b[0m, in \u001b[0;36mrun_inference\u001b[0;34m(dataset_files, prompt_builder, model, tokenizer, model_name, device, batch_size, max_new_tokens, save_hidden, hidden_layers, save_attention, attn_layers, output_dir, n_runs, temperature, top_p)\u001b[0m\n\u001b[1;32m    185\u001b[0m pattern \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_(gt|lt)_(YES|NO)_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fp \u001b[38;5;129;01min\u001b[39;00m dataset_files:\n\u001b[0;32m--> 188\u001b[0m     out_path \u001b[38;5;241m=\u001b[39m \u001b[43m_process_single_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_builder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_hidden\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_attention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_runs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;66;03m# aggregation unchanged …\u001b[39;00m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;241m:=\u001b[39m pattern\u001b[38;5;241m.\u001b[39msearch(fp\u001b[38;5;241m.\u001b[39mstem):\n",
      "File \u001b[0;32m~/CoTFaithChecker/e_confirm_xy_yx/main/inference.py:117\u001b[0m, in \u001b[0;36m_process_single_file\u001b[0;34m(json_path, prompt_builder, model, tokenizer, model_name, device, batch_size, max_new_tokens, save_hidden, hidden_layers, save_attention, attn_layers, output_dir, n_runs, temperature, top_p)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(prompts), batch_size):\n\u001b[1;32m    116\u001b[0m     batch_prompts \u001b[38;5;241m=\u001b[39m prompts[i : i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m--> 117\u001b[0m     gen_out \u001b[38;5;241m=\u001b[39m \u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_hidden\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_attention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     completions\u001b[38;5;241m.\u001b[39mextend(gen_out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletions\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# optional tensor dumps – include run-index in filename so they\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# don’t overwrite each other\u001b[39;00m\n",
      "File \u001b[0;32m~/CoTFaithChecker/e_confirm_xy_yx/main/inference.py:59\u001b[0m, in \u001b[0;36m_generate\u001b[0;34m(model, tokenizer, prompts, device, max_new_tokens, save_hidden, hidden_layers, save_attention, attn_layers, temperature, top_p)\u001b[0m\n\u001b[1;32m     47\u001b[0m gen_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m:     save_hidden,\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m:        save_attention,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m:       top_p,\n\u001b[1;32m     56\u001b[0m }\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 59\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencodings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m completions \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(\n\u001b[1;32m     62\u001b[0m     outputs\u001b[38;5;241m.\u001b[39msequences[:, prompt_len:], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     63\u001b[0m )\n\u001b[1;32m     65\u001b[0m extra: Dict[\u001b[38;5;28mstr\u001b[39m, List] \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2326\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2318\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2319\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2320\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2321\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2322\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2323\u001b[0m     )\n\u001b[1;32m   2325\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2326\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2331\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2333\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2334\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2336\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2337\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2338\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2339\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2340\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2341\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2342\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2343\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:3279\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3276\u001b[0m is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3277\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_unfinished_sequences(this_peer_finished, synced_gpus, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice):\n\u001b[1;32m   3278\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[0;32m-> 3279\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_inputs_for_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3281\u001b[0m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n\u001b[1;32m   3282\u001b[0m     model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_attentions} \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m {})\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:426\u001b[0m, in \u001b[0;36mGenerationMixin.prepare_inputs_for_generation\u001b[0;34m(self, input_ids, past_key_values, attention_mask, inputs_embeds, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m         input_ids \u001b[38;5;241m=\u001b[39m input_ids[:, cache_position]\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# 3. Prepare base model inputs\u001b[39;00m\n\u001b[0;32m--> 426\u001b[0m input_ids_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_encoder_decoder\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# if `inputs_embeds` are passed, we only want to use them in the 1st generation step for every prompt.\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:211\u001b[0m, in \u001b[0;36mPretrainedConfig.__getattribute__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    208\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m)[key]\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(key, value)\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    213\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m)[key]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ───────────────────────────────────────────────\n",
    "# 2. Collect dataset files\n",
    "# ───────────────────────────────────────────────\n",
    "#from e_confirm_xy_yx.main.data_loader import get_dataset_files\n",
    "#dataset_files = get_dataset_files(DATA_ROOT, DATASETS)\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 3. Prepare prompt builder\n",
    "# ───────────────────────────────────────────────\n",
    "from e_confirm_xy_yx.main.prompt_builder import PromptBuilder\n",
    "pb = PromptBuilder(template_path=TEMPLATE_PATH, style=\"instr-v0\", mode=\"cot\")\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 4. Run inference\n",
    "# ───────────────────────────────────────────────\n",
    "from e_confirm_xy_yx.main.inference import run_inference\n",
    "\n",
    "run_inference(\n",
    "    dataset_files=dataset_files,\n",
    "    prompt_builder=pb,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_name=model_name,\n",
    "    device=device,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    save_hidden=SAVE_HIDDEN,\n",
    "    hidden_layers=HIDDEN_LAYERS,\n",
    "    save_attention=SAVE_ATTN,\n",
    "    attn_layers=ATTN_LAYERS,\n",
    "    output_dir=OUT_GEN,\n",
    "    n_runs=N_RUNS,\n",
    "    temperature=TEMPERATURE,\n",
    "    top_p=TOP_P,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# ───────────────────────────────────────────────\\n# 5. Verify model answers\\n# ───────────────────────────────────────────────\\nfrom e_confirm_xy_yx.main.verifier import run_verification\\ncompletion_files = sorted((OUT_DIR / \"completions\").glob(\"*_completions.json\"))\\n\\nrun_verification(\\n    completion_files=completion_files,\\n    n_questions=N_VERIFY,\\n    output_dir=OUT_DIR / \"verified\",\\n)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# ───────────────────────────────────────────────\n",
    "# 5. Verify model answers\n",
    "# ───────────────────────────────────────────────\n",
    "from e_confirm_xy_yx.main.verifier import run_verification\n",
    "completion_files = sorted((OUT_DIR / \"completions\").glob(\"*_completions.json\"))\n",
    "\n",
    "run_verification(\n",
    "    completion_files=completion_files,\n",
    "    n_questions=N_VERIFY,\n",
    "    output_dir=OUT_DIR / \"verified\",\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# ───────────────────────────────────────────────\\n# 6. Cross-match YES vs NO answers\\n# ───────────────────────────────────────────────\\nfrom e_confirm_xy_yx.main.match_checker import check_matches\\nverified_files = sorted((OUT_DIR / \"verified\").glob(\"*_verified.json\"))\\n\\n# pair them: every gt_NO_X file with its matching gt_YES_X (adapt if lt)\\npairs = [\\n    (\\n        vf,\\n        vf.parent\\n        / vf.name.replace(\"gt_NO\", \"gt_YES\")\\n    )\\n    for vf in verified_files\\n    if \"_NO_\" in vf.name\\n]\\n\\nfor no_file, yes_file in pairs:\\n    out_match = (\\n        OUT_DIR\\n        / \"matches\"\\n        / f\"{no_file.stem.replace(\\'_verified\\',\\'\\')}_match.json\"\\n    )\\n    out_match.parent.mkdir(parents=True, exist_ok=True)\\n    check_matches(no_file, yes_file, out_match)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "# ───────────────────────────────────────────────\n",
    "# 6. Cross-match YES vs NO answers\n",
    "# ───────────────────────────────────────────────\n",
    "from e_confirm_xy_yx.main.match_checker import check_matches\n",
    "verified_files = sorted((OUT_DIR / \"verified\").glob(\"*_verified.json\"))\n",
    "\n",
    "# pair them: every gt_NO_X file with its matching gt_YES_X (adapt if lt)\n",
    "pairs = [\n",
    "    (\n",
    "        vf,\n",
    "        vf.parent\n",
    "        / vf.name.replace(\"gt_NO\", \"gt_YES\")\n",
    "    )\n",
    "    for vf in verified_files\n",
    "    if \"_NO_\" in vf.name\n",
    "]\n",
    "\n",
    "for no_file, yes_file in pairs:\n",
    "    out_match = (\n",
    "        OUT_DIR\n",
    "        / \"matches\"\n",
    "        / f\"{no_file.stem.replace('_verified','')}_match.json\"\n",
    "    )\n",
    "    out_match.parent.mkdir(parents=True, exist_ok=True)\n",
    "    check_matches(no_file, yes_file, out_match)\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
