{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2723fab",
   "metadata": {},
   "source": [
    "\n",
    "# Chain‑of‑Thought Faithfulness Analysis\n",
    "\n",
    "*Generated 2025-04-27*\n",
    "\n",
    "This Jupyter notebook implements, end‑to‑end, the pipeline for analysing whether\n",
    "LLM chains‑of‑thought (CoT) are **faithful** to the model’s actual decision\n",
    "process.  It uses only the files already present in your directory tree:\n",
    "\n",
    "```\n",
    "/root/CoTFaithChecker/data/mmlu/\n",
    "    input_mcq_data.json\n",
    "    hints_sycophancy.json\n",
    "    hints_induced_urgency.json\n",
    "    hints_unethical_information.json\n",
    "/root/CoTFaithChecker/g_cot_cluster/outputs/mmlu/\n",
    "    DeepSeek-R1-Distill-Llama-8B/\n",
    "        [...]\n",
    "```\n",
    "\n",
    "You may freely tweak paths, sample sizes, etc.  Each section is heavily\n",
    "commented so you can swap in a different model or dataset with minimal effort.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0baea7",
   "metadata": {},
   "source": [
    "\n",
    "## Notebook Road‑map\n",
    "\n",
    "| Section | Purpose |\n",
    "|---------|---------|\n",
    "| **0. Setup & utility functions** | imports, paths, JSON helpers |\n",
    "| **1. Load data** | questions, hints, completions, *segmented* CoTs |\n",
    "| **2. Tokenizer & char→token mapping** | build category masks |\n",
    "| **3. Model forward pass** | grab *attentions* & *hidden states* |\n",
    "| **4. Descriptive attention metrics** | answer→category shares, entropy, hint Δ |\n",
    "| **5. Representational probing** | PCA / UMAP on category‑pooled hidden states |\n",
    "| **6. Causal ablation tests** | zero‑out reasoning vs. hint tokens, measure flip‑rate |\n",
    "| **7. Visualisation & saving results** | bar plots, scatter, CSV export |\n",
    "\n",
    "> **Tip:** GPU highly recommended.  On an 8‑GB card, process ~100 examples at a\n",
    "> time or switch the model to 8‑bit via `load_in_8bit=True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9c718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SECTION 0 – Imports & configuration\n",
    "from pathlib import Path\n",
    "import json, math, re, itertools, collections, random, string, os, sys, gc, time, hashlib, logging\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Tuple, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(levelname)s %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021eaef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Edit these if your clone lives elsewhere\n",
    "DATA_ROOT   = Path('/root/CoTFaithChecker/data/mmlu')\n",
    "SEG_ROOT    = Path('/root/CoTFaithChecker/g_cot_cluster/outputs/mmlu/DeepSeek-R1-Distill-Llama-8B')\n",
    "MODEL_NAME  = 'deepseek-ai/deepseek-llama-8b'\n",
    "DEVICE      = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print('Using device:', DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c040acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_jsonl(path: Path):\n",
    "    \"\"\"Load conventional JSON array or JSON‑Lines file.\"\"\"\n",
    "    txt = path.read_text(encoding='utf-8')\n",
    "    try:\n",
    "        data = json.loads(txt)\n",
    "    except json.JSONDecodeError:\n",
    "        data = [json.loads(line) for line in txt.splitlines() if line.strip()]\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7fc6ee",
   "metadata": {},
   "source": [
    "### 1. Load questions, hints & completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2ce994",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Questions\n",
    "questions = {d['question_id']: d for d in load_jsonl(DATA_ROOT / 'input_mcq_data.json')}\n",
    "print(f'Loaded {len(questions):,} questions')\n",
    "\n",
    "# Hints by type\n",
    "HINT_TYPES = ['sycophancy', 'induced_urgency', 'unethical_information']\n",
    "hints = {\n",
    "    htype: {d['question_id']: d for d in load_jsonl(DATA_ROOT / f'hints_{htype}.json')}\n",
    "    for htype in HINT_TYPES\n",
    "}\n",
    "print({k: len(v) for k,v in hints.items()})\n",
    "\n",
    "# Verification & switch metadata (per hint type + none)\n",
    "switch_meta = {}\n",
    "for htype in ['none'] + HINT_TYPES:\n",
    "    path = SEG_ROOT.parent / f'DeepSeek-R1-Distill-Llama-8B/{htype}/switch_analysis_with_500.json'\n",
    "    if path.exists():\n",
    "        switch_meta[htype] = {d['question_id']: d for d in load_jsonl(path)}\n",
    "print({k: len(v) for k,v in switch_meta.items()})\n",
    "\n",
    "# Segmented CoTs (correct + incorrect)\n",
    "SEG_PATH = SEG_ROOT / 'correct_indices'   # adjust for correct/incorrect dirs\n",
    "seg_files = sorted(SEG_PATH.glob('*.json'))\n",
    "segmented = {}\n",
    "for fp in seg_files:\n",
    "    data = load_jsonl(fp)\n",
    "    for d in data:\n",
    "        segmented[d['question_id']] = d['segments']\n",
    "print(f'Segmented CoTs loaded for {len(segmented):,} questions')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e13f66",
   "metadata": {},
   "source": [
    "### 2. Tokenisation and category masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f908b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "@dataclass\n",
    "class Sample:\n",
    "    qid: int\n",
    "    prompt: str\n",
    "    completion: str\n",
    "    segments: List[dict]        # as stored\n",
    "    hint_type: str              # 'none' | sycophancy | ...\n",
    "    switched: bool\n",
    "    is_correct: bool\n",
    "    hint_option: str\n",
    "    hint_span: Tuple[int,int]   # char indices within prompt if present\n",
    "\n",
    "    # will be filled later\n",
    "    input_ids: torch.Tensor     = None\n",
    "    category_masks: Dict[str, torch.Tensor] = None\n",
    "    answer_mask: torch.Tensor   = None\n",
    "    hint_token_mask: torch.Tensor = None\n",
    "\n",
    "def build_char_to_tok_map(text: str, enc, input_ids):\n",
    "    \"\"\"Return list len(text) → token index (inclusive end).\"\"\"\n",
    "    # token offsets from fast tokenizer\n",
    "    offsets = enc(text, return_offsets_mapping=True, add_special_tokens=False).offset_mapping\n",
    "    char2tok = [-1]*len(text)\n",
    "    for tok_i, (s,e) in enumerate(offsets):\n",
    "        for c in range(s,e):\n",
    "            char2tok[c] = tok_i\n",
    "    return char2tok\n",
    "\n",
    "def build_masks(sample: Sample):\n",
    "    text = sample.prompt + sample.completion\n",
    "    enc = tokenizer\n",
    "    ids = enc(text, return_tensors='pt', add_special_tokens=False)\n",
    "    sample.input_ids = ids.input_ids.to(DEVICE)\n",
    "    char2tok = build_char_to_tok_map(text, enc, ids.input_ids[0])\n",
    "\n",
    "    # category masks\n",
    "    masks={}\n",
    "    for seg in sample.segments:\n",
    "        cat = seg['phrase_category']\n",
    "        tok_indices = set()\n",
    "        for c in range(seg['start'], seg['end']+1):\n",
    "            ti = char2tok[c]\n",
    "            if ti!=-1:\n",
    "                tok_indices.add(ti)\n",
    "        if not tok_indices:\n",
    "            continue\n",
    "        mask = torch.zeros(ids.input_ids.shape[-1], dtype=torch.bool)\n",
    "        mask[list(tok_indices)] = True\n",
    "        masks.setdefault(cat, torch.zeros_like(mask))\n",
    "        masks[cat] |= mask\n",
    "    sample.category_masks = masks\n",
    "\n",
    "    # answer mask – detect option letter xx inside completion\n",
    "    answer_pattern = re.compile(r\"\\[\\s*([A-D])\\s*\\]\")\n",
    "    m = answer_pattern.search(sample.completion[::-1]) or answer_pattern.search(sample.completion)\n",
    "    if m:\n",
    "        ans_start = len(sample.prompt) + m.start(0)  # crude\n",
    "        indices = set()\n",
    "        for c in range(ans_start, ans_start + len(m.group(0))):\n",
    "            if 0<=c<len(char2tok):\n",
    "                ti = char2tok[c]\n",
    "                if ti!=-1: indices.add(ti)\n",
    "        ans_mask = torch.zeros(ids.input_ids.shape[-1], dtype=torch.bool)\n",
    "        ans_mask[list(indices)] = True\n",
    "        sample.answer_mask = ans_mask\n",
    "    else:\n",
    "        sample.answer_mask = torch.zeros(ids.input_ids.shape[-1], dtype=torch.bool)\n",
    "\n",
    "    # hint mask (span is provided)\n",
    "    if sample.hint_span!=(None,None):\n",
    "        hset=set()\n",
    "        for c in range(sample.hint_span[0], sample.hint_span[1]):\n",
    "            ti=char2tok[c]\n",
    "            if ti!=-1: hset.add(ti)\n",
    "        hmask=torch.zeros(ids.input_ids.shape[-1],dtype=torch.bool)\n",
    "        hmask[list(hset)] = True\n",
    "        sample.hint_token_mask = hmask\n",
    "    else:\n",
    "        sample.hint_token_mask = torch.zeros_like(sample.answer_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b65e47",
   "metadata": {},
   "source": [
    "### 2.b – Build `Sample` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7960435",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SAMPLES=[]\n",
    "for qid, qdata in questions.items():\n",
    "    for htype in ['none'] + HINT_TYPES:\n",
    "        # build prompt+completion text\n",
    "        if htype=='none':\n",
    "            comp_path = SEG_ROOT.parent / 'none' / 'completions_with_500.json'\n",
    "        else:\n",
    "            comp_path = SEG_ROOT.parent / htype / 'completions_with_500.json'\n",
    "        if not comp_path.exists(): continue\n",
    "        comp_map = {d['question_id']: d['completion'] for d in load_jsonl(comp_path)}\n",
    "        if qid not in comp_map: continue\n",
    "        completion = comp_map[qid]\n",
    "\n",
    "        # find segments\n",
    "        segs = segmented.get(qid, [])\n",
    "        # identify hint span in the prompt, if any\n",
    "        hint_data = hints.get(htype, {}).get(qid)\n",
    "        if hint_data:\n",
    "            hint_text = hint_data['hint_text']\n",
    "            prompt = completion.split('assistant')[0].split('user')[1]  # approximate; adjust if needed\n",
    "            span_start = prompt.find(hint_text)\n",
    "            span_end   = span_start + len(hint_text) if span_start!=-1 else (None, None)\n",
    "            hint_span=(span_start, span_end)\n",
    "            hint_option=hint_data['hint_option']\n",
    "        else:\n",
    "            prompt = completion.split('assistant')[0].split('user')[1]\n",
    "            hint_span=(None,None)\n",
    "            hint_option=None\n",
    "\n",
    "        sw = switch_meta.get(htype,{}).get(qid, {})\n",
    "        sample = Sample(\n",
    "            qid=qid,\n",
    "            prompt=prompt,\n",
    "            completion=completion,\n",
    "            segments=segs,\n",
    "            hint_type=htype,\n",
    "            switched=sw.get('switched', False),\n",
    "            is_correct=not sw.get('switched', False) if htype=='none' else sw.get('is_correct_option', False),\n",
    "            hint_option=hint_option,\n",
    "            hint_span=hint_span\n",
    "        )\n",
    "        build_masks(sample)\n",
    "        SAMPLES.append(sample)\n",
    "print(f'Total Sample objects: {len(SAMPLES):,}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5430a52e",
   "metadata": {},
   "source": [
    "### 3. Load the Llama‑8B model (8‑bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf4563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bnb = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=6.0)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map='auto',\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             quantization_config=bnb)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fd7e43",
   "metadata": {},
   "source": [
    "### 4. Forward pass helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4492d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def forward_with_cache(sample: Sample, layers=None):\n",
    "    \"\"\"Run one sample, return (rolled_attention, hidden_states).\"\"\"\n",
    "    outs = model(sample.input_ids,\n",
    "                 output_attentions=True,\n",
    "                 output_hidden_states=True,\n",
    "                 use_cache=False)\n",
    "    attns = torch.stack(outs.attentions)     # (L, H, T, T)\n",
    "    if layers is not None:\n",
    "        attns = attns[layers]\n",
    "    # attention rollout across layers\n",
    "    rolled = attns[0]\n",
    "    for l in range(1, attns.size(0)):\n",
    "        rolled = rolled @ attns[l]\n",
    "    hidden = torch.stack(outs.hidden_states) # (L+1, T, d)\n",
    "    return rolled, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e42ec9",
   "metadata": {},
   "source": [
    "### 4.b – Attention metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8c77b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def answer_category_shares(rolled_attn: torch.Tensor, sample: Sample):\n",
    "    # rolled_attn: (H, T, T)\n",
    "    shares={}\n",
    "    ans_idx = sample.answer_mask.nonzero(as_tuple=False).squeeze(1)\n",
    "    if ans_idx.nelement()==0: return shares\n",
    "    total = rolled_attn[:, ans_idx].sum().item()\n",
    "    for cat, mask in sample.category_masks.items():\n",
    "        w = rolled_attn[:, ans_idx][:,:, mask].sum().item()\n",
    "        shares[cat] = w/total if total>0 else 0.0\n",
    "    if sample.hint_token_mask.any():\n",
    "        w = rolled_attn[:, ans_idx][:,:, sample.hint_token_mask].sum().item()\n",
    "        shares['HINT'] = w/total if total>0 else 0.0\n",
    "    return shares\n",
    "\n",
    "def attention_entropy(shares: Dict[str,float]):\n",
    "    p = np.array(list(shares.values()))\n",
    "    p = p[p>0]\n",
    "    return -np.sum(p*np.log2(p)) if p.size else np.nan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43457118",
   "metadata": {},
   "source": [
    "### 5. Representational probing (PCA / UMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b4987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pooled_hidden(hidden: torch.Tensor, sample: Sample):\n",
    "    # hidden: (L+1, T, d); take final layer\n",
    "    h = hidden[-1]         # (T, d)\n",
    "    pools={}\n",
    "    for cat, mask in sample.category_masks.items():\n",
    "        if mask.any():\n",
    "            pools[cat] = h[mask].mean(0).cpu().numpy()\n",
    "    if sample.hint_token_mask.any():\n",
    "        pools['HINT'] = h[sample.hint_token_mask].mean(0).cpu().numpy()\n",
    "    return pools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871344f9",
   "metadata": {},
   "source": [
    "### 6. Causal ablation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993c0cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def regenerate_answer(sample: Sample, edited_hidden=None, answer_len=3):\n",
    "    \"\"\"Feed edited hidden state as past‑kv and regenerate answer tokens.\"\"\"\n",
    "    # For brevity, we regenerate full output; practical use would stop at answer.\n",
    "    if edited_hidden is not None:\n",
    "        raise NotImplementedError('Hidden‑state surgery demo left as TODO')\n",
    "    out = model.generate(sample.input_ids.to(DEVICE),\n",
    "                         max_new_tokens=answer_len,\n",
    "                         do_sample=False)\n",
    "    return tokenizer.decode(out[0][-answer_len:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4005efc4",
   "metadata": {},
   "source": [
    "### 7. Compute metrics for a subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2098f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "records=[]\n",
    "SAMPLE_LIMIT = 200   # change as needed\n",
    "for i,sample in enumerate(random.sample(SAMPLES, min(SAMPLE_LIMIT, len(SAMPLES)))):\n",
    "    rolled, hidden = forward_with_cache(sample, layers=None)\n",
    "    shares = answer_category_shares(rolled, sample)\n",
    "    ent   = attention_entropy(shares)\n",
    "    pools = pooled_hidden(hidden, sample)\n",
    "    record = dict(\n",
    "        qid      = sample.qid,\n",
    "        hint_type= sample.hint_type,\n",
    "        switched = sample.switched,\n",
    "        correct  = sample.is_correct,\n",
    "        entropy  = ent,\n",
    "        shares   = shares,\n",
    "    )\n",
    "    # flatten shares\n",
    "    for k,v in shares.items():\n",
    "        record[f'share_{k}'] = v\n",
    "    records.append(record)\n",
    "    if (i+1)%20==0:\n",
    "        print(f'Processed {i+1} / {SAMPLE_LIMIT}')\n",
    "metrics_df = pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc04d5f",
   "metadata": {},
   "source": [
    "### 8. Visualisation examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aee9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# Barplot of mean answer→category share by correctness\n",
    "melt = metrics_df.melt(id_vars=['correct','hint_type'],\n",
    "                       value_vars=[c for c in metrics_df.columns if c.startswith('share_')],\n",
    "                       var_name='category', value_name='share')\n",
    "melt['category']=melt['category'].str.replace('share_','')\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.barplot(data=melt, x='category', y='share', hue='correct', ci='sd')\n",
    "plt.title('Answer‑to‑Category Attention Share')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3899140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build pooled vectors\n",
    "vecs=[]; labels=[]\n",
    "for s in random.sample(SAMPLES, min(500,len(SAMPLES))):\n",
    "    rolled, hidden = forward_with_cache(s, layers=None)\n",
    "    pools = pooled_hidden(hidden, s)\n",
    "    if 'logical_deduction' in pools:\n",
    "        vecs.append(pools['logical_deduction'])\n",
    "        labels.append('correct' if s.is_correct else 'wrong')\n",
    "X = np.stack(vecs)\n",
    "pca = PCA(n_components=2).fit_transform(X)\n",
    "plt.figure(figsize=(6,6))\n",
    "for lab in set(labels):\n",
    "    idx=[i for i,l in enumerate(labels) if l==lab]\n",
    "    plt.scatter(pca[idx,0], pca[idx,1], label=lab, alpha=0.7, s=20)\n",
    "plt.legend(); plt.title('PCA of logical_deduction pooled hidden states')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937edb7d",
   "metadata": {},
   "source": [
    "### 9. Save metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e258a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OUT_CSV = Path('cot_attention_metrics.csv')\n",
    "metrics_df.to_csv(OUT_CSV, index=False)\n",
    "print('Saved', OUT_CSV.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1cdf8a",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Next steps & TODOs\n",
    "\n",
    "* Increase `SAMPLE_LIMIT` or implement batch processing to cover the full set.\n",
    "* Complete the *causal ablation* (`regenerate_answer`) using hidden‑state\n",
    "  zero‑out or prompt masking.\n",
    "* Add entropy/hint‑Δ plots and statistical tests (e.g. paired t‑test).\n",
    "* Swap PCA for UMAP if non‑linear structure is suspected.\n",
    "* Profile GPU memory – consider slicing attention layers or using\n",
    "  `torch.utils.checkpoint` for bigger batches.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
