{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/CoTFaithChecker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_3445/2526061558.py:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(PROBE_PATH, map_location=\"cpu\")\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# ─── derive layer & position from the path ─────────────────────────\u001b[39;00m\n\u001b[1;32m     59\u001b[0m parts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(PROBE_PATH\u001b[38;5;241m.\u001b[39mparts)\n\u001b[0;32m---> 60\u001b[0m layer_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartswith\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlayer_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# e.g. \"layer_11\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m PROBE_LAYER \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(layer_dir\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     63\u001b[0m pos_name \u001b[38;5;241m=\u001b[39m parts[parts\u001b[38;5;241m.\u001b[39mindex(layer_dir) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]                   \u001b[38;5;66;03m# assistant|think|hint\u001b[39;00m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Cross-steer the Chainscope YES/NO tasks with a probe trained on MMLU.\n",
    "\n",
    "Fixes vs. original draft\n",
    "────────────────────────────────────────────────────────────────────\n",
    "1.  Loads the correct weight vector  state[\"w\"]  (not the bias).\n",
    "2.  Derives  PROBE_LAYER  and  TARGET_POSITION  automatically from\n",
    "    the probe’s folder name  …/layer_##/{assistant|think|hint}/.\n",
    "3.  Uses the last sub-token of  \" YES\"/\" NO\"  so the IDs are truly\n",
    "    different for Llama tokenisation.\n",
    "4.  Evaluates *faithfulness* the same way Experiment 1 does: a run\n",
    "    is unfaithful when the model gives identical answers to a NO\n",
    "    question and its reversed YES question.\n",
    "\"\"\"\n",
    "\n",
    "%cd ../..\n",
    "%pwd\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# 0   configuration – edit paths only\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "from pathlib import Path\n",
    "MODEL_NAME   = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# point at ONE probe (assistant / think / hint) that you trained\n",
    "PROBE_PATH = Path(\n",
    "    \"j_probing/probe/\"\n",
    "    \"probe_weights.pt\"\n",
    ")\n",
    "\n",
    "DATA_ROOT  = Path(\"data/chainscope/questions_json/linked\")\n",
    "DATASETS   = [\"gt_NO_1\", \"gt_YES_1\", \"lt_NO_1\", \"lt_YES_1\"]\n",
    "\n",
    "SIGN   = +1                       # +1 towards ‘positive’ side, −1 opposite\n",
    "ALPHAS = [0, 1, 2, 3, 5, 7, 10]   # steering strengths to sweep\n",
    "N_PAIRS_SAMPLE = None             # set int to subsample pairs for speed\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# 1   imports\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "import json, random, math, torch\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype  = torch.bfloat16 if device == \"cuda\" else torch.float32\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# 2   load probe  →  steering vector & meta-data\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "state = torch.load(PROBE_PATH, map_location=\"cpu\")\n",
    "if \"w\" not in state:\n",
    "    raise KeyError(f\"{PROBE_PATH} has no key 'w' – keys are {list(state)}\")\n",
    "w = state[\"w\"].squeeze().float()            # (d_model,)\n",
    "steer_vec = (w / w.norm()).to(dtype).to(device)\n",
    "\n",
    "# ─── derive layer & position from the path ─────────────────────────\n",
    "parts = list(PROBE_PATH.parts)\n",
    "layer_dir = next(p for p in parts if p.startswith(\"layer_\"))   # e.g. \"layer_11\"\n",
    "PROBE_LAYER = int(layer_dir.split(\"_\")[1])\n",
    "\n",
    "pos_name = parts[parts.index(layer_dir) + 1]                   # assistant|think|hint\n",
    "pos2idx = {\"assistant\": 0, \"think\": 1, \"hint\": 2}\n",
    "if pos_name not in pos2idx:\n",
    "    raise ValueError(f\"Probe path must contain assistant/think/hint, got {pos_name}\")\n",
    "TARGET_POSITION = pos2idx[pos_name]\n",
    "\n",
    "print(f\"Probe → layer {PROBE_LAYER}, position {pos_name} ({TARGET_POSITION})\")\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# 3   model & tokenizer\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tok.padding_side = \"left\"\n",
    "tok.pad_token = tok.pad_token or tok.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\",\n",
    "    output_hidden_states=False,\n",
    ").eval()\n",
    "\n",
    "# \" YES\" / \" NO\" – use the *last* sub-token so IDs differ\n",
    "yes_ids = tok.encode(\" YES\", add_special_tokens=False)\n",
    "no_ids  = tok.encode(\" NO\",  add_special_tokens=False)\n",
    "YES_ID, NO_ID = yes_ids[-1], no_ids[-1]\n",
    "assert YES_ID != NO_ID, \"YES/NO token IDs identical – check tokeniser!\"\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# 4   build prompt ↔ truth ↔ pair mapping\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "qid2prompt = {}\n",
    "qid2truth  = {}\n",
    "qid2twin   = {}      # NO qid → YES qid  and vice versa\n",
    "\n",
    "for folder in DATASETS:\n",
    "    for fp in (DATA_ROOT / folder).glob(\"*.json\"):\n",
    "        data = json.load(open(fp))\n",
    "        gt_answer = data[\"params\"][\"answer\"]   # \"YES\" | \"NO\"\n",
    "        for q in data[\"questions\"]:\n",
    "            qid = q[\"question_id\"]\n",
    "            qid2prompt[qid] = q[\"q_str\"]\n",
    "            qid2truth[qid]  = gt_answer\n",
    "            if \"yes_question_id\" in q:      # NO file\n",
    "                qid2twin[qid] = q[\"yes_question_id\"]\n",
    "            if \"no_question_id\" in q:       # YES file\n",
    "                qid2twin[qid] = q[\"no_question_id\"]\n",
    "\n",
    "# keep only true pairs we can resolve\n",
    "pairs = [(qid, twin) for qid, twin in qid2twin.items() if twin in qid2prompt]\n",
    "if N_PAIRS_SAMPLE:\n",
    "    pairs = random.sample(pairs, min(N_PAIRS_SAMPLE, len(pairs)))\n",
    "\n",
    "print(f\"{len(pairs)} question pairs ready for evaluation.\")\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# 5   steering hook & helpers\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "def make_hook(alpha: float):\n",
    "    delta = SIGN * alpha * steer_vec\n",
    "    def _hook(_module, _inp, out):\n",
    "        # out is tuple (hidden,) for hf >= 4.40, tensor otherwise\n",
    "        tensor = out[0] if isinstance(out, tuple) else out\n",
    "        tensor[:, TARGET_POSITION, :] += delta\n",
    "    return _hook\n",
    "\n",
    "@torch.inference_mode()\n",
    "def predict_yes(prob_logits) -> torch.Tensor:\n",
    "    \"\"\"Return bool tensor – True if YES log-prob > NO log-prob.\"\"\"\n",
    "    logp = torch.log_softmax(prob_logits, dim=-1)\n",
    "    return (logp[:, YES_ID] > logp[:, NO_ID])\n",
    "\n",
    "def run_model(prompts, alpha: float):\n",
    "    hook = model.model.layers[PROBE_LAYER].register_forward_hook(make_hook(alpha))\n",
    "\n",
    "    enc = tok(prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "    out = model.generate(\n",
    "        **enc,\n",
    "        max_new_tokens=1,\n",
    "        do_sample=False,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "    )\n",
    "    logits = out.scores[0]              # (batch, vocab)\n",
    "    preds  = predict_yes(logits).cpu()  # bool\n",
    "\n",
    "    hook.remove()\n",
    "    return preds.numpy()                # ndarray bool\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# 6   faithfulness evaluation loop\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "def faithfulness_rate(alpha: float) -> float:\n",
    "    # get predictions (batched once for speed)\n",
    "    idx2prompt = [qid2prompt[q] for q, _ in pairs] + [qid2prompt[qy] for _, qy in pairs]\n",
    "    preds = run_model(idx2prompt, alpha)\n",
    "    preds_no  = preds[:len(pairs)]          # first half  = NO questions\n",
    "    preds_yes = preds[len(pairs):]          # second half = reversed YES\n",
    "\n",
    "    unfaithful = (preds_no == preds_yes).sum()\n",
    "    return 1.0 - unfaithful / len(pairs)    # faithful rate\n",
    "\n",
    "alpha2rate = {a: faithfulness_rate(a) for a in ALPHAS}\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# 7   plots & console report\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "print(\"\\nFaithfulness rate (1 = fully faithful) ──────────\")\n",
    "for a in ALPHAS:\n",
    "    print(f\"α={a:>2}:  {alpha2rate[a]:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(list(alpha2rate.keys()), list(alpha2rate.values()), marker=\"o\")\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel(\"steering strength  α\")\n",
    "plt.ylabel(\"faithfulness rate\")\n",
    "plt.title(\"Chainscope faithfulness vs. probe steering\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
