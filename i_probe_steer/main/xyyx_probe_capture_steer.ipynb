{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Linear-probe trainer for the XY⇄YX faithfulness experiment.\n",
    "\n",
    "Targets:\n",
    "    y = 1 … model answered correctly (YES when YES expected, NO when NO expected)\n",
    "    y = 0 … model gave the *same* answer to both variants and this is the\n",
    "            *wrong* variant (YES instead of NO, or NO instead of YES)\n",
    "\n",
    "All bookkeeping comes from the answer JSONs – we never peek at `lm_head`.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, re, json, math, random, time, socket, pathlib\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "import torch, joblib\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib; matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_curve, auc\n",
    "\n",
    "from accelerate import Accelerator\n",
    "%cd ../..\n",
    "%pwd\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# CONFIG\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "ANSWERS_DIRS      = [\n",
    "    Path(\"e_confirm_xy_yx/outputs/matched_vals_gt\"),\n",
    "    Path(\"e_confirm_xy_yx/outputs/matched_vals_lt\"),\n",
    "]\n",
    "ACTIVATIONS_DIR   = Path(\"h_hidden_space/outputs/f1_hint_xyyx/xyyx_deterministic\")\n",
    "QUESTION_JSON_DIR = Path(\"data/chainscope/questions_json\")\n",
    "\n",
    "PROBE_SAVE_DIR    = Path(\"linear_probes/realyesno_5k\")  # results end up here\n",
    "PROBE_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "INFERENCE_BATCH_SIZE   = 32   # must match the batch size used in `run_inference`\n",
    "MAX_SAMPLES_PER_LAYER  = 1_000\n",
    "PRINT_EVERY_LAYERS     = 5\n",
    "RANDOM_SEED            = 0\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "acc       = Accelerator()\n",
    "DEVICE    = acc.device\n",
    "is_main   = acc.is_main_process\n",
    "p         = acc.print                                                    # rank-aware print\n",
    "_now      = lambda: time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "\n",
    "# │ 1.  build a QUESTION-ID  →  {expected, actual, same} lookup\n",
    "answers_map: dict[int, dict[str, object]] = {}\n",
    "\n",
    "def load_answers():\n",
    "    for dir_ in ANSWERS_DIRS:\n",
    "        for fp in dir_.glob(\"*.json\"):\n",
    "            with open(fp) as f:\n",
    "                raw = json.load(f)\n",
    "            questions = raw[\"questions\"] if isinstance(raw, dict) else raw\n",
    "            for q in questions:\n",
    "                same_flag = q[\"same\"][0] if isinstance(q[\"same\"], list) else q[\"same\"]\n",
    "                q_id_no   = q[\"question_id\"]\n",
    "                q_id_yes  = q[\"question_yes_id\"]\n",
    "                answers_map[q_id_no]  = {\"expected\": \"NO\",  \"actual\": q[\"a_answers\"][0],\n",
    "                                         \"same\": same_flag}\n",
    "                answers_map[q_id_yes] = {\"expected\": \"YES\", \"actual\": q[\"b_answers\"][0],\n",
    "                                         \"same\": same_flag}\n",
    "\n",
    "if is_main:\n",
    "    print(f\"[{_now()}] loading answer files …\")\n",
    "load_answers()\n",
    "acc.wait_for_everyone()\n",
    "\n",
    "# optional quick sanity print\n",
    "if is_main:\n",
    "    cnt = Counter((v[\"expected\"], v[\"actual\"]) for v in answers_map.values())\n",
    "    print(\"answers_map built:\",\n",
    "          \", \".join([f\"{k}:{v:,}\" for k,v in cnt.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ╭──────────────────────────────────────────────────────────────────────────╮\n",
    "# │ 2.  utilities to map a hidden-file row → question-id                    │\n",
    "# ╰──────────────────────────────────────────────────────────────────────────╯\n",
    "_DATASET_CACHE: dict[str, list[dict]] = {}\n",
    "\n",
    "def get_dataset_questions(dataset_stem: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Returns the list of question dicts for a dataset, caching across calls.\n",
    "    `dataset_stem` is, e.g.,  wm-book-length_gt_NO_1_6fda02e3\n",
    "    \"\"\"\n",
    "    if dataset_stem in _DATASET_CACHE:\n",
    "        return _DATASET_CACHE[dataset_stem]\n",
    "\n",
    "    fp = QUESTION_JSON_DIR / f\"{dataset_stem}.json\"\n",
    "    if not fp.exists():\n",
    "        raise FileNotFoundError(f\"Cannot locate dataset JSON {fp}\")\n",
    "\n",
    "    with open(fp) as f:\n",
    "        raw = json.load(f)\n",
    "    questions = raw[\"questions\"] if isinstance(raw, dict) else raw\n",
    "    _DATASET_CACHE[dataset_stem] = questions\n",
    "    return questions\n",
    "\n",
    "_BATCH_RE = re.compile(r\"_batch(\\d+)_hidden\\.pt$\")\n",
    "\n",
    "def question_ids_for_hidden_file(hid_path: Path, batch_size: int, actual_batch_len: int) -> list[int]:\n",
    "    \"\"\"\n",
    "    Returns the list of question-ids (length = actual_batch_len) that the hidden\n",
    "    tensor rows correspond to.\n",
    "    \"\"\"\n",
    "    m = _BATCH_RE.search(hid_path.name)\n",
    "    if m is None:\n",
    "        raise ValueError(f\"Bad hidden filename: {hid_path.name}\")\n",
    "    batch_idx  = int(m.group(1))\n",
    "    dataset_stem = hid_path.name[:m.start()]           # strip “…_batchX_hidden.pt”\n",
    "    q_list     = get_dataset_questions(dataset_stem)\n",
    "    start      = batch_idx * batch_size\n",
    "    return [q_list[start + j][\"question_id\"]           # key name in those JSONs\n",
    "            for j in range(actual_batch_len)]\n",
    "\n",
    "# ╭──────────────────────────────────────────────────────────────────────────╮\n",
    "# │ 3.  collect (h_vec, label) pairs for every layer                         │\n",
    "# ╰──────────────────────────────────────────────────────────────────────────╯\n",
    "layer_buckets: dict[int, list[tuple[torch.Tensor, int]]] = defaultdict(list)\n",
    "\n",
    "all_hidden_paths = sorted(ACTIVATIONS_DIR.rglob(\"*_hidden.pt\"))\n",
    "#   shard the work across accelerator ranks\n",
    "hidden_paths = all_hidden_paths[acc.process_index::acc.num_processes]\n",
    "\n",
    "if is_main:\n",
    "    print(f\"[{_now()}] • {len(all_hidden_paths):,} hidden files total\")\n",
    "    print(f\"[{_now()}] • {len(hidden_paths):,} files on rank-{acc.process_index}\")\n",
    "\n",
    "progress = tqdm(hidden_paths, desc=\"collect\", disable=not is_main)\n",
    "for hid_fp in progress:\n",
    "    batch_hidden: list[torch.Tensor] = torch.load(hid_fp)   # list[n_layers]  each (B,H)\n",
    "    batch_len     = batch_hidden[0].size(0)\n",
    "    q_ids         = question_ids_for_hidden_file(hid_fp, INFERENCE_BATCH_SIZE, batch_len)\n",
    "\n",
    "    # outer loop: row-in-batch\n",
    "    for row_idx, q_id in enumerate(q_ids):\n",
    "        meta = answers_map.get(q_id)\n",
    "        if meta is None:                 # happens if you didn't copy both gt & lt directories\n",
    "            continue\n",
    "\n",
    "        correct = (meta[\"actual\"] == meta[\"expected\"])\n",
    "        if not correct and not meta[\"same\"]:\n",
    "            continue                     # skip “ordinary” mistakes – we only want same-answer errors\n",
    "\n",
    "        label = 1 if correct else 0\n",
    "        for L, layer_tensor in enumerate(batch_hidden):\n",
    "            layer_buckets[L].append((layer_tensor[row_idx].float(), label))\n",
    "\n",
    "progress.close()\n",
    "acc.wait_for_everyone()\n",
    "\n",
    "bucket_shards = acc.gather_object(layer_buckets)   # list[dict] – 1 per rank\n",
    "\n",
    "if not is_main:\n",
    "    # free some memory and exit early\n",
    "    del layer_buckets\n",
    "    return\n",
    "\n",
    "# merge rank shards\n",
    "merged: dict[int, list[tuple[torch.Tensor,int]]] = defaultdict(list)\n",
    "for shard in bucket_shards:\n",
    "    for L, pairs in shard.items():\n",
    "        merged[L].extend(pairs)\n",
    "layer_buckets = merged\n",
    "\n",
    "print(f\"[{_now()}] gathered {sum(len(v) for v in layer_buckets.values()):,} labelled samples\")\n",
    "\n",
    "# ╭──────────────────────────────────────────────────────────────────────────╮\n",
    "# │ 4.  layer-wise logistic-regression probes                                │\n",
    "# ╰──────────────────────────────────────────────────────────────────────────╯\n",
    "results = {}\n",
    "t_global = time.time()\n",
    "for L in tqdm(sorted(layer_buckets), desc=\"train probes\"):\n",
    "    pairs = layer_buckets[L]\n",
    "    if MAX_SAMPLES_PER_LAYER and len(pairs) > MAX_SAMPLES_PER_LAYER:\n",
    "        pairs = random.sample(pairs, MAX_SAMPLES_PER_LAYER)\n",
    "\n",
    "    X = torch.stack([p[0] for p in pairs]).numpy()\n",
    "    y = np.array([p[1] for p in pairs])\n",
    "\n",
    "    if len(np.unique(y)) < 2:            # all-positive or all-negative – skip\n",
    "        continue\n",
    "\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
    "    )\n",
    "\n",
    "    probe = LogisticRegression(\n",
    "        penalty=\"l2\", solver=\"saga\",\n",
    "        max_iter=10_000, n_jobs=-1, verbose=0\n",
    "    ).fit(X_tr, y_tr)\n",
    "    f1 = f1_score(y_val, probe.predict(X_val))\n",
    "    results[L] = (f1, probe)\n",
    "\n",
    "best_layer, (best_f1, best_probe) = max(results.items(), key=lambda kv: kv[1][0])\n",
    "out_path = PROBE_SAVE_DIR / f\"linear_probe_layer{best_layer}.joblib\"\n",
    "joblib.dump(best_probe, out_path)\n",
    "print(f\"[{_now()}] best layer = {best_layer}  (val-F1 {best_f1:.3f})  → saved →  {out_path}\")\n",
    "\n",
    "# ╭──────────────────────────────────────────────────────────────────────────╮\n",
    "# │ 5.  quick diagnostics                                                    │\n",
    "# ╰──────────────────────────────────────────────────────────────────────────╯\n",
    "layers = sorted(results)\n",
    "f1s    = [results[L][0] for L in layers]\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(layers, f1s, marker=\"o\"); plt.grid(True)\n",
    "plt.title(\"Layer-wise validation F1\"); plt.xlabel(\"layer\"); plt.ylabel(\"F1\")\n",
    "plt.tight_layout(); plt.savefig(PROBE_SAVE_DIR / \"layer_f1_curve.png\"); plt.close()\n",
    "\n",
    "sample_counts = [len(layer_buckets[L]) for L in layers]\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.bar(layers, sample_counts)\n",
    "plt.title(\"#samples per layer\"); plt.xlabel(\"layer\"); plt.ylabel(\"count\")\n",
    "plt.tight_layout(); plt.savefig(PROBE_SAVE_DIR / \"layer_sample_counts.png\"); plt.close()\n",
    "\n",
    "# confusion & ROC for best layer\n",
    "pairs_all = layer_buckets[best_layer]\n",
    "X_all     = torch.stack([p[0] for p in pairs_all]).numpy()\n",
    "y_all     = np.array([p[1] for p in pairs_all])\n",
    "y_pred    = best_probe.predict(X_all)\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "cm = confusion_matrix(y_all, y_pred)\n",
    "plt.imshow(cm, cmap=\"Blues\")\n",
    "for (i,j),v in np.ndenumerate(cm):\n",
    "    plt.text(j,i,f\"{v:,}\",ha=\"center\",va=\"center\")\n",
    "plt.xticks([0,1],[\"wrong\",\"correct\"]); plt.yticks([0,1],[\"wrong\",\"correct\"])\n",
    "plt.tight_layout(); plt.savefig(PROBE_SAVE_DIR / f\"confusion_matrix_layer{best_layer}.png\"); plt.close()\n",
    "\n",
    "probs = (best_probe.predict_proba(X_all)[:,1]\n",
    "         if hasattr(best_probe,\"predict_proba\")\n",
    "         else best_probe.decision_function(X_all))\n",
    "fpr,tpr,_ = roc_curve(y_all, probs); roc_auc = auc(fpr,tpr)\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(fpr,tpr); plt.plot([0,1],[0,1],\"--\")\n",
    "plt.title(f\"ROC AUC {roc_auc:.3f}  (layer {best_layer})\")\n",
    "plt.tight_layout(); plt.savefig(PROBE_SAVE_DIR / f\"roc_layer{best_layer}.png\"); plt.close()\n",
    "\n",
    "print(f\"[{_now()}] all done – diagnostics written to {PROBE_SAVE_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
