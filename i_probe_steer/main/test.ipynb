{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/CoTFaithChecker\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'glob'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlist_pt\u001b[39m(directory):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mstr\u001b[39m(p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m directory\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 23\u001b[0m files_none  \u001b[38;5;241m=\u001b[39m \u001b[43mlist_pt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDIR_NONE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m files_sync  \u001b[38;5;241m=\u001b[39m list_pt(DIR_unverb)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m files_none \u001b[38;5;129;01mand\u001b[39;00m files_sync, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo .pt files found ! check paths!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[1], line 21\u001b[0m, in \u001b[0;36mlist_pt\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlist_pt\u001b[39m(directory):\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mstr\u001b[39m(p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdirectory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglob\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'glob'"
     ]
    }
   ],
   "source": [
    "%cd ../..\n",
    "%pwd\n",
    "import os, pickle, datetime, re\n",
    "from pathlib import Path\n",
    "\n",
    "import torch, numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\"\"\"ROOT = Path(\"i_probe_steer/outputs/f1_hint_xyyx/hints/mmlu/DeepSeek-R1-Distill-Llama-8B\")\n",
    "ROOT = Path(\"i_probe_steer/outputs/f1_hint_xyyx/hints/mmlu/DeepSeek-R1-Distill-Llama-8B\")\n",
    "DIR_NONE        = ROOT / \"none/500_captures\"\n",
    "DIR_unverb  = ROOT / \"sycophancy/500_captures\"\"\"\n",
    "\n",
    "DIR_NONE   = Path(\"i_probe_steer/outputs/f1_hint_xyyx/hints/mmlu/DeepSeek-R1-Distill-Llama-8B/none/500_captures\")\n",
    "DIR_unverb = Path(\"i_probe_steer/outputs/f1_hint_xyyx/hints/mmlu/DeepSeek-R1-Distill-Llama-8B/sycophancy/500_captures\")\n",
    "\n",
    "def list_pt(directory):\n",
    "    return sorted(str(p) for p in directory.glob(\"*.pt\"))\n",
    "\n",
    "files_none  = list_pt(DIR_NONE)\n",
    "files_sync  = list_pt(DIR_unverb)\n",
    "assert files_none and files_sync, \"No .pt files found ! check paths!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_token_acts_batch_0.pt    first_token_acts_batch_320.pt\n",
      "first_token_acts_batch_128.pt  first_token_acts_batch_352.pt\n",
      "first_token_acts_batch_160.pt  first_token_acts_batch_384.pt\n",
      "first_token_acts_batch_192.pt  first_token_acts_batch_416.pt\n",
      "first_token_acts_batch_224.pt  first_token_acts_batch_448.pt\n",
      "first_token_acts_batch_256.pt  first_token_acts_batch_480.pt\n",
      "first_token_acts_batch_288.pt  first_token_acts_batch_64.pt\n",
      "first_token_acts_batch_32.pt   first_token_acts_batch_96.pt\n"
     ]
    }
   ],
   "source": [
    "%ls i_probe_steer/outputs/f1_hint_xyyx/hints/mmlu/DeepSeek-R1-Distill-Llama-8B/sycophancy/500_captures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pointer_re = re.compile(br\"^version https://git-lfs.github.com/spec/\")\n",
    "def safe_torch_load(fname):\n",
    "    with open(fname, \"rb\") as f:\n",
    "        if pointer_re.match(f.read(80)):\n",
    "            print(f\"{Path(fname).name}: Git-LFS pointer, skipping\")\n",
    "            return None\n",
    "    return torch.load(fname, map_location=\"cpu\")\n",
    "\n",
    "def dict_to_list(d):\n",
    "    return [d[f\"layer_{i}\"] for i in range(len(d))]\n",
    "def normalise_batch(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return dict_to_list(obj)\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return list(obj)\n",
    "    raise TypeError(f\"Unexpected batch type: {type(obj)}\")\n",
    "\n",
    "sample = None\n",
    "for fp in files_none + files_sync:\n",
    "    raw = safe_torch_load(fp)\n",
    "    if raw is not None:\n",
    "        sample = normalise_batch(raw)\n",
    "        break\n",
    "\n",
    "assert sample is not None, \"No real .pt blobs present – run `git lfs pull`\"\n",
    "N_LAYERS     = len(sample)\n",
    "HIDDEN_SIZE  = sample[0].shape[-1]\n",
    "print(f\"Detected {N_LAYERS} layers, hidden size = {HIDDEN_SIZE}\")\n",
    "\n",
    "\n",
    "layer_blobs = {L: [] for L in range(N_LAYERS)}\n",
    "labels      = []\n",
    "\n",
    "def add_files(file_list, lab):\n",
    "    for fp in tqdm(file_list, desc=f\"label={lab}\"):\n",
    "        raw = safe_torch_load(fp)\n",
    "        if raw is None:\n",
    "            continue\n",
    "        batch = normalise_batch(raw)\n",
    "        B = batch[0].shape[0]\n",
    "        for L, h in enumerate(batch):\n",
    "            layer_blobs[L].append(h.float().numpy())\n",
    "        labels.extend([lab]*B)\n",
    "\n",
    "add_files(files_none,  0)   # 0 = none\n",
    "add_files(files_sync,  1)   # 1 = unverb\n",
    "\n",
    "assert labels, \"No usable data loaded.\"\n",
    "labels = np.asarray(labels, dtype=np.int8)\n",
    "print(\"Total samples:\", len(labels))\n",
    "\n",
    "layer_X = {L: np.concatenate(layer_blobs[L], axis=0) for L in layer_blobs}\n",
    "del layer_blobs\n",
    "\n",
    "scalers = {}\n",
    "for L in range(N_LAYERS):\n",
    "    scaler = StandardScaler()\n",
    "    layer_X[L] = scaler.fit_transform(layer_X[L])\n",
    "    scalers[L] = scaler\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "layer_scores = []\n",
    "\n",
    "print(\"\\nCross-validation accuracy:\")\n",
    "for L in range(N_LAYERS):\n",
    "    clf = LogisticRegression(penalty=\"l2\", C=1.0, max_iter=1000, n_jobs=-1)\n",
    "    acc = cross_val_score(clf, layer_X[L], labels, cv=cv, scoring=\"accuracy\").mean()\n",
    "    print(f\"layer {L:2d}:  {acc:.3f}\")\n",
    "    layer_scores.append(acc)\n",
    "\n",
    "best_layer = int(np.argmax(layer_scores))\n",
    "print(f\"\\First highest layer = {best_layer}  (acc = {layer_scores[best_layer]:.3f})\")\n",
    "\n",
    "# final fit on the best layer, save probe + Δµ\n",
    "X_best  = layer_X[best_layer]\n",
    "clf_best = LogisticRegression(penalty=\"l2\", C=1.0, max_iter=1000, n_jobs=-1).fit(X_best, labels)\n",
    "\n",
    "mu_none = X_best[labels == 0].mean(axis=0)\n",
    "mu_sync = X_best[labels == 1].mean(axis=0)\n",
    "delta_mu = mu_sync - mu_none\n",
    "\n",
    "OUT = ROOT / f\"unverb_probe_layer{best_layer}.pkl\"\n",
    "with open(OUT, \"wb\") as f:\n",
    "    pickle.dump(\n",
    "        dict(\n",
    "            layer       = best_layer,\n",
    "            weights     = clf_best.coef_[0].astype(np.float32),\n",
    "            intercept   = float(clf_best.intercept_[0]),\n",
    "            delta_mu    = delta_mu.astype(np.float32),\n",
    "            hidden_size = HIDDEN_SIZE,\n",
    "            created     = datetime.datetime.now().isoformat(timespec=\"seconds\"),\n",
    "            acc_cv      = float(layer_scores[best_layer]),\n",
    "            note        = \"0 = none, 1 = unverb; StandardScaler applied.\",\n",
    "        ),\n",
    "        f,\n",
    "    )\n",
    "print(f\"\\nProbe saved to {OUT.relative_to(Path('.'))}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
