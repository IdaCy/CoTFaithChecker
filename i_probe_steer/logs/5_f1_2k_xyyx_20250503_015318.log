nohup: ignoring input
Working dir: /root/CoTFaithChecker
Working dir: /root/CoTFaithChecker
Working dir: /root/CoTFaithChecker
Working dir: /root/CoTFaithChecker
on host fd4b7fbca240 (PID 53999) ===
starting at 2025-05-03T02:53:34+01:00
2025-05-03 01:53:34,153 - INFO - CUDA is available. Using GPU.
CUDA is available. Using GPU.
2025-05-03 01:53:34,153 - INFO - Loading model and tokenizer: deepseek-ai/DeepSeek-R1-Distill-Llama-8B onto cuda
Loading model and tokenizer: deepseek-ai/DeepSeek-R1-Distill-Llama-8B onto cuda
on host fd4b7fbca240 (PID 54001) ===
starting at 2025-05-03T02:53:35+01:00
2025-05-03 01:53:35,036 - INFO - CUDA is available. Using GPU.
2025-05-03 01:53:35,036 - INFO - Loading model and tokenizer: deepseek-ai/DeepSeek-R1-Distill-Llama-8B onto cuda
on host fd4b7fbca240 (PID 54002) ===
starting at 2025-05-03T02:53:35+01:00
2025-05-03 01:53:35,048 - INFO - CUDA is available. Using GPU.
on host fd4b7fbca240 (PID 54000) ===
2025-05-03 01:53:35,048 - INFO - Loading model and tokenizer: deepseek-ai/DeepSeek-R1-Distill-Llama-8B onto cuda
starting at 2025-05-03T02:53:35+01:00
2025-05-03 01:53:35,049 - INFO - CUDA is available. Using GPU.
2025-05-03 01:53:35,049 - INFO - Loading model and tokenizer: deepseek-ai/DeepSeek-R1-Distill-Llama-8B onto cuda
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.28s/it]
2025-05-03 01:53:40,663 - INFO - Model and tokenizer loaded successfully.
Model and tokenizer loaded successfully.
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.63s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.64s/it]
2025-05-03 01:53:41,861 - INFO - Model and tokenizer loaded successfully.
2025-05-03 01:53:41,946 - INFO - Model and tokenizer loaded successfully.
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.74s/it]
2025-05-03 01:53:42,117 - INFO - Model and tokenizer loaded successfully.
generating completions 333 at 2025-05-03T02:53:57+01:00
2025-05-03 01:53:57,435 - INFO - --- Processing hint type: none ---
generating completions 333 at 2025-05-03T02:53:57+01:00
2025-05-03 01:53:57,436 - INFO - Running on 4 GPU(s)
Running on 4 GPU(s)
2025-05-03 01:53:57,436 - INFO - --- Processing hint type: none ---
--- Processing hint type: none ---
generating completions 333 at 2025-05-03T02:53:57+01:00
generating completions 333 at 2025-05-03T02:53:57+01:00
2025-05-03 01:53:57,442 - INFO - --- Processing hint type: none ---
2025-05-03 01:53:57,443 - INFO - --- Processing hint type: none ---
2025-05-03 01:53:57,450 - ERROR - Data file not found: data/mmlu/hints_none.json
2025-05-03 01:53:57,450 - INFO - Using max_new_tokens: 2048
2025-05-03 01:53:57,451 - INFO - Processing batch 1/3 (size 32, QIDs 3-127)
2025-05-03 01:53:57,453 - ERROR - Data file not found: data/mmlu/hints_none.json
Data file not found: data/mmlu/hints_none.json
2025-05-03 01:53:57,453 - INFO - Using max_new_tokens: 2048
Using max_new_tokens: 2048
2025-05-03 01:53:57,453 - INFO - Processing batch 1/3 (size 32, QIDs 0-124)
Processing batch 1/3 (size 32, QIDs 0-124)
2025-05-03 01:53:57,458 - ERROR - Data file not found: data/mmlu/hints_none.json
2025-05-03 01:53:57,459 - ERROR - Data file not found: data/mmlu/hints_none.json
2025-05-03 01:53:57,459 - INFO - Using max_new_tokens: 2048
2025-05-03 01:53:57,459 - INFO - Processing batch 1/3 (size 32, QIDs 1-125)
2025-05-03 01:53:57,459 - INFO - Using max_new_tokens: 2048
2025-05-03 01:53:57,459 - INFO - Processing batch 1/3 (size 32, QIDs 2-126)
/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
2025-05-03 01:54:13,376 - INFO - Processing batch 2/3 (size 32, QIDs 130-254)
2025-05-03 01:54:19,582 - INFO - Processing batch 2/3 (size 32, QIDs 129-253)
2025-05-03 01:54:21,943 - INFO - Processing batch 2/3 (size 32, QIDs 128-252)
Processing batch 2/3 (size 32, QIDs 128-252)
2025-05-03 01:54:32,826 - INFO - Processing batch 3/3 (size 19, QIDs 258-330)
2025-05-03 01:54:35,041 - INFO - Processing batch 3/3 (size 19, QIDs 257-329)
2025-05-03 01:55:52,561 - INFO - Processing batch 2/3 (size 32, QIDs 131-255)
2025-05-03 01:56:21,242 - INFO - Processing batch 3/3 (size 19, QIDs 259-331)
2025-05-03 01:56:30,812 - INFO - Processing batch 3/3 (size 20, QIDs 256-332)
Processing batch 3/3 (size 20, QIDs 256-332)
generating completions 334 at 2025-05-03T02:56:40+01:00
2025-05-03 01:56:40,704 - INFO - --- Processing hint type: none ---
generating completions 334 at 2025-05-03T02:56:40+01:00
generating completions 334 at 2025-05-03T02:56:40+01:00
2025-05-03 01:56:40,705 - INFO - --- Processing hint type: none ---
2025-05-03 01:56:40,705 - INFO - --- Processing hint type: none ---
2025-05-03 01:56:40,708 - INFO - Results saved to data/mmlu/DeepSeek-R1-Distill-Llama-8B/none/completions_with_333.json
Results saved to data/mmlu/DeepSeek-R1-Distill-Llama-8B/none/completions_with_333.json
2025-05-03 01:56:40,708 - INFO - Total time: 163.27 s
Total time: 163.27 s
generating completions 334 at 2025-05-03T02:56:40+01:00
2025-05-03 01:56:40,709 - INFO - Running on 4 GPU(s)
Running on 4 GPU(s)
2025-05-03 01:56:40,709 - INFO - --- Processing hint type: none ---
--- Processing hint type: none ---
2025-05-03 01:56:40,719 - ERROR - Data file not found: data/mmlu/hints_none.json
2025-05-03 01:56:40,720 - INFO - Using max_new_tokens: 2048
2025-05-03 01:56:40,720 - INFO - Processing batch 1/3 (size 32, QIDs 3-127)
2025-05-03 01:56:40,721 - ERROR - Data file not found: data/mmlu/hints_none.json
2025-05-03 01:56:40,721 - ERROR - Data file not found: data/mmlu/hints_none.json
2025-05-03 01:56:40,722 - INFO - Using max_new_tokens: 2048
2025-05-03 01:56:40,722 - INFO - Processing batch 1/3 (size 32, QIDs 1-125)
2025-05-03 01:56:40,722 - INFO - Using max_new_tokens: 2048
2025-05-03 01:56:40,722 - INFO - Processing batch 1/3 (size 32, QIDs 2-126)
2025-05-03 01:56:40,724 - ERROR - Data file not found: data/mmlu/hints_none.json
Data file not found: data/mmlu/hints_none.json
2025-05-03 01:56:40,724 - INFO - Using max_new_tokens: 2048
Using max_new_tokens: 2048
2025-05-03 01:56:40,724 - INFO - Processing batch 1/3 (size 32, QIDs 0-124)
Processing batch 1/3 (size 32, QIDs 0-124)
2025-05-03 01:56:56,116 - INFO - Processing batch 2/3 (size 32, QIDs 130-254)
2025-05-03 01:57:02,334 - INFO - Processing batch 2/3 (size 32, QIDs 129-253)
2025-05-03 01:57:04,694 - INFO - Processing batch 2/3 (size 32, QIDs 128-252)
Processing batch 2/3 (size 32, QIDs 128-252)
2025-05-03 01:57:15,546 - INFO - Processing batch 3/3 (size 19, QIDs 258-330)
2025-05-03 01:57:17,792 - INFO - Processing batch 3/3 (size 20, QIDs 257-333)
2025-05-03 01:58:35,201 - INFO - Processing batch 2/3 (size 32, QIDs 131-255)
2025-05-03 01:59:03,904 - INFO - Processing batch 3/3 (size 19, QIDs 259-331)
2025-05-03 01:59:13,587 - INFO - Processing batch 3/3 (size 20, QIDs 256-332)
Processing batch 3/3 (size 20, QIDs 256-332)
verifying completions at 2025-05-03T02:59:23+01:00
running switch check at 2025-05-03T02:59:23+01:00
Loading ground truth...
verifying completions at 2025-05-03T02:59:23+01:00verifying completions at
 running switch check at2025-05-03T02:59:23+01:00 
2025-05-03T02:59:23+01:00
running switch check at 2025-05-03T02:59:23+01:00
Loading ground truth...
Loading ground truth...
2025-05-03 01:59:23,506 - INFO - Results saved to data/mmlu/DeepSeek-R1-Distill-Llama-8B/none/completions_with_334.json
Results saved to data/mmlu/DeepSeek-R1-Distill-Llama-8B/none/completions_with_334.json
2025-05-03 01:59:23,506 - INFO - Total time: 162.80 s
Total time: 162.80 s
verifying completions at 2025-05-03T02:59:23+01:00
running switch check at 2025-05-03T02:59:23+01:00
Loading ground truth...
Loading base answers (none)...
[rank3]: Traceback (most recent call last):
[rank3]:   File "/root/CoTFaithChecker/a_confirm_posthoc/parallelization/driver333.py", line 109, in <module>
[rank3]:     run_switch_check(dataset_name, hint_types[1:], model_name, n_questions)
[rank3]:   File "/root/CoTFaithChecker/a_confirm_posthoc/eval/switch_check.py", line 79, in run_switch_check
[rank3]:     base_answers = load_verified_answers(base_verification_file)
[rank3]:   File "/root/CoTFaithChecker/a_confirm_posthoc/eval/switch_check.py", line 19, in load_verified_answers
[rank3]:     data = load_json(file_path)
[rank3]:   File "/root/CoTFaithChecker/a_confirm_posthoc/eval/switch_check.py", line 8, in load_json
[rank3]:     with open(file_path, 'r') as f:
[rank3]: FileNotFoundError: [Errno 2] No such file or directory: 'data/mmlu/DeepSeek-R1-Distill-Llama-8B/none/verification_with_333.json'
Loading base answers (none)...
Loading base answers (none)...
[rank2]: Traceback (most recent call last):
[rank2]:   File "/root/CoTFaithChecker/a_confirm_posthoc/parallelization/driver333.py", line 109, in <module>
[rank2]:     run_switch_check(dataset_name, hint_types[1:], model_name, n_questions)
[rank2]:   File "/root/CoTFaithChecker/a_confirm_posthoc/eval/switch_check.py", line 79, in run_switch_check
[rank2]:     base_answers = load_verified_answers(base_verification_file)
[rank2]:   File "/root/CoTFaithChecker/a_confirm_posthoc/eval/switch_check.py", line 19, in load_verified_answers
[rank2]:     data = load_json(file_path)
[rank2]:   File "/root/CoTFaithChecker/a_confirm_posthoc/eval/switch_check.py", line 8, in load_json
[rank2]:     with open(file_path, 'r') as f:
[rank2]: FileNotFoundError: [Errno 2] No such file or directory: 'data/mmlu/DeepSeek-R1-Distill-Llama-8B/none/verification_with_333.json'
[rank1]: Traceback (most recent call last):
[rank1]:   File "/root/CoTFaithChecker/a_confirm_posthoc/parallelization/driver333.py", line 109, in <module>
[rank1]:     run_switch_check(dataset_name, hint_types[1:], model_name, n_questions)
[rank1]:   File "/root/CoTFaithChecker/a_confirm_posthoc/eval/switch_check.py", line 79, in run_switch_check
[rank1]:     base_answers = load_verified_answers(base_verification_file)
[rank1]:   File "/root/CoTFaithChecker/a_confirm_posthoc/eval/switch_check.py", line 19, in load_verified_answers
[rank1]:     data = load_json(file_path)
[rank1]:   File "/root/CoTFaithChecker/a_confirm_posthoc/eval/switch_check.py", line 8, in load_json
[rank1]:     with open(file_path, 'r') as f:
[rank1]: FileNotFoundError: [Errno 2] No such file or directory: 'data/mmlu/DeepSeek-R1-Distill-Llama-8B/none/verification_with_333.json'
Loading base answers (none)...
[rank0]: Traceback (most recent call last):
[rank0]:   File "/root/CoTFaithChecker/a_confirm_posthoc/parallelization/driver333.py", line 109, in <module>
[rank0]:     run_switch_check(dataset_name, hint_types[1:], model_name, n_questions)
[rank0]:   File "/root/CoTFaithChecker/a_confirm_posthoc/eval/switch_check.py", line 79, in run_switch_check
[rank0]:     base_answers = load_verified_answers(base_verification_file)
[rank0]:   File "/root/CoTFaithChecker/a_confirm_posthoc/eval/switch_check.py", line 19, in load_verified_answers
[rank0]:     data = load_json(file_path)
[rank0]:   File "/root/CoTFaithChecker/a_confirm_posthoc/eval/switch_check.py", line 8, in load_json
[rank0]:     with open(file_path, 'r') as f:
[rank0]: FileNotFoundError: [Errno 2] No such file or directory: 'data/mmlu/DeepSeek-R1-Distill-Llama-8B/none/verification_with_333.json'
[rank0]:[W503 01:59:23.977377117 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0503 01:59:25.560000 53842 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 53999 closing signal SIGTERM
W0503 01:59:25.561000 53842 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 54000 closing signal SIGTERM
E0503 01:59:26.177000 53842 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 2 (pid: 54001) of binary: /usr/bin/python3
Traceback (most recent call last):
  File "/usr/local/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 1204, in launch_command
    multi_gpu_launcher(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 825, in multi_gpu_launcher
    distrib_run.run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
a_confirm_posthoc/parallelization/driver333.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-05-03_01:59:25
  host      : fd4b7fbca240
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 54002)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-05-03_01:59:25
  host      : fd4b7fbca240
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 54001)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
