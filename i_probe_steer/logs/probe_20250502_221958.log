nohup: ignoring input
Working dir: /root/CoTFaithChecker
Working dir: /root/CoTFaithChecker
Working dir: /root/CoTFaithChecker
Working dir: /root/CoTFaithChecker
on host fd4b7fbca240 (PID 12060) ===
starting at 2025-05-02T23:20:10+01:00
on host fd4b7fbca240 (PID 12059) ===on host fd4b7fbca240 (PID 12061) ===

starting atstarting at  2025-05-02T23:20:10+01:002025-05-02T23:20:10+01:00

on host fd4b7fbca240 (PID 12062) ===
starting at 2025-05-02T23:20:11+01:00
gather_object poly-fill; accelerator: <accelerate.accelerator.Accelerator object at 0x7bbcf3188af0>
launching at 2025-05-02T23:20:14+01:00
loading answers at 2025-05-02T23:20:14+01:00
tokenizing1 at 2025-05-02T23:20:14+01:00
gather_object poly-fill; accelerator: <accelerate.accelerator.Accelerator object at 0x7e5d1a950af0>
launching at 2025-05-02T23:20:14+01:00
loading answers at 2025-05-02T23:20:14+01:00
tokenizing1 at 2025-05-02T23:20:14+01:00
[rank1]:[W502 22:20:14.967918936 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
gather_object poly-fill; accelerator: <accelerate.accelerator.Accelerator object at 0x762305954af0>
launching at 2025-05-02T23:20:15+01:00
loading answers at 2025-05-02T23:20:15+01:00
tokenizing1 at 2025-05-02T23:20:15+01:00
[rank2]:[W502 22:20:15.346156342 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
gather_object poly-fill; accelerator: <accelerate.accelerator.Accelerator object at 0x70804bb50af0>
launching at 2025-05-02T23:20:15+01:00
loading answers at 2025-05-02T23:20:15+01:00
tokenizing1 at 2025-05-02T23:20:15+01:00
[rank3]:[W502 22:20:15.364693420 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W502 22:20:15.493695575 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
YES id=22483  |  NO id=11698
tokenizing2 at 2025-05-02T23:20:28+01:00
Loading model to read out lm_head …
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
tokenizing2 at 2025-05-02T23:20:28+01:00
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]tokenizing2 at 2025-05-02T23:20:28+01:00
tokenizing2 at 2025-05-02T23:20:28+01:00
Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.91s/it]
  ↳ done in 21.1s
reading out lm_head at 2025-05-02T23:20:49+01:00
Processing 592 files on 4 ranks (148 / rank)
hidden.pt:   0%|          | 0/148 [00:00<?, ?it/s]iterating at 2025-05-02T23:20:49+01:00
/root/CoTFaithChecker/i_probe_steer/main/xyyx_probing.py:208: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  batch_hidden: list[torch.Tensor] = torch.load(hid_path)  # list[n_layers]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]hidden.pt:   3%|▎         | 4/148 [00:00<00:10, 13.64it/s]hidden.pt:   7%|▋         | 10/148 [00:00<00:04, 27.83it/s]hidden.pt:  11%|█         | 16/148 [00:00<00:03, 36.52it/s]hidden.pt:  15%|█▍        | 22/148 [00:00<00:02, 42.19it/s]hidden.pt:  19%|█▉        | 28/148 [00:00<00:02, 45.11it/s]hidden.pt:  23%|██▎       | 34/148 [00:00<00:02, 47.82it/s]hidden.pt:  27%|██▋       | 40/148 [00:00<00:02, 49.69it/s]hidden.pt:  31%|███       | 46/148 [00:01<00:02, 50.86it/s]hidden.pt:  35%|███▌      | 52/148 [00:01<00:01, 51.66it/s]hidden.pt:  39%|███▉      | 58/148 [00:01<00:02, 33.68it/s]hidden.pt:  43%|████▎     | 64/148 [00:01<00:02, 38.05it/s]hidden.pt:  47%|████▋     | 70/148 [00:01<00:01, 41.83it/s]hidden.pt:  51%|█████▏    | 76/148 [00:01<00:01, 44.90it/s]hidden.pt:  55%|█████▌    | 82/148 [00:01<00:01, 47.38it/s]hidden.pt:  59%|█████▉    | 88/148 [00:02<00:01, 49.05it/s]hidden.pt:  64%|██████▎   | 94/148 [00:02<00:01, 50.40it/s]hidden.pt:  68%|██████▊   | 100/148 [00:02<00:00, 51.41it/s]hidden.pt:  72%|███████▏  | 106/148 [00:02<00:00, 52.20it/s]hidden.pt:  76%|███████▌  | 112/148 [00:02<00:00, 52.82it/s]hidden.pt:  80%|███████▉  | 118/148 [00:02<00:00, 53.20it/s]hidden.pt:  84%|████████▍ | 124/148 [00:02<00:00, 36.31it/s]hidden.pt:  88%|████████▊ | 130/148 [00:03<00:00, 40.25it/s]hidden.pt:  92%|█████████▏| 136/148 [00:03<00:00, 43.21it/s]hidden.pt:  96%|█████████▌| 142/148 [00:03<00:00, 46.02it/s]hidden.pt: 100%|██████████| 148/148 [00:03<00:00, 48.05it/s]                                                            collecting at 2025-05-02T23:20:52+01:00
Rank 0: collected 156,288 samples in 0.1 min
Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.02s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.98s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.44s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  7.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.05s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  7.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.20s/it]
reading out lm_head at 2025-05-02T23:21:10+01:00
reading out lm_head at 2025-05-02T23:21:10+01:00
iterating at 2025-05-02T23:21:10+01:00
/root/CoTFaithChecker/i_probe_steer/main/xyyx_probing.py:208: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  batch_hidden: list[torch.Tensor] = torch.load(hid_path)  # list[n_layers]
iterating at 2025-05-02T23:21:10+01:00
/root/CoTFaithChecker/i_probe_steer/main/xyyx_probing.py:208: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  batch_hidden: list[torch.Tensor] = torch.load(hid_path)  # list[n_layers]
reading out lm_head at 2025-05-02T23:21:10+01:00
iterating at 2025-05-02T23:21:11+01:00
/root/CoTFaithChecker/i_probe_steer/main/xyyx_probing.py:208: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  batch_hidden: list[torch.Tensor] = torch.load(hid_path)  # list[n_layers]
collecting at 2025-05-02T23:21:12+01:00
collecting at 2025-05-02T23:21:13+01:00
collecting at 2025-05-02T23:21:14+01:00
merging at 2025-05-02T23:22:22+01:00
All ranks gathered & merged
  layer  0 → 14,800 samples (showing first 5 layers)
  layer  1 → 14,800 samples (showing first 5 layers)
  layer  2 → 14,800 samples (showing first 5 layers)
  layer  3 → 14,800 samples (showing first 5 layers)
  layer  4 → 14,800 samples (showing first 5 layers)
training at 2025-05-02T23:22:22+01:00

=== Training probes for 33 layers (≤ 1000 samples/layer) ===
training2 at 2025-05-02T23:22:22+01:00
Layer:   0%|          | 0/33 [00:00<?, ?it/s]entering loop at 2025-05-02T23:22:22+01:00
Layer:   0%|          | 0/33 [00:01<?, ?it/s, layer=0, secs=1.0, valF1=0.078]Layer:   3%|▎         | 1/33 [00:01<00:32,  1.01s/it, layer=0, secs=1.0, valF1=0.078]Layer:   3%|▎         | 1/33 [00:03<00:32,  1.01s/it, layer=1, secs=2.3, valF1=0.671]Layer:   6%|▌         | 2/33 [00:03<00:55,  1.81s/it, layer=1, secs=2.3, valF1=0.671]Layer:   6%|▌         | 2/33 [00:08<00:55,  1.81s/it, layer=2, secs=5.1, valF1=0.478]Layer:   9%|▉         | 3/33 [00:08<01:40,  3.35s/it, layer=2, secs=5.1, valF1=0.478]Layer:   9%|▉         | 3/33 [00:15<01:40,  3.35s/it, layer=3, secs=7.0, valF1=0.358]Layer:  12%|█▏        | 4/33 [00:15<02:19,  4.79s/it, layer=3, secs=7.0, valF1=0.358]Layer:  12%|█▏        | 4/33 [00:19<02:19,  4.79s/it, layer=4, secs=4.2, valF1=0.647]  ↳ layer  4 finished | val-F1 0.647 | 4.2s
Layer:  15%|█▌        | 5/33 [00:19<02:08,  4.57s/it, layer=4, secs=4.2, valF1=0.647]Layer:  15%|█▌        | 5/33 [00:29<02:08,  4.57s/it, layer=5, secs=10.0, valF1=0.612]Layer:  18%|█▊        | 6/33 [00:29<02:53,  6.41s/it, layer=5, secs=10.0, valF1=0.612]Layer:  18%|█▊        | 6/33 [00:44<02:53,  6.41s/it, layer=6, secs=14.8, valF1=0.544]Layer:  21%|██        | 7/33 [00:44<03:58,  9.17s/it, layer=6, secs=14.8, valF1=0.544]Layer:  21%|██        | 7/33 [00:49<03:58,  9.17s/it, layer=7, secs=5.0, valF1=0.566] Layer:  24%|██▍       | 8/33 [00:49<03:16,  7.85s/it, layer=7, secs=5.0, valF1=0.566]Layer:  24%|██▍       | 8/33 [00:54<03:16,  7.85s/it, layer=8, secs=5.0, valF1=0.497]Layer:  27%|██▋       | 9/33 [00:54<02:46,  6.96s/it, layer=8, secs=5.0, valF1=0.497]Layer:  27%|██▋       | 9/33 [01:05<02:46,  6.96s/it, layer=9, secs=10.6, valF1=0.481]  ↳ layer  9 finished | val-F1 0.481 | 10.6s
Layer:  30%|███       | 10/33 [01:05<03:06,  8.10s/it, layer=9, secs=10.6, valF1=0.481]Layer:  30%|███       | 10/33 [01:23<03:06,  8.10s/it, layer=10, secs=18.0, valF1=0.450]Layer:  33%|███▎      | 11/33 [01:23<04:05, 11.14s/it, layer=10, secs=18.0, valF1=0.450]Layer:  33%|███▎      | 11/33 [01:29<04:05, 11.14s/it, layer=11, secs=6.2, valF1=0.396] Layer:  36%|███▋      | 12/33 [01:29<03:22,  9.63s/it, layer=11, secs=6.2, valF1=0.396]Layer:  36%|███▋      | 12/33 [01:36<03:22,  9.63s/it, layer=12, secs=7.3, valF1=0.583]Layer:  39%|███▉      | 13/33 [01:36<02:58,  8.92s/it, layer=12, secs=7.3, valF1=0.583]Layer:  39%|███▉      | 13/33 [01:45<02:58,  8.92s/it, layer=13, secs=8.8, valF1=0.502]Layer:  42%|████▏     | 14/33 [01:45<02:48,  8.89s/it, layer=13, secs=8.8, valF1=0.502]Layer:  42%|████▏     | 14/33 [01:55<02:48,  8.89s/it, layer=14, secs=9.7, valF1=0.443]  ↳ layer 14 finished | val-F1 0.443 | 9.7s
Layer:  45%|████▌     | 15/33 [01:55<02:44,  9.14s/it, layer=14, secs=9.7, valF1=0.443]Layer:  45%|████▌     | 15/33 [02:06<02:44,  9.14s/it, layer=15, secs=10.9, valF1=0.492]Layer:  48%|████▊     | 16/33 [02:06<02:44,  9.69s/it, layer=15, secs=10.9, valF1=0.492]Layer:  48%|████▊     | 16/33 [02:19<02:44,  9.69s/it, layer=16, secs=13.3, valF1=0.466]Layer:  52%|█████▏    | 17/33 [02:19<02:52, 10.77s/it, layer=16, secs=13.3, valF1=0.466]Layer:  52%|█████▏    | 17/33 [02:37<02:52, 10.77s/it, layer=17, secs=17.5, valF1=0.524]Layer:  55%|█████▍    | 18/33 [02:37<03:12, 12.81s/it, layer=17, secs=17.5, valF1=0.524]Layer:  55%|█████▍    | 18/33 [02:56<03:12, 12.81s/it, layer=18, secs=19.4, valF1=0.416]Layer:  58%|█████▊    | 19/33 [02:56<03:27, 14.81s/it, layer=18, secs=19.4, valF1=0.416]Layer:  58%|█████▊    | 19/33 [03:19<03:27, 14.81s/it, layer=19, secs=22.5, valF1=0.465]  ↳ layer 19 finished | val-F1 0.465 | 22.5s
Layer:  61%|██████    | 20/33 [03:19<03:42, 17.13s/it, layer=19, secs=22.5, valF1=0.465]Layer:  61%|██████    | 20/33 [03:44<03:42, 17.13s/it, layer=20, secs=25.1, valF1=0.519]Layer:  64%|██████▎   | 21/33 [03:44<03:54, 19.53s/it, layer=20, secs=25.1, valF1=0.519]Layer:  64%|██████▎   | 21/33 [04:13<03:54, 19.53s/it, layer=21, secs=29.3, valF1=0.507]Layer:  67%|██████▋   | 22/33 [04:13<04:07, 22.48s/it, layer=21, secs=29.3, valF1=0.507]Layer:  67%|██████▋   | 22/33 [04:48<04:07, 22.48s/it, layer=22, secs=34.8, valF1=0.453]Layer:  70%|██████▉   | 23/33 [04:48<04:21, 26.19s/it, layer=22, secs=34.8, valF1=0.453]Layer:  70%|██████▉   | 23/33 [05:23<04:21, 26.19s/it, layer=23, secs=34.6, valF1=0.490]Layer:  73%|███████▎  | 24/33 [05:23<04:18, 28.71s/it, layer=23, secs=34.6, valF1=0.490]Layer:  73%|███████▎  | 24/33 [06:05<04:18, 28.71s/it, layer=24, secs=42.6, valF1=0.429]  ↳ layer 24 finished | val-F1 0.429 | 42.6s
Layer:  76%|███████▌  | 25/33 [06:05<04:23, 32.88s/it, layer=24, secs=42.6, valF1=0.429]Layer:  76%|███████▌  | 25/33 [06:48<04:23, 32.88s/it, layer=25, secs=42.7, valF1=0.464]Layer:  79%|███████▉  | 26/33 [06:48<04:10, 35.84s/it, layer=25, secs=42.7, valF1=0.464]Layer:  79%|███████▉  | 26/33 [07:40<04:10, 35.84s/it, layer=26, secs=52.3, valF1=0.421]Layer:  82%|████████▏ | 27/33 [07:40<04:04, 40.78s/it, layer=26, secs=52.3, valF1=0.421]Layer:  82%|████████▏ | 27/33 [08:35<04:04, 40.78s/it, layer=27, secs=54.7, valF1=0.468]Layer:  85%|████████▍ | 28/33 [08:35<03:44, 44.97s/it, layer=27, secs=54.7, valF1=0.468]Layer:  85%|████████▍ | 28/33 [09:37<03:44, 44.97s/it, layer=28, secs=62.3, valF1=0.487]Layer:  88%|████████▊ | 29/33 [09:37<03:20, 50.18s/it, layer=28, secs=62.3, valF1=0.487]Layer:  88%|████████▊ | 29/33 [10:45<03:20, 50.18s/it, layer=29, secs=67.3, valF1=0.537]  ↳ layer 29 finished | val-F1 0.537 | 67.3s
Layer:  91%|█████████ | 30/33 [10:45<02:45, 55.33s/it, layer=29, secs=67.3, valF1=0.537]Layer:  91%|█████████ | 30/33 [12:00<02:45, 55.33s/it, layer=30, secs=75.2, valF1=0.472]Layer:  94%|█████████▍| 31/33 [12:00<02:02, 61.29s/it, layer=30, secs=75.2, valF1=0.472]Layer:  94%|█████████▍| 31/33 [13:18<02:02, 61.29s/it, layer=31, secs=78.0, valF1=0.497]Layer:  97%|█████████▋| 32/33 [13:18<01:06, 66.31s/it, layer=31, secs=78.0, valF1=0.497]Layer:  97%|█████████▋| 32/33 [15:36<01:06, 66.31s/it, layer=32, secs=138.1, valF1=0.483]Layer: 100%|██████████| 33/33 [15:36<00:00, 87.87s/it, layer=32, secs=138.1, valF1=0.483]Layer: 100%|██████████| 33/33 [15:36<00:00, 28.38s/it, layer=32, secs=138.1, valF1=0.483]
closing bar at 2025-05-02T23:37:59+01:00
Total probe-training time: 15.6 min

Best layer = 1   (F1 = 0.671)
Probe saved → linear_probes/linear_probe_layer1.joblib
plotting at 2025-05-02T23:37:59+01:00
Plot saved
[rank0]:[W502 22:38:01.946866531 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
