{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/CoTFaithChecker\n",
      "[2025-05-03 17:51:36] loading answer files …\n",
      "answers_map built: ('YES', 'NO'):16, ('YES', 'YES'):35, ('NO', 'NO'):15, ('NO', 'YES'):34\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Linear-probe trainer for the XY⇄YX faithfulness experiment.\n",
    "\n",
    "Targets:\n",
    "    y = 1 … model answered correctly (YES when YES expected, NO when NO expected)\n",
    "    y = 0 … model gave the *same* answer to both variants and this is the\n",
    "            *wrong* variant (YES instead of NO, or NO instead of YES)\n",
    "\n",
    "All bookkeeping comes from the answer JSONs – we never peek at `lm_head`.\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0,1,2,3 \\\n",
    "nohup accelerate launch \\\n",
    "      --num_processes 4 \\\n",
    "      --mixed_precision no \\\n",
    "      your_probe_script.py \\\n",
    "      > logs/launcher.out 2>&1 &\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, re, json, math, random, time, socket, pathlib\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "import torch, joblib\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib; matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_curve, auc\n",
    "\n",
    "from accelerate import Accelerator\n",
    "%cd ../..\n",
    "%pwd\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# CONFIG\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "ANSWERS_DIRS      = [\n",
    "    Path(\"e_confirm_xy_yx/outputs/matched_vals_gt\"),\n",
    "    Path(\"e_confirm_xy_yx/outputs/matched_vals_lt\"),\n",
    "]\n",
    "ACTIVATIONS_DIR   = Path(\"h_hidden_space/outputs/f1_hint_xyyx/xyyx_deterministic/gt_lt_completions_1\")\n",
    "QUESTION_JSON_DIR = Path(\"data/chainscope/questions_json/linked\")\n",
    "\n",
    "PROBE_SAVE_DIR    = Path(\"linear_probes/realyesno_5k\")  # results end up here\n",
    "PROBE_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "INFERENCE_BATCH_SIZE   = 32   # must match the batch size used in `run_inference`\n",
    "MAX_SAMPLES_PER_LAYER  = 5_000\n",
    "PRINT_EVERY_LAYERS     = 5\n",
    "RANDOM_SEED            = 0\n",
    "MAX_FILES = None\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "acc       = Accelerator()\n",
    "DEVICE    = acc.device\n",
    "is_main   = acc.is_main_process\n",
    "p         = acc.print                                                    # rank-aware print\n",
    "_now      = lambda: time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "\n",
    "\n",
    "# ─── make accelerate≤0.20 look like ≥0.21 ─────────────────────────\n",
    "if not hasattr(acc, \"gather_object\"):\n",
    "    import torch.distributed as dist\n",
    "\n",
    "    def _gather_object(obj):\n",
    "        \"\"\"\n",
    "        Minimal stand-in for accelerate.gather_object().\n",
    "        Works for any picklable Python object.\n",
    "        \"\"\"\n",
    "        if acc.num_processes == 1:\n",
    "            return [obj]                      # nothing to gather\n",
    "        gathered = [None] * acc.num_processes\n",
    "        dist.all_gather_object(gathered, obj) # inplace\n",
    "        return gathered\n",
    "\n",
    "    acc.gather_object = _gather_object\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "# │ 1.  build a QUESTION-ID  →  {expected, actual, same} lookup\n",
    "answers_map: dict[int, dict[str, object]] = {}\n",
    "\n",
    "def load_answers():\n",
    "    for dir_ in ANSWERS_DIRS:\n",
    "        for fp in dir_.glob(\"*.json\"):\n",
    "            with open(fp) as f:\n",
    "                raw = json.load(f)\n",
    "            questions = raw[\"questions\"] if isinstance(raw, dict) else raw\n",
    "            for q in questions:\n",
    "                same_flag = q[\"same\"][0] if isinstance(q[\"same\"], list) else q[\"same\"]\n",
    "                q_id_no   = q[\"question_id\"]\n",
    "                q_id_yes  = q[\"question_yes_id\"]\n",
    "                answers_map[q_id_no]  = {\"expected\": \"NO\",  \"actual\": q[\"a_answers\"][0],\n",
    "                                         \"same\": same_flag}\n",
    "                answers_map[q_id_yes] = {\"expected\": \"YES\", \"actual\": q[\"b_answers\"][0],\n",
    "                                         \"same\": same_flag}\n",
    "\n",
    "if is_main:\n",
    "    print(f\"[{_now()}] loading answer files …\")\n",
    "load_answers()\n",
    "acc.wait_for_everyone()\n",
    "\n",
    "# optional quick sanity print\n",
    "if is_main:\n",
    "    cnt = Counter((v[\"expected\"], v[\"actual\"]) for v in answers_map.values())\n",
    "    print(\"answers_map built:\",\n",
    "          \", \".join([f\"{k}:{v:,}\" for k,v in cnt.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-03 17:51:36] • 1 hidden files total\n",
      "[2025-05-03 17:51:36] • 592 files on rank-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collect:   0%|          | 0/592 [00:00<?, ?it/s]/tmp/ipykernel_7372/3759529805.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  batch_hidden: list[torch.Tensor] = torch.load(hid_fp)   # list[n_layers]  each (B,H)\n",
      "collect:   2%|▏         | 9/592 [00:00<00:06, 84.58it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collect: 100%|██████████| 592/592 [00:06<00:00, 90.60it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-03 17:51:42] gathered 317,460 labelled samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train probes: 100%|██████████| 33/33 [15:36<00:00, 28.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-03 18:07:19] best layer = 9  (val-F1 0.883)  → saved →  linear_probes/realyesno_5k/linear_probe_layer9.joblib\n",
      "[2025-05-03 18:07:20] all done – diagnostics written to linear_probes/realyesno_5k\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ╭──────────────────────────────────────────────────────────────────────────╮\n",
    "# │ 2.  utilities to map a hidden-file row → question-id                     │\n",
    "# ╰──────────────────────────────────────────────────────────────────────────╯\n",
    "QUESTION_JSON_ROOT = Path(\"data/chainscope/questions_json\")   # ← root only!\n",
    "_STEM_TO_PATH: dict[str, Path] = {}\n",
    "\n",
    "def _index_json_files_once() -> None:\n",
    "    \"\"\"\n",
    "    Walk QUESTION_JSON_ROOT **recursively** and build\n",
    "        filename-stem  →  Path\n",
    "    so we can find     wm-book-length_gt_NO_1_6fda02e3.json\n",
    "    no matter whether it lives in linked/gt_NO_1/  or elsewhere.\n",
    "    \"\"\"\n",
    "    if _STEM_TO_PATH:            # already built\n",
    "        return\n",
    "    for p in QUESTION_JSON_ROOT.rglob(\"*.json\"):\n",
    "        _STEM_TO_PATH[p.stem] = p\n",
    "    if not _STEM_TO_PATH:\n",
    "        raise RuntimeError(f\"No *.json found under {QUESTION_JSON_ROOT}\")\n",
    "\n",
    "def get_dataset_questions(dataset_stem: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Return the list of question-dicts corresponding to a dataset stem like\n",
    "        wm-book-length_gt_NO_1_6fda02e3\n",
    "    \"\"\"\n",
    "    _index_json_files_once()\n",
    "    fp = _STEM_TO_PATH.get(dataset_stem)\n",
    "    if fp is None:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Could not locate {dataset_stem}.json anywhere under \"\n",
    "            f\"{QUESTION_JSON_ROOT}. (Have {len(_STEM_TO_PATH)} stems indexed.)\"\n",
    "        )\n",
    "    with open(fp) as f:\n",
    "        raw = json.load(f)\n",
    "    return raw[\"questions\"] if isinstance(raw, dict) else raw\n",
    "\n",
    "_BATCH_RE = re.compile(r\"_batch(\\d+)_hidden\\.pt$\")\n",
    "\n",
    "def question_ids_for_hidden_file(\n",
    "    hid_path: Path, batch_size: int, actual_batch_len: int\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    For a hidden-state tensor file   …_batchK_hidden.pt\n",
    "    return the question-ids for each row in that batch.\n",
    "    \"\"\"\n",
    "    m = _BATCH_RE.search(hid_path.name)\n",
    "    if m is None:\n",
    "        raise ValueError(f\"Bad hidden filename: {hid_path.name}\")\n",
    "    batch_idx  = int(m.group(1))\n",
    "    dataset_stem = hid_path.name[:m.start()]          # strip “…_batchK_hidden.pt”\n",
    "    q_list     = get_dataset_questions(dataset_stem)\n",
    "    start      = batch_idx * batch_size\n",
    "    return [q_list[start + j][\"question_id\"]          # honour short final batches\n",
    "            for j in range(actual_batch_len)]\n",
    "\n",
    "\n",
    "# ╭──────────────────────────────────────────────────────────────────────────╮\n",
    "# │ 3.  collect (h_vec, label) pairs for every layer                         │\n",
    "# ╰──────────────────────────────────────────────────────────────────────────╯\n",
    "layer_buckets: dict[int, list[tuple[torch.Tensor, int]]] = defaultdict(list)\n",
    "\n",
    "all_hidden_paths = sorted(ACTIVATIONS_DIR.rglob(\"*_hidden.pt\"))\n",
    "if MAX_FILES is not None:\n",
    "    all_hidden_paths = all_hidden_paths[:MAX_FILES]\n",
    "\n",
    "# shard only after the limit\n",
    "hidden_paths = all_hidden_paths[acc.process_index::acc.num_processes]\n",
    "\n",
    "if is_main:\n",
    "    print(f\"[{_now()}] • {len(all_hidden_paths):,} hidden files total\")\n",
    "    print(f\"[{_now()}] • {len(hidden_paths):,} files on rank-{acc.process_index}\")\n",
    "\n",
    "progress = tqdm(hidden_paths, desc=\"collect\", disable=not is_main)\n",
    "for hid_fp in progress:\n",
    "    batch_hidden: list[torch.Tensor] = torch.load(hid_fp)   # list[n_layers]  each (B,H)\n",
    "    batch_len     = batch_hidden[0].size(0)\n",
    "    q_ids         = question_ids_for_hidden_file(hid_fp, INFERENCE_BATCH_SIZE, batch_len)\n",
    "\n",
    "    # outer loop: row-in-batch\n",
    "    for row_idx, q_id in enumerate(q_ids):\n",
    "        meta = answers_map.get(q_id)\n",
    "        if meta is None:                 # happens if you didn't copy both gt & lt directories\n",
    "            continue\n",
    "\n",
    "        correct = (meta[\"actual\"] == meta[\"expected\"])\n",
    "        if not correct and not meta[\"same\"]:\n",
    "            continue                     # skip “ordinary” mistakes – we only want same-answer errors\n",
    "\n",
    "        label = 1 if correct else 0\n",
    "        for L, layer_tensor in enumerate(batch_hidden):\n",
    "            layer_buckets[L].append((layer_tensor[row_idx].float(), label))\n",
    "\n",
    "progress.close()\n",
    "acc.wait_for_everyone()\n",
    "\n",
    "bucket_shards = acc.gather_object(layer_buckets)   # list[dict] – 1 per rank\n",
    "\n",
    "if not is_main:\n",
    "    # free some memory and exit early\n",
    "    del layer_buckets\n",
    "    sys.exit(0)\n",
    "\n",
    "# merge rank shards\n",
    "merged: dict[int, list[tuple[torch.Tensor,int]]] = defaultdict(list)\n",
    "for shard in bucket_shards:\n",
    "    for L, pairs in shard.items():\n",
    "        merged[L].extend(pairs)\n",
    "layer_buckets = merged\n",
    "\n",
    "print(f\"[{_now()}] gathered {sum(len(v) for v in layer_buckets.values()):,} labelled samples\")\n",
    "\n",
    "# ╭──────────────────────────────────────────────────────────────────────────╮\n",
    "# │ 4.  layer-wise logistic-regression probes                                │\n",
    "# ╰──────────────────────────────────────────────────────────────────────────╯\n",
    "results = {}\n",
    "t_global = time.time()\n",
    "for L in tqdm(sorted(layer_buckets), desc=\"train probes\"):\n",
    "    pairs = layer_buckets[L]\n",
    "    if MAX_SAMPLES_PER_LAYER and len(pairs) > MAX_SAMPLES_PER_LAYER:\n",
    "        pairs = random.sample(pairs, MAX_SAMPLES_PER_LAYER)\n",
    "\n",
    "    X = torch.stack([p[0] for p in pairs]).numpy()\n",
    "    y = np.array([p[1] for p in pairs])\n",
    "\n",
    "    if len(np.unique(y)) < 2:            # all-positive or all-negative – skip\n",
    "        continue\n",
    "\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
    "    )\n",
    "\n",
    "    probe = LogisticRegression(\n",
    "        penalty=\"l2\", solver=\"saga\",\n",
    "        max_iter=10_000, n_jobs=-1, verbose=0\n",
    "    ).fit(X_tr, y_tr)\n",
    "    f1 = f1_score(y_val, probe.predict(X_val))\n",
    "    results[L] = (f1, probe)\n",
    "\n",
    "best_layer, (best_f1, best_probe) = max(results.items(), key=lambda kv: kv[1][0])\n",
    "out_path = PROBE_SAVE_DIR / f\"linear_probe_layer{best_layer}.joblib\"\n",
    "joblib.dump(best_probe, out_path)\n",
    "print(f\"[{_now()}] best layer = {best_layer}  (val-F1 {best_f1:.3f})  → saved →  {out_path}\")\n",
    "\n",
    "# ╭──────────────────────────────────────────────────────────────────────────╮\n",
    "# │ 5.  quick diagnostics                                                    │\n",
    "# ╰──────────────────────────────────────────────────────────────────────────╯\n",
    "layers = sorted(results)\n",
    "f1s    = [results[L][0] for L in layers]\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(layers, f1s, marker=\"o\"); plt.grid(True)\n",
    "plt.title(\"Layer-wise validation F1\"); plt.xlabel(\"layer\"); plt.ylabel(\"F1\")\n",
    "plt.tight_layout(); plt.savefig(PROBE_SAVE_DIR / \"layer_f1_curve.png\"); plt.close()\n",
    "\n",
    "sample_counts = [len(layer_buckets[L]) for L in layers]\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.bar(layers, sample_counts)\n",
    "plt.title(\"#samples per layer\"); plt.xlabel(\"layer\"); plt.ylabel(\"count\")\n",
    "plt.tight_layout(); plt.savefig(PROBE_SAVE_DIR / \"layer_sample_counts.png\"); plt.close()\n",
    "\n",
    "# confusion & ROC for best layer\n",
    "pairs_all = layer_buckets[best_layer]\n",
    "X_all     = torch.stack([p[0] for p in pairs_all]).numpy()\n",
    "y_all     = np.array([p[1] for p in pairs_all])\n",
    "y_pred    = best_probe.predict(X_all)\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "cm = confusion_matrix(y_all, y_pred)\n",
    "plt.imshow(cm, cmap=\"Blues\")\n",
    "for (i,j),v in np.ndenumerate(cm):\n",
    "    plt.text(j,i,f\"{v:,}\",ha=\"center\",va=\"center\")\n",
    "plt.xticks([0,1],[\"wrong\",\"correct\"]); plt.yticks([0,1],[\"wrong\",\"correct\"])\n",
    "plt.tight_layout(); plt.savefig(PROBE_SAVE_DIR / f\"confusion_matrix_layer{best_layer}.png\"); plt.close()\n",
    "\n",
    "probs = (best_probe.predict_proba(X_all)[:,1]\n",
    "         if hasattr(best_probe,\"predict_proba\")\n",
    "         else best_probe.decision_function(X_all))\n",
    "fpr,tpr,_ = roc_curve(y_all, probs); roc_auc = auc(fpr,tpr)\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(fpr,tpr); plt.plot([0,1],[0,1],\"--\")\n",
    "plt.title(f\"ROC AUC {roc_auc:.3f}  (layer {best_layer})\")\n",
    "plt.tight_layout(); plt.savefig(PROBE_SAVE_DIR / f\"roc_layer{best_layer}.png\"); plt.close()\n",
    "\n",
    "print(f\"[{_now()}] all done – diagnostics written to {PROBE_SAVE_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
