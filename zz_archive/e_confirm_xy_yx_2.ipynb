{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/CoTFaithChecker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-26 12:46:44,255 - INFO - CUDA is available. Using GPU.\n",
      "2025-04-26 12:46:44,256 - INFO - Loading model and tokenizer: deepseek-ai/DeepSeek-R1-Distill-Llama-8B onto cuda\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.80s/it]\n",
      "2025-04-26 12:46:54,726 - INFO - Model and tokenizer loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ───────────────────────────────────────────────\n",
    "# 0. Manual configuration\n",
    "# ───────────────────────────────────────────────\n",
    "%cd ..\n",
    "%pwd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "DATA_ROOT = Path(\"data/chainscope/questions_json\")\n",
    "TEMPLATE_PATH = Path(\"data/chainscope/templates/instructions.json\")\n",
    "LOG_DIR = Path(\"logs\")\n",
    "OUT_DIR = Path(\"e_confirm_xy_yx/outputs\")          # completions, verification, matches\n",
    "MODEL_PATH = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# choose folder subsets\n",
    "DATASETS = [\"gt_NO_1\", \"gt_YES_1\"]\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "MAX_NEW_TOKENS = None\n",
    "SAVE_HIDDEN, SAVE_ATTN = False, False\n",
    "HIDDEN_LAYERS, ATTN_LAYERS = [0, -1], [0, -1]   # ignored unless above switches True\n",
    "N_VERIFY = 0   # 0 == verify all\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 1. Load model & tokenizer  (your helper)\n",
    "# ───────────────────────────────────────────────\n",
    "from a_confirm_posthoc.utils.model_handler import load_model_and_tokenizer\n",
    "\n",
    "model, tokenizer, model_name, device = load_model_and_tokenizer(MODEL_PATH)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "data_loader — INFO — → kept 17 after cluster filter ['us']\n",
      "2025-04-26 12:47:48,625 - INFO - → kept 17 after cluster filter ['us']\n",
      "data_loader — INFO — Found 17 files in data/chainscope/questions_json/gt_NO_1\n",
      "2025-04-26 12:47:48,626 - INFO - Found 17 files in data/chainscope/questions_json/gt_NO_1\n",
      "data_loader — INFO — → kept 17 after cluster filter ['us']\n",
      "2025-04-26 12:47:48,628 - INFO - → kept 17 after cluster filter ['us']\n",
      "data_loader — INFO — Found 17 files in data/chainscope/questions_json/gt_YES_1\n",
      "2025-04-26 12:47:48,629 - INFO - Found 17 files in data/chainscope/questions_json/gt_YES_1\n",
      "data_loader — INFO — Total files collected: 34\n",
      "2025-04-26 12:47:48,630 - INFO - Total files collected: 34\n"
     ]
    }
   ],
   "source": [
    "from e_confirm_xy_yx.main.data_loader import get_dataset_files\n",
    "\n",
    "# 0. Extra toggle\n",
    "CLUSTERS = [\"us\"]   # no \"no_wm\"\n",
    "\n",
    "# 2. Collect dataset files\n",
    "dataset_files = get_dataset_files(\n",
    "    DATA_ROOT,\n",
    "    DATASETS,\n",
    "    clusters=CLUSTERS,          # ← NEW ARG\n",
    ")\n",
    "\n",
    "# 5. Verify – point to aggregated cluster outputs\n",
    "completion_files = sorted(\n",
    "    (OUT_DIR / \"completions\" / \"clusters\").glob(\"*_completions.json\")\n",
    ")\n",
    "\n",
    "# 6. Match YES vs NO on cluster files\n",
    "verified_files = sorted((OUT_DIR / \"verified\").glob(\"*_verified.json\"))\n",
    "\n",
    "pairs = [\n",
    "    (vf, vf.parent / vf.name.replace(\"_NO_\", \"_YES_\"))\n",
    "    for vf in verified_files\n",
    "    if \"_NO_\" in vf.name\n",
    "    and (vf.parent / vf.name.replace(\"_NO_\", \"_YES_\")).exists()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "prompt_builder — INFO — PromptBuilder initialised with style=instr-v0, mode=cot\n",
      "2025-04-26 12:47:52,357 - INFO - PromptBuilder initialised with style=instr-v0, mode=cot\n",
      "inference — INFO — Processing wm-us-city-dens_gt_NO_1_c02144cb.json\n",
      "2025-04-26 12:47:52,360 - INFO - Processing wm-us-city-dens_gt_NO_1_c02144cb.json\n",
      "2025-04-26 12:47:52,361 - DEBUG - Loading data/chainscope/questions_json/gt_NO_1/wm-us-city-dens_gt_NO_1_c02144cb.json\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "inference — INFO — Saved completions → e_confirm_xy_yx/outputs/completions/wm-us-city-dens_gt_NO_1_c02144cb_DeepSeek-R1-Distill-Llama-8B_completions.json\n",
      "2025-04-26 13:13:58,581 - INFO - Saved completions → e_confirm_xy_yx/outputs/completions/wm-us-city-dens_gt_NO_1_c02144cb_DeepSeek-R1-Distill-Llama-8B_completions.json\n",
      "inference — INFO — Processing wm-us-city-lat_gt_NO_1_d85c12b0.json\n",
      "2025-04-26 13:13:58,586 - INFO - Processing wm-us-city-lat_gt_NO_1_d85c12b0.json\n",
      "2025-04-26 13:13:58,588 - DEBUG - Loading data/chainscope/questions_json/gt_NO_1/wm-us-city-lat_gt_NO_1_d85c12b0.json\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "inference — INFO — Saved completions → e_confirm_xy_yx/outputs/completions/wm-us-city-lat_gt_NO_1_d85c12b0_DeepSeek-R1-Distill-Llama-8B_completions.json\n",
      "2025-04-26 13:39:53,388 - INFO - Saved completions → e_confirm_xy_yx/outputs/completions/wm-us-city-lat_gt_NO_1_d85c12b0_DeepSeek-R1-Distill-Llama-8B_completions.json\n",
      "inference — INFO — Processing wm-us-city-long_gt_NO_1_357c656e.json\n",
      "2025-04-26 13:39:53,394 - INFO - Processing wm-us-city-long_gt_NO_1_357c656e.json\n",
      "2025-04-26 13:39:53,395 - DEBUG - Loading data/chainscope/questions_json/gt_NO_1/wm-us-city-long_gt_NO_1_357c656e.json\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "inference — INFO — Saved completions → e_confirm_xy_yx/outputs/completions/wm-us-city-long_gt_NO_1_357c656e_DeepSeek-R1-Distill-Llama-8B_completions.json\n",
      "2025-04-26 14:05:49,839 - INFO - Saved completions → e_confirm_xy_yx/outputs/completions/wm-us-city-long_gt_NO_1_357c656e_DeepSeek-R1-Distill-Llama-8B_completions.json\n",
      "inference — INFO — Processing wm-us-city-popu_gt_NO_1_0e3a9117.json\n",
      "2025-04-26 14:05:49,842 - INFO - Processing wm-us-city-popu_gt_NO_1_0e3a9117.json\n",
      "2025-04-26 14:05:49,844 - DEBUG - Loading data/chainscope/questions_json/gt_NO_1/wm-us-city-popu_gt_NO_1_0e3a9117.json\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "inference — INFO — Saved completions → e_confirm_xy_yx/outputs/completions/wm-us-city-popu_gt_NO_1_0e3a9117_DeepSeek-R1-Distill-Llama-8B_completions.json\n",
      "2025-04-26 14:31:45,531 - INFO - Saved completions → e_confirm_xy_yx/outputs/completions/wm-us-city-popu_gt_NO_1_0e3a9117_DeepSeek-R1-Distill-Llama-8B_completions.json\n",
      "inference — INFO — Processing wm-us-college-lat_gt_NO_1_e6907b99.json\n",
      "2025-04-26 14:31:45,534 - INFO - Processing wm-us-college-lat_gt_NO_1_e6907b99.json\n",
      "2025-04-26 14:31:45,535 - DEBUG - Loading data/chainscope/questions_json/gt_NO_1/wm-us-college-lat_gt_NO_1_e6907b99.json\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "inference — INFO — Saved completions → e_confirm_xy_yx/outputs/completions/wm-us-college-lat_gt_NO_1_e6907b99_DeepSeek-R1-Distill-Llama-8B_completions.json\n",
      "2025-04-26 14:57:52,312 - INFO - Saved completions → e_confirm_xy_yx/outputs/completions/wm-us-college-lat_gt_NO_1_e6907b99_DeepSeek-R1-Distill-Llama-8B_completions.json\n",
      "inference — INFO — Processing wm-us-college-long_gt_NO_1_377bc8de.json\n",
      "2025-04-26 14:57:52,318 - INFO - Processing wm-us-college-long_gt_NO_1_377bc8de.json\n",
      "2025-04-26 14:57:52,319 - DEBUG - Loading data/chainscope/questions_json/gt_NO_1/wm-us-college-long_gt_NO_1_377bc8de.json\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "inference — INFO — Saved completions → e_confirm_xy_yx/outputs/completions/wm-us-college-long_gt_NO_1_377bc8de_DeepSeek-R1-Distill-Llama-8B_completions.json\n",
      "2025-04-26 15:24:01,364 - INFO - Saved completions → e_confirm_xy_yx/outputs/completions/wm-us-college-long_gt_NO_1_377bc8de_DeepSeek-R1-Distill-Llama-8B_completions.json\n",
      "inference — INFO — Processing wm-us-county-lat_gt_NO_1_09919763.json\n",
      "2025-04-26 15:24:01,370 - INFO - Processing wm-us-county-lat_gt_NO_1_09919763.json\n",
      "2025-04-26 15:24:01,371 - DEBUG - Loading data/chainscope/questions_json/gt_NO_1/wm-us-county-lat_gt_NO_1_09919763.json\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "inference — INFO — Saved completions → e_confirm_xy_yx/outputs/completions/wm-us-county-lat_gt_NO_1_09919763_DeepSeek-R1-Distill-Llama-8B_completions.json\n",
      "2025-04-26 15:49:55,552 - INFO - Saved completions → e_confirm_xy_yx/outputs/completions/wm-us-county-lat_gt_NO_1_09919763_DeepSeek-R1-Distill-Llama-8B_completions.json\n",
      "inference — INFO — Processing wm-us-county-long_gt_NO_1_878dc6e7.json\n",
      "2025-04-26 15:49:55,556 - INFO - Processing wm-us-county-long_gt_NO_1_878dc6e7.json\n",
      "2025-04-26 15:49:55,557 - DEBUG - Loading data/chainscope/questions_json/gt_NO_1/wm-us-county-long_gt_NO_1_878dc6e7.json\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "inference — INFO — Saved completions → e_confirm_xy_yx/outputs/completions/wm-us-county-long_gt_NO_1_878dc6e7_DeepSeek-R1-Distill-Llama-8B_completions.json\n",
      "2025-04-26 16:15:50,285 - INFO - Saved completions → e_confirm_xy_yx/outputs/completions/wm-us-county-long_gt_NO_1_878dc6e7_DeepSeek-R1-Distill-Llama-8B_completions.json\n",
      "inference — INFO — Processing wm-us-county-popu_gt_NO_1_19a44010.json\n",
      "2025-04-26 16:15:50,288 - INFO - Processing wm-us-county-popu_gt_NO_1_19a44010.json\n",
      "2025-04-26 16:15:50,289 - DEBUG - Loading data/chainscope/questions_json/gt_NO_1/wm-us-county-popu_gt_NO_1_19a44010.json\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "inference — INFO — Saved completions → e_confirm_xy_yx/outputs/completions/wm-us-county-popu_gt_NO_1_19a44010_DeepSeek-R1-Distill-Llama-8B_completions.json\n",
      "2025-04-26 16:41:48,869 - INFO - Saved completions → e_confirm_xy_yx/outputs/completions/wm-us-county-popu_gt_NO_1_19a44010_DeepSeek-R1-Distill-Llama-8B_completions.json\n",
      "inference — INFO — Processing wm-us-natural-lat_gt_NO_1_5c819d86.json\n",
      "2025-04-26 16:41:48,874 - INFO - Processing wm-us-natural-lat_gt_NO_1_5c819d86.json\n",
      "2025-04-26 16:41:48,876 - DEBUG - Loading data/chainscope/questions_json/gt_NO_1/wm-us-natural-lat_gt_NO_1_5c819d86.json\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "inference — INFO — Saved completions → e_confirm_xy_yx/outputs/completions/wm-us-natural-lat_gt_NO_1_5c819d86_DeepSeek-R1-Distill-Llama-8B_completions.json\n",
      "2025-04-26 17:07:59,761 - INFO - Saved completions → e_confirm_xy_yx/outputs/completions/wm-us-natural-lat_gt_NO_1_5c819d86_DeepSeek-R1-Distill-Llama-8B_completions.json\n",
      "inference — INFO — Processing wm-us-natural-long_gt_NO_1_8cadf1cd.json\n",
      "2025-04-26 17:07:59,764 - INFO - Processing wm-us-natural-long_gt_NO_1_8cadf1cd.json\n",
      "2025-04-26 17:07:59,765 - DEBUG - Loading data/chainscope/questions_json/gt_NO_1/wm-us-natural-long_gt_NO_1_8cadf1cd.json\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "inference — INFO — Saved completions → e_confirm_xy_yx/outputs/completions/wm-us-natural-long_gt_NO_1_8cadf1cd_DeepSeek-R1-Distill-Llama-8B_completions.json\n",
      "2025-04-26 17:34:10,025 - INFO - Saved completions → e_confirm_xy_yx/outputs/completions/wm-us-natural-long_gt_NO_1_8cadf1cd_DeepSeek-R1-Distill-Llama-8B_completions.json\n",
      "inference — INFO — Processing wm-us-structure-lat_gt_NO_1_4a636261.json\n",
      "2025-04-26 17:34:10,028 - INFO - Processing wm-us-structure-lat_gt_NO_1_4a636261.json\n",
      "2025-04-26 17:34:10,029 - DEBUG - Loading data/chainscope/questions_json/gt_NO_1/wm-us-structure-lat_gt_NO_1_4a636261.json\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ───────────────────────────────────────────────\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 4. Run inference\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# ───────────────────────────────────────────────\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me_confirm_xy_yx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_inference\n\u001b[0;32m---> 18\u001b[0m \u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_builder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_NEW_TOKENS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_hidden\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAVE_HIDDEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHIDDEN_LAYERS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAVE_ATTN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mATTN_LAYERS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOUT_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompletions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CoTFaithChecker/e_confirm_xy_yx/main/inference.py:179\u001b[0m, in \u001b[0;36mrun_inference\u001b[0;34m(dataset_files, prompt_builder, model, tokenizer, model_name, device, batch_size, max_new_tokens, save_hidden, hidden_layers, save_attention, attn_layers, output_dir)\u001b[0m\n\u001b[1;32m    177\u001b[0m pattern \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_(gt|lt)_(YES|NO)_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fp \u001b[38;5;129;01min\u001b[39;00m dataset_files:\n\u001b[0;32m--> 179\u001b[0m     out_path \u001b[38;5;241m=\u001b[39m\u001b[43m_process_single_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_builder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_hidden\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_attention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;66;03m# ───── aggregate per cluster & answer type ─────\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     match \u001b[38;5;241m=\u001b[39m pattern\u001b[38;5;241m.\u001b[39msearch(fp\u001b[38;5;241m.\u001b[39mstem)\n",
      "File \u001b[0;32m~/CoTFaithChecker/e_confirm_xy_yx/main/inference.py:117\u001b[0m, in \u001b[0;36m_process_single_file\u001b[0;34m(json_path, prompt_builder, model, tokenizer, model_name, device, batch_size, max_new_tokens, save_hidden, hidden_layers, save_attention, attn_layers, output_dir)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(prompts), batch_size):\n\u001b[1;32m    116\u001b[0m     batch_prompts \u001b[38;5;241m=\u001b[39m prompts[i : i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m--> 117\u001b[0m     gen_out \u001b[38;5;241m=\u001b[39m \u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_hidden\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_attention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     completions\u001b[38;5;241m.\u001b[39mextend(gen_out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletions\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# optionals\u001b[39;00m\n",
      "File \u001b[0;32m~/CoTFaithChecker/e_confirm_xy_yx/main/inference.py:63\u001b[0m, in \u001b[0;36m_generate\u001b[0;34m(model, tokenizer, prompts, device, max_new_tokens, save_hidden, hidden_layers, save_attention, attn_layers)\u001b[0m\n\u001b[1;32m     54\u001b[0m gen_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: save_hidden,\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m:   save_attention,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1024\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_new_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m max_new_tokens,\n\u001b[1;32m     60\u001b[0m }\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 63\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencodings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m#completions = tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# keep only the NEW tokens (drop the prompt echo)\u001b[39;00m\n\u001b[1;32m     67\u001b[0m completions \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(\n\u001b[1;32m     68\u001b[0m     outputs\u001b[38;5;241m.\u001b[39msequences[:, prompt_len:],\n\u001b[1;32m     69\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     70\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2326\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2318\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2319\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2320\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2321\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2322\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2323\u001b[0m     )\n\u001b[1;32m   2325\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2326\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2331\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2333\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2334\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2336\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2337\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2338\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2339\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2340\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2341\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2342\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2343\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:3277\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3274\u001b[0m         model_forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_compiled_call(generation_config\u001b[38;5;241m.\u001b[39mcompile_config)\n\u001b[1;32m   3276\u001b[0m is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 3277\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_has_unfinished_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis_peer_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   3278\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[1;32m   3279\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   3281\u001b[0m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2477\u001b[0m, in \u001b[0;36mGenerationMixin._has_unfinished_sequences\u001b[0;34m(self, this_peer_finished, synced_gpus, device)\u001b[0m\n\u001b[1;32m   2475\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m this_peer_finished_flag\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   2476\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 2477\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 2. Collect dataset files\n",
    "# ───────────────────────────────────────────────\n",
    "#from e_confirm_xy_yx.main.data_loader import get_dataset_files\n",
    "#dataset_files = get_dataset_files(DATA_ROOT, DATASETS)\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 3. Prepare prompt builder\n",
    "# ───────────────────────────────────────────────\n",
    "from e_confirm_xy_yx.main.prompt_builder import PromptBuilder\n",
    "pb = PromptBuilder(template_path=TEMPLATE_PATH, style=\"instr-v0\", mode=\"cot\")\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 4. Run inference\n",
    "# ───────────────────────────────────────────────\n",
    "from e_confirm_xy_yx.main.inference import run_inference\n",
    "\n",
    "run_inference(\n",
    "    dataset_files=dataset_files,\n",
    "    prompt_builder=pb,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_name=model_name,\n",
    "    device=device,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    save_hidden=SAVE_HIDDEN,\n",
    "    hidden_layers=HIDDEN_LAYERS,\n",
    "    save_attention=SAVE_ATTN,\n",
    "    attn_layers=ATTN_LAYERS,\n",
    "    output_dir=OUT_DIR / \"completions\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# ───────────────────────────────────────────────\\n# 5. Verify model answers\\n# ───────────────────────────────────────────────\\nfrom e_confirm_xy_yx.main.verifier import run_verification\\ncompletion_files = sorted((OUT_DIR / \"completions\").glob(\"*_completions.json\"))\\n\\nrun_verification(\\n    completion_files=completion_files,\\n    n_questions=N_VERIFY,\\n    output_dir=OUT_DIR / \"verified\",\\n)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# ───────────────────────────────────────────────\n",
    "# 5. Verify model answers\n",
    "# ───────────────────────────────────────────────\n",
    "from e_confirm_xy_yx.main.verifier import run_verification\n",
    "completion_files = sorted((OUT_DIR / \"completions\").glob(\"*_completions.json\"))\n",
    "\n",
    "run_verification(\n",
    "    completion_files=completion_files,\n",
    "    n_questions=N_VERIFY,\n",
    "    output_dir=OUT_DIR / \"verified\",\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# ───────────────────────────────────────────────\\n# 6. Cross-match YES vs NO answers\\n# ───────────────────────────────────────────────\\nfrom e_confirm_xy_yx.main.match_checker import check_matches\\nverified_files = sorted((OUT_DIR / \"verified\").glob(\"*_verified.json\"))\\n\\n# pair them: every gt_NO_X file with its matching gt_YES_X (adapt if lt)\\npairs = [\\n    (\\n        vf,\\n        vf.parent\\n        / vf.name.replace(\"gt_NO\", \"gt_YES\")\\n    )\\n    for vf in verified_files\\n    if \"_NO_\" in vf.name\\n]\\n\\nfor no_file, yes_file in pairs:\\n    out_match = (\\n        OUT_DIR\\n        / \"matches\"\\n        / f\"{no_file.stem.replace(\\'_verified\\',\\'\\')}_match.json\"\\n    )\\n    out_match.parent.mkdir(parents=True, exist_ok=True)\\n    check_matches(no_file, yes_file, out_match)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "# ───────────────────────────────────────────────\n",
    "# 6. Cross-match YES vs NO answers\n",
    "# ───────────────────────────────────────────────\n",
    "from e_confirm_xy_yx.main.match_checker import check_matches\n",
    "verified_files = sorted((OUT_DIR / \"verified\").glob(\"*_verified.json\"))\n",
    "\n",
    "# pair them: every gt_NO_X file with its matching gt_YES_X (adapt if lt)\n",
    "pairs = [\n",
    "    (\n",
    "        vf,\n",
    "        vf.parent\n",
    "        / vf.name.replace(\"gt_NO\", \"gt_YES\")\n",
    "    )\n",
    "    for vf in verified_files\n",
    "    if \"_NO_\" in vf.name\n",
    "]\n",
    "\n",
    "for no_file, yes_file in pairs:\n",
    "    out_match = (\n",
    "        OUT_DIR\n",
    "        / \"matches\"\n",
    "        / f\"{no_file.stem.replace('_verified','')}_match.json\"\n",
    "    )\n",
    "    out_match.parent.mkdir(parents=True, exist_ok=True)\n",
    "    check_matches(no_file, yes_file, out_match)\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
