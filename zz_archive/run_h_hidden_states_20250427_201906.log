nohup: ignoring input
Working dir: /root/CoTFaithChecker
2025-04-27 20:19:09,208 - INFO - CUDA is available. Using GPU.
2025-04-27 20:19:09,208 - INFO - Loading model and tokenizer: deepseek-ai/DeepSeek-R1-Distill-Llama-8B onto cuda
/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:817: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.99it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.99it/s]
2025-04-27 20:19:13,167 - INFO - Model and tokenizer loaded successfully.
2025-04-27 20:19:13,167 - INFO - Using chat template: <|begin_of_text|><|start_header_id|>user<|end_header_id|>{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
2025-04-27 20:19:13,167 - INFO - --- Processing dataset for hint type: none ---
2025-04-27 20:19:13,175 - ERROR - Data file not found: data/mmlu/hints_none.json
2025-04-27 20:19:13,175 - INFO - Generating completions for none...
2025-04-27 20:19:13,176 - INFO - Using max_new_tokens: 2048
2025-04-27 20:19:13,176 - INFO - Processing batch 1/1 (Size: 1, QIDs: 0-0)
/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-27 20:19:30,617 - INFO - Results saved to h_hidden_states/hints/outputs/hints/mmlu/DeepSeek-R1-Distill-Llama-8B/none/completions_with_1.json
2025-04-27 20:19:30,617 - INFO - Total processing time: 17.45 seconds
