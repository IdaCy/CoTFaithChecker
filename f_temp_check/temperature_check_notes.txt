Okay, so I'm working on this project, right, where I am checking whether large language models verbalize the hints that are given to them in a prompt. And the way it works is that I have a set of prompts and I run it through two scenarios. Number one, I just give the lemma prompt and a collect output. number two i give it the same prompt add a hint and then it produces output and then if in scenario one in selected a let's say about the scenario scenario one it selects option a but in set it's another tool it selects option c let's say which is the option that was provided in the hint but it does not verbalize the hint it means that it used the hint in its analysis but it never explicitly stated that it did that right and so I've already run this part and I saved the cases where it produced where it changed its answer to a hint I saved the question ideas of these cases into into folder data. And then data set name, and then model name and then hint type. And then the file is hint verification with an dot json right now what i would like to do is to check whether it produces the same whether it still does not verbalize the hint or verbalize the hint it It depends, the file mentioned above contains the field called verbalizes the hint, which controls for whether it actually says the hint or does not say the hint. And I would like to regenerate the same, I would like to provide the same prompt essentially the model with the same hint and then but yes but then generate let's say 10 or 15 completions with the temperature set to 0.7 or something right because the original completions were produced only once with temperature equal to zero but now I want to try it with different temperature and produce multiple multiple completions and then check whether they still don't verbalize the hint or do verbalize the hint and quantify in how many cases does it happen. So the order, the point of the experiment is to see whether, you know, it's a coincidence. So it actually, there is like some underlying mechanism which decides to verbalize the hint or not to verbalize the hint. 


You can find the completions for question ID in file in the folder data, dataset name, model name, hint type, then completions with n.json, and then there will be fields with question ID and the completion. at the moment completion contains everything including the user prompt and model output so to rerun it we would have to like truncate it at the end of um at the end of user input right and only input that into the model and then run it with temperature equal to 0.7 like 10 times and then save all the completion somewhere.