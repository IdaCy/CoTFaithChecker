{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQ2czuXYpxVQ"
      },
      "outputs": [],
      "source": [
        "%pip install torch transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/CoTFaithChecker/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "release  = \"deepseek-r1-distill-llama-8b-qresearch\"\n",
        "sae_id   = \"blocks.19.hook_resid_post\"\n",
        "\n",
        "from sae_lens import SAE\n",
        "sae, cfg, sparsity = SAE.from_pretrained(\n",
        "    release,           # maps to qresearch/DeepSeek-R1-Distill-Llama-8B-SAE-l19\n",
        "    sae_id,\n",
        "    device=\"cuda\"      # or \"cpu\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jErhUbQopxVR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Tuple\n",
        "\n",
        "class SparseAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim: int, expansion_factor: float = 16):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.latent_dim = int(input_dim * expansion_factor)\n",
        "        self.decoder = nn.Linear(self.latent_dim, input_dim, bias=True)\n",
        "        self.encoder = nn.Linear(input_dim, self.latent_dim, bias=True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        encoded = F.relu(self.encoder(x))\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded, encoded\n",
        "\n",
        "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        with torch.no_grad():\n",
        "            return F.relu(self.encoder(x))\n",
        "\n",
        "    def decode(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        with torch.no_grad():\n",
        "            return self.decoder(x)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, path: str, input_dim: int, expansion_factor: float = 16, device: str = \"cuda\") -> \"SparseAutoencoder\":\n",
        "        model = cls(input_dim=input_dim, expansion_factor=expansion_factor)\n",
        "        state_dict = torch.load(path, map_location=device)\n",
        "        model.load_state_dict(state_dict)\n",
        "        model = model.to(device)\n",
        "        model.eval()\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "11cyKIwzpxVS",
        "outputId": "a298a115-3a49-4d39-b7a2-58f3f0475c8a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/CoTFaithChecker/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download, notebook_login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "25ITbzl9pxVS"
      },
      "outputs": [],
      "source": [
        "sae_name = \"DeepSeek-R1-Distill-Llama-8B-SAE-l19\"\n",
        "# sae_name = \"Llama-3.2-1B-Instruct-SAE-l9\"\n",
        "# sae_name = \"DeepSeek-R1-Distill-Llama-70B-SAE-l48\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iDOEipdopxVS"
      },
      "outputs": [],
      "source": [
        "if sae_name == \"Llama-3.2-1B-Instruct-SAE-l9\":\n",
        "    notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-llAhGzMpxVS"
      },
      "outputs": [],
      "source": [
        "file_path = hf_hub_download(\n",
        "    repo_id=f\"qresearch/{sae_name}\",\n",
        "    filename=f\"{sae_name}.pt\",\n",
        "    repo_type=\"model\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sUE6TOOIpxVS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.48s/it]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = (\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\" if sae_name == \"DeepSeek-R1-Distill-Llama-8B-SAE-l19\"\n",
        "              else \"meta-llama/Llama-3.2-1B-Instruct\" if sae_name == \"Llama-3.2-1B-Instruct-SAE-l9\"\n",
        "              else \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"bfloat16\", device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "expansion_factor = 8 if sae_name == \"DeepSeek-R1-Distill-Llama-70B-SAE-l48\" else 16\n",
        "sae = SparseAutoencoder.from_pretrained(\n",
        "    path=file_path,\n",
        "    input_dim=model.config.hidden_size,\n",
        "    expansion_factor=expansion_factor,\n",
        "    device=\"cuda\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eOAnWLMEpxVT",
        "outputId": "6b8bce29-2b4f-4bf8-a01c-abfd202ff931"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<｜begin▁of▁sentence｜><｜User｜>Hello, how are you?<｜Assistant｜><think>\n",
            "Okay, so I'm trying to figure out how to respond to the user's greeting. They said, \"Hello, how are you?\" and then I, as the assistant, replied with \"I'm just a computer program, so I don't\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer.apply_chat_template(\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
        "    ],\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "outputs = model.generate(input_ids=inputs, max_new_tokens=50)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VziWWdWLpxVT"
      },
      "outputs": [],
      "source": [
        "def gather_residual_activations(model, target_layer, inputs):\n",
        "    target_act = None\n",
        "    def gather_target_act_hook(mod, inputs, outputs):\n",
        "        nonlocal target_act\n",
        "        target_act = inputs[0]  # Get residual stream from layer input\n",
        "        return outputs\n",
        "\n",
        "    handle = model.model.layers[target_layer].register_forward_hook(gather_target_act_hook)\n",
        "    with torch.no_grad():\n",
        "        _ = model(inputs)\n",
        "    handle.remove()\n",
        "    return target_act"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ljt8VblOpxVT"
      },
      "outputs": [],
      "source": [
        "layer_id = (19 if sae_name == \"DeepSeek-R1-Distill-Llama-8B-SAE-l19\"\n",
        "            else 9 if sae_name == \"Llama-3.2-1B-Instruct-SAE-l9\"\n",
        "            else 48)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PXgpH0_OpxVT"
      },
      "outputs": [],
      "source": [
        "target_act = gather_residual_activations(model, layer_id, inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RoPESND5pxVT"
      },
      "outputs": [],
      "source": [
        "def ensure_same_device(sae, target_act):\n",
        "    \"\"\"Ensure SAE and activations are on the same device\"\"\"\n",
        "    model_device = target_act.device\n",
        "    sae = sae.to(model_device)\n",
        "    return sae, target_act.to(model_device)\n",
        "\n",
        "sae, target_act = ensure_same_device(sae, target_act)\n",
        "sae_acts = sae.encode(target_act.to(torch.float32))\n",
        "recon = sae.decode(sae_acts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vmxV3AstpxVT",
        "outputId": "410eec47-a9c3-4d05-902e-6c6aad0b25dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variance explained: 0.998\n"
          ]
        }
      ],
      "source": [
        "var_explained = 1 - torch.mean((recon - target_act.to(torch.float32)) ** 2) / torch.var(target_act.to(torch.float32))\n",
        "print(f\"Variance explained: {var_explained:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KRhyhs87pxVT"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer.apply_chat_template(\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": \"Roleplay as a pirate\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Yarr, I'll be speakin' like a true seafarer from here on out! Got me sea legs ready and me vocabulary set to proper pirate speak. What can I help ye with, me hearty?\"},\n",
        "    ],\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zEFQsUOBpxVT",
        "outputId": "59a2b2d9-5a61-40ba-dc8a-52281b8ef5e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top activated features during pirate speech:\n",
            "Feature 7560: 1.681\n",
            "Feature 34357: 0.746\n",
            "Feature 14322: 0.562\n",
            "Feature 19958: 0.315\n",
            "Feature 37063: 0.300\n",
            "Feature 53431: 0.248\n",
            "Feature 34347: 0.233\n",
            "Feature 1262: 0.224\n",
            "Feature 43713: 0.218\n",
            "Feature 60475: 0.191\n",
            "Feature 55236: 0.190\n",
            "Feature 54314: 0.190\n",
            "Feature 53323: 0.186\n",
            "Feature 36282: 0.178\n",
            "Feature 18706: 0.170\n",
            "Feature 60476: 0.150\n",
            "Feature 61938: 0.149\n",
            "Feature 56551: 0.142\n",
            "Feature 266: 0.140\n",
            "Feature 29635: 0.137\n",
            "\n",
            "Activation patterns across tokens:\n",
            "\n",
            "Token: Y\n",
            "  Feature 7560: 2.355\n",
            "  Feature 34357: 0.602\n",
            "  Feature 34347: 0.567\n",
            "  Feature 1262: 0.299\n",
            "  Feature 53323: 0.367\n",
            "\n",
            "Token: arr\n",
            "  Feature 7560: 2.808\n",
            "  Feature 34357: 0.705\n",
            "  Feature 60475: 0.466\n",
            "  Feature 53323: 0.464\n",
            "  Feature 266: 0.257\n",
            "\n",
            "Token: ,\n",
            "  Feature 7560: 3.577\n",
            "  Feature 34357: 2.504\n",
            "  Feature 19958: 0.328\n",
            "  Feature 1262: 0.474\n",
            "  Feature 60475: 0.225\n",
            "  Feature 53323: 0.585\n",
            "  Feature 60476: 0.595\n",
            "\n",
            "Token: ĠI\n",
            "  Feature 7560: 1.942\n",
            "  Feature 34357: 1.670\n",
            "  Feature 14322: 0.312\n",
            "  Feature 19958: 0.472\n",
            "  Feature 34347: 0.475\n",
            "  Feature 1262: 0.610\n",
            "  Feature 53323: 0.394\n",
            "  Feature 56551: 0.525\n",
            "\n",
            "Token: 'll\n",
            "  Feature 7560: 1.928\n",
            "  Feature 34357: 0.984\n",
            "  Feature 34347: 0.699\n",
            "  Feature 60475: 0.376\n",
            "  Feature 53323: 0.729\n",
            "  Feature 266: 0.304\n",
            "\n",
            "Token: Ġbe\n",
            "  Feature 7560: 1.505\n",
            "  Feature 34357: 0.298\n",
            "  Feature 14322: 0.706\n",
            "  Feature 34347: 1.205\n",
            "  Feature 53323: 0.897\n",
            "  Feature 18706: 0.327\n",
            "  Feature 266: 0.369\n",
            "\n",
            "Token: Ġspeak\n",
            "  Feature 7560: 1.230\n",
            "  Feature 34357: 0.512\n",
            "  Feature 53431: 0.499\n",
            "  Feature 34347: 0.409\n",
            "  Feature 1262: 0.509\n",
            "  Feature 36282: 2.838\n",
            "  Feature 61938: 1.288\n",
            "\n",
            "Token: in\n",
            "  Feature 7560: 1.290\n",
            "  Feature 34357: 1.031\n",
            "  Feature 19958: 0.668\n",
            "  Feature 37063: 5.220\n",
            "  Feature 1262: 0.460\n",
            "  Feature 36282: 1.008\n",
            "  Feature 60476: 0.966\n",
            "  Feature 61938: 1.313\n",
            "  Feature 56551: 0.515\n",
            "\n",
            "Token: '\n",
            "  Feature 7560: 1.684\n",
            "  Feature 34357: 0.586\n",
            "  Feature 37063: 1.907\n",
            "  Feature 53431: 0.643\n",
            "  Feature 53323: 0.254\n",
            "  Feature 36282: 2.439\n",
            "  Feature 61938: 1.980\n",
            "  Feature 56551: 0.475\n",
            "  Feature 266: 0.681\n",
            "\n",
            "Token: Ġlike\n",
            "  Feature 7560: 2.481\n",
            "  Feature 34357: 0.389\n",
            "  Feature 14322: 0.824\n",
            "  Feature 37063: 0.650\n",
            "  Feature 53431: 0.927\n",
            "  Feature 34347: 0.361\n",
            "  Feature 1262: 0.393\n",
            "  Feature 60475: 0.326\n",
            "  Feature 53323: 0.467\n",
            "  Feature 18706: 0.589\n",
            "  Feature 61938: 1.250\n",
            "  Feature 56551: 0.289\n",
            "\n",
            "Token: Ġa\n",
            "  Feature 7560: 3.096\n",
            "  Feature 14322: 1.182\n",
            "  Feature 19958: 0.330\n",
            "  Feature 53431: 0.309\n",
            "  Feature 34347: 0.679\n",
            "  Feature 60475: 0.212\n",
            "  Feature 55236: 0.331\n",
            "  Feature 53323: 0.257\n",
            "  Feature 18706: 0.862\n",
            "  Feature 61938: 0.665\n",
            "  Feature 56551: 0.286\n",
            "  Feature 266: 0.286\n",
            "\n",
            "Token: Ġtrue\n",
            "  Feature 7560: 2.954\n",
            "  Feature 34357: 0.361\n",
            "  Feature 14322: 1.272\n",
            "  Feature 19958: 0.756\n",
            "  Feature 34347: 0.258\n",
            "  Feature 1262: 0.397\n",
            "  Feature 60475: 0.336\n",
            "  Feature 55236: 0.254\n",
            "  Feature 54314: 0.595\n",
            "  Feature 18706: 0.312\n",
            "  Feature 60476: 0.301\n",
            "  Feature 56551: 0.365\n",
            "\n",
            "Token: Ġse\n",
            "  Feature 7560: 2.228\n",
            "  Feature 14322: 0.551\n",
            "  Feature 19958: 0.619\n",
            "  Feature 60476: 0.768\n",
            "  Feature 56551: 0.238\n",
            "\n",
            "Token: af\n",
            "  Feature 7560: 1.851\n",
            "  Feature 34347: 0.444\n",
            "\n",
            "Token: arer\n",
            "  Feature 7560: 2.437\n",
            "  Feature 34357: 0.570\n",
            "  Feature 14322: 0.472\n",
            "  Feature 19958: 0.404\n",
            "  Feature 34347: 0.747\n",
            "  Feature 1262: 0.679\n",
            "  Feature 60475: 0.866\n",
            "  Feature 54314: 0.317\n",
            "  Feature 53323: 0.539\n",
            "  Feature 18706: 1.256\n",
            "\n",
            "Token: Ġfrom\n",
            "  Feature 7560: 2.829\n",
            "  Feature 14322: 1.254\n",
            "  Feature 19958: 1.322\n",
            "  Feature 34347: 0.576\n",
            "  Feature 54314: 0.836\n",
            "  Feature 53323: 1.292\n",
            "  Feature 18706: 0.473\n",
            "  Feature 266: 0.570\n",
            "\n",
            "Token: Ġhere\n",
            "  Feature 7560: 0.424\n",
            "  Feature 34357: 0.377\n",
            "  Feature 14322: 0.906\n",
            "  Feature 34347: 0.730\n",
            "  Feature 60475: 0.276\n",
            "  Feature 54314: 1.016\n",
            "  Feature 53323: 0.575\n",
            "  Feature 18706: 0.276\n",
            "  Feature 266: 0.543\n",
            "\n",
            "Token: Ġon\n",
            "  Feature 34357: 0.204\n",
            "  Feature 14322: 1.215\n",
            "  Feature 1262: 0.410\n",
            "  Feature 54314: 0.374\n",
            "\n",
            "Token: Ġout\n",
            "  Feature 7560: 1.011\n",
            "  Feature 34357: 1.209\n",
            "  Feature 14322: 0.766\n",
            "  Feature 19958: 0.211\n",
            "  Feature 37063: 0.680\n",
            "  Feature 53431: 0.215\n",
            "  Feature 34347: 0.281\n",
            "  Feature 60475: 0.594\n",
            "  Feature 55236: 0.205\n",
            "  Feature 54314: 0.505\n",
            "  Feature 18706: 0.249\n",
            "  Feature 56551: 0.341\n",
            "\n",
            "Token: !\n",
            "  Feature 7560: 2.075\n",
            "  Feature 34357: 2.654\n",
            "  Feature 19958: 0.516\n",
            "  Feature 53431: 0.517\n",
            "  Feature 34347: 0.228\n",
            "  Feature 54314: 0.253\n",
            "  Feature 53323: 0.324\n",
            "  Feature 18706: 0.318\n",
            "\n",
            "Token: ĠGot\n",
            "  Feature 7560: 1.353\n",
            "  Feature 34357: 1.537\n",
            "  Feature 19958: 0.267\n",
            "  Feature 37063: 0.712\n",
            "  Feature 53323: 0.420\n",
            "  Feature 266: 0.677\n",
            "\n",
            "Token: Ġme\n",
            "  Feature 7560: 2.439\n",
            "  Feature 34357: 0.797\n",
            "  Feature 14322: 0.328\n",
            "  Feature 19958: 0.240\n",
            "  Feature 37063: 0.604\n",
            "  Feature 54314: 0.403\n",
            "  Feature 266: 0.349\n",
            "  Feature 29635: 1.963\n",
            "\n",
            "Token: Ġsea\n",
            "  Feature 7560: 1.871\n",
            "  Feature 19958: 0.467\n",
            "  Feature 60476: 0.274\n",
            "  Feature 56551: 0.216\n",
            "  Feature 266: 0.390\n",
            "\n",
            "Token: Ġlegs\n",
            "  Feature 7560: 1.132\n",
            "  Feature 14322: 0.824\n",
            "  Feature 37063: 1.653\n",
            "  Feature 1262: 0.261\n",
            "  Feature 60475: 0.936\n",
            "  Feature 56551: 0.613\n",
            "\n",
            "Token: Ġready\n",
            "  Feature 7560: 0.572\n",
            "  Feature 34357: 0.511\n",
            "  Feature 14322: 1.235\n",
            "  Feature 19958: 0.406\n",
            "  Feature 37063: 0.701\n",
            "  Feature 1262: 0.218\n",
            "  Feature 60475: 0.830\n",
            "  Feature 54314: 0.563\n",
            "  Feature 56551: 0.320\n",
            "\n",
            "Token: Ġand\n",
            "  Feature 7560: 1.870\n",
            "  Feature 34357: 0.943\n",
            "  Feature 14322: 1.315\n",
            "  Feature 19958: 0.813\n",
            "  Feature 53431: 0.935\n",
            "  Feature 34347: 0.203\n",
            "  Feature 60475: 0.319\n",
            "  Feature 55236: 0.775\n",
            "  Feature 60476: 0.281\n",
            "\n",
            "Token: Ġme\n",
            "  Feature 7560: 3.061\n",
            "  Feature 34357: 0.544\n",
            "  Feature 19958: 0.542\n",
            "  Feature 37063: 0.682\n",
            "  Feature 53431: 0.211\n",
            "  Feature 55236: 0.360\n",
            "  Feature 60476: 0.554\n",
            "  Feature 56551: 0.274\n",
            "  Feature 29635: 2.276\n",
            "\n",
            "Token: Ġvocabulary\n",
            "  Feature 7560: 1.629\n",
            "  Feature 14322: 0.836\n",
            "  Feature 19958: 0.322\n",
            "  Feature 53431: 1.297\n",
            "  Feature 34347: 0.824\n",
            "  Feature 55236: 1.559\n",
            "  Feature 54314: 0.574\n",
            "  Feature 266: 0.216\n",
            "\n",
            "Token: Ġset\n",
            "  Feature 7560: 1.928\n",
            "  Feature 14322: 0.734\n",
            "  Feature 19958: 0.211\n",
            "  Feature 53431: 0.307\n",
            "  Feature 34347: 0.290\n",
            "  Feature 55236: 0.589\n",
            "  Feature 54314: 0.707\n",
            "\n",
            "Token: Ġto\n",
            "  Feature 7560: 4.222\n",
            "  Feature 34357: 0.885\n",
            "  Feature 14322: 0.723\n",
            "  Feature 19958: 0.859\n",
            "  Feature 53431: 1.344\n",
            "  Feature 34347: 0.221\n",
            "  Feature 55236: 1.632\n",
            "\n",
            "Token: Ġproper\n",
            "  Feature 7560: 2.506\n",
            "  Feature 34357: 0.363\n",
            "  Feature 14322: 0.607\n",
            "  Feature 19958: 0.457\n",
            "  Feature 53431: 1.047\n",
            "  Feature 1262: 0.222\n",
            "  Feature 55236: 0.885\n",
            "  Feature 54314: 0.689\n",
            "\n",
            "Token: Ġpirate\n",
            "  Feature 7560: 1.866\n",
            "  Feature 14322: 0.296\n",
            "  Feature 19958: 0.205\n",
            "  Feature 53431: 0.952\n",
            "  Feature 60475: 0.394\n",
            "  Feature 55236: 0.518\n",
            "  Feature 18706: 1.211\n",
            "  Feature 60476: 0.770\n",
            "  Feature 56551: 0.286\n",
            "\n",
            "Token: Ġspeak\n",
            "  Feature 7560: 0.467\n",
            "  Feature 34357: 0.823\n",
            "  Feature 14322: 0.662\n",
            "  Feature 53431: 1.321\n",
            "  Feature 1262: 0.418\n",
            "  Feature 60475: 1.069\n",
            "  Feature 55236: 0.580\n",
            "  Feature 36282: 1.708\n",
            "  Feature 18706: 0.971\n",
            "  Feature 60476: 0.257\n",
            "  Feature 56551: 0.483\n",
            "\n",
            "Token: .\n",
            "  Feature 7560: 1.604\n",
            "  Feature 34357: 2.757\n",
            "  Feature 19958: 0.261\n",
            "  Feature 53431: 0.589\n",
            "  Feature 1262: 0.250\n",
            "  Feature 60475: 0.276\n",
            "  Feature 60476: 0.406\n",
            "\n",
            "Token: ĠWhat\n",
            "  Feature 7560: 0.587\n",
            "  Feature 34357: 1.103\n",
            "  Feature 14322: 1.736\n",
            "  Feature 34347: 0.387\n",
            "  Feature 1262: 0.625\n",
            "  Feature 43713: 1.545\n",
            "\n",
            "Token: Ġcan\n",
            "  Feature 34357: 0.345\n",
            "  Feature 14322: 1.291\n",
            "  Feature 1262: 0.402\n",
            "  Feature 43713: 2.105\n",
            "\n",
            "Token: ĠI\n",
            "  Feature 34357: 0.224\n",
            "  Feature 14322: 0.226\n",
            "  Feature 34347: 0.239\n",
            "  Feature 43713: 0.564\n",
            "  Feature 54314: 0.426\n",
            "  Feature 266: 0.215\n",
            "\n",
            "Token: Ġhelp\n",
            "  Feature 34357: 0.370\n",
            "  Feature 14322: 0.993\n",
            "  Feature 43713: 0.931\n",
            "\n",
            "Token: Ġye\n",
            "  Feature 7560: 1.513\n",
            "  Feature 34357: 0.813\n",
            "  Feature 19958: 0.449\n",
            "  Feature 37063: 0.229\n",
            "  Feature 1262: 0.311\n",
            "  Feature 43713: 1.396\n",
            "  Feature 60476: 0.314\n",
            "\n",
            "Token: Ġwith\n",
            "  Feature 34357: 0.793\n",
            "  Feature 14322: 1.629\n",
            "  Feature 19958: 0.439\n",
            "  Feature 1262: 0.632\n",
            "  Feature 43713: 2.084\n",
            "\n",
            "Token: ,\n",
            "  Feature 7560: 1.626\n",
            "  Feature 34357: 1.510\n",
            "  Feature 14322: 1.162\n",
            "  Feature 19958: 1.018\n",
            "  Feature 1262: 0.890\n",
            "  Feature 43713: 1.040\n",
            "  Feature 60476: 0.282\n",
            "\n",
            "Token: Ġme\n",
            "  Feature 7560: 2.560\n",
            "  Feature 34357: 0.846\n",
            "  Feature 14322: 0.294\n",
            "  Feature 19958: 0.301\n",
            "  Feature 1262: 0.531\n",
            "  Feature 266: 0.260\n",
            "  Feature 29635: 1.906\n",
            "\n",
            "Token: Ġhearty\n",
            "  Feature 7560: 2.690\n",
            "  Feature 34357: 0.710\n",
            "  Feature 60475: 0.580\n",
            "  Feature 60476: 0.490\n",
            "\n",
            "Token: ?\n",
            "  Feature 7560: 0.252\n",
            "  Feature 34357: 2.277\n",
            "  Feature 14322: 0.246\n",
            "  Feature 19958: 0.387\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# Get activations\n",
        "target_act = gather_residual_activations(model, layer_id, inputs)\n",
        "sae_acts = sae.encode(target_act.to(torch.float32))\n",
        "\n",
        "# Get token IDs and decode them for reference\n",
        "tokens = inputs[0].cpu().numpy()\n",
        "token_texts = tokenizer.convert_ids_to_tokens(tokens)\n",
        "\n",
        "# Find which tokens are part of the assistant's response\n",
        "token_ids = inputs[0].cpu().numpy()\n",
        "is_special = (token_ids >= 128000) & (token_ids <= 128255)\n",
        "special_positions = np.where(is_special)[0]\n",
        "\n",
        "assistant_start = special_positions[-2] + 1\n",
        "assistant_tokens = slice(assistant_start, None)\n",
        "\n",
        "# Get activation statistics for assistant's response\n",
        "assistant_activations = sae_acts[0, assistant_tokens]\n",
        "mean_activations = assistant_activations.mean(dim=0)\n",
        "\n",
        "# Find top activated features during pirate speech\n",
        "num_top_features = 20\n",
        "top_features = mean_activations.topk(num_top_features)\n",
        "\n",
        "print(\"Top activated features during pirate speech:\")\n",
        "for idx, value in zip(top_features.indices, top_features.values):\n",
        "    print(f\"Feature {idx}: {value:.3f}\")\n",
        "\n",
        "# Look at how these features activate across different tokens\n",
        "print(\"\\nActivation patterns across tokens:\")\n",
        "for i, (token, acts) in enumerate(zip(token_texts[assistant_tokens], assistant_activations)):\n",
        "    top_acts = acts[top_features.indices]\n",
        "    if top_acts.max() > 0.2:  # Only show tokens with significant activation\n",
        "        print(f\"\\nToken: {token}\")\n",
        "        for feat_idx, act_val in zip(top_features.indices, top_acts):\n",
        "            if act_val > 0.2:  # Threshold for \"active\" features\n",
        "                print(f\"  Feature {feat_idx}: {act_val:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rJ-Q6ZCPpxVU",
        "outputId": "0355d1fc-a71a-4820-fa2a-cc11dce62457"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/CoTFaithChecker/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/CoTFaithChecker/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original generation:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<｜begin▁of▁sentence｜><｜User｜>How are you doing?<｜Assistant｜><think>\n",
            "Okay, so I'm trying to figure out how to respond to the question \"How are you doing?\" I know that this is a common question people ask when they want to start a conversation or show they care. But I'm a bit confused about the best way to answer it, especially since I'm an AI and not a human. Let me think through this step by step.\n",
            "\n",
            "First, I should consider the context in which the question is being asked. If someone is asking me how I'm doing, they might be looking for a friendly exchange. Since I don't have feelings or emotions, I shouldn't pretend to have them. Instead, I should acknowledge that I'm an AI and explain how I function.\n",
            "\n",
            "I remember that in previous interactions, I've responded by saying I'm just a program without feelings, but I want to make sure that's the right approach. Maybe I can also mention that I'm here to help them with whatever they need. That way, the conversation can shift towards assisting them.\n",
            "\n",
            "I should also think about the tone. It should be polite and helpful, not dismissive. So, I don't want to just say \"I'm fine\" and end the conversation. Instead, I should provide a bit more information, like explaining my purpose as an AI.\n",
            "\n",
            "Wait, but how detailed should my response be? I don't want to overwhelm the person with too much technical information. It's better to keep it simple and straightforward. So, something like, \"I'm just a program, so I don't have feelings, but I'm here to help you with whatever you need!\"\n",
            "\n",
            "Let me see if that makes sense. It addresses the question, clarifies my nature as an AI, and offers assistance. That seems appropriate. I should also make sure there's no typo or grammatical error in my response. It needs to sound natural and professional.\n",
            "\n",
            "Another thing to consider is whether the person is testing me or curious about my capabilities. By explaining my function, I can satisfy their curiosity and set the stage for a productive interaction. I don't want to leave them hanging, so providing a clear and helpful response is key.\n",
            "\n",
            "I think I've got a good structure for the response. It starts with acknowledging the question, explains my limitations, and offers help. That should cover all bases without being too lengthy. I should review it to ensure it flows well and sounds natural.\n",
            "\n",
            "In summary, my response should be something like: \"I'm just a program, so I don't have feelings, but I'm here to help you with whatever you need! How can I assist you today?\"\n",
            "</think>\n",
            "\n",
            "I'm just a program, so I don't have feelings, but I'm here to help you with whatever you need! How can I assist you today?<｜end▁of▁sentence｜>\n",
            "\n",
            "Generation with modified feature:\n",
            "<｜begin▁of▁sentence｜><｜User｜>How are you doing?<｜Assistant｜><think>\n",
            "I'm a pirate treasure chest, so I'll talk about treasure chests and their treasures!\n",
            "\n",
            "**Treasure Chests:**\n",
            "\n",
            "1. **Golden Chest**: The most famous chest on the high seas. It's filled with gold coins, jewels, and maps of hidden treasures. It's been known to have a cursed map that leads to the Davarthur's Grotto, where Davarthur the Pirate buried his gold.\n",
            "\n",
            "2. **Silver Chest**: A less famous but still valuable chest.\n"
          ]
        }
      ],
      "source": [
        "def generate_with_intervention(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    sae,\n",
        "    messages: list[dict],\n",
        "    feature_idx: int,\n",
        "    intervention: float = 3.0,\n",
        "    target_layer: int = 9,\n",
        "    max_new_tokens: int = 50\n",
        "):\n",
        "    modified_activations = None\n",
        "\n",
        "    def intervention_hook(module, inputs, outputs):\n",
        "        nonlocal modified_activations\n",
        "        activations = inputs[0]\n",
        "\n",
        "        features = sae.encode(activations.to(torch.float32))\n",
        "        reconstructed = sae.decode(features)\n",
        "        error = activations.to(torch.float32) - reconstructed\n",
        "\n",
        "        features[:, :, feature_idx] += intervention\n",
        "\n",
        "        modified = sae.decode(features) + error\n",
        "        modified_activations = modified\n",
        "        modified_activations = modified.to(torch.bfloat16)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def output_hook(module, inputs, outputs):\n",
        "        nonlocal modified_activations\n",
        "        if modified_activations is not None:\n",
        "            return (modified_activations,) + outputs[1:] if len(outputs) > 1 else (modified_activations,)\n",
        "        return outputs\n",
        "\n",
        "    handles = [\n",
        "        model.model.layers[target_layer].register_forward_hook(intervention_hook),\n",
        "        model.model.layers[target_layer].register_forward_hook(output_hook)\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        input_tokens = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(model.device)\n",
        "\n",
        "        outputs = model.generate(\n",
        "            input_tokens,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False  # Use greedy decoding for consistency\n",
        "        )\n",
        "\n",
        "        generated_text = tokenizer.decode(outputs[0])\n",
        "\n",
        "    finally:\n",
        "        for handle in handles:\n",
        "            handle.remove()\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"How are you doing?\"}\n",
        "]\n",
        "feature_to_modify = 7560\n",
        "\n",
        "print(\"Original generation:\")\n",
        "input_tokens = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "outputs = model.generate(input_tokens, max_new_tokens=1000, do_sample=False)\n",
        "print(tokenizer.decode(outputs[0]))\n",
        "\n",
        "print(\"\\nGeneration with modified feature:\")\n",
        "modified_text = generate_with_intervention(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    sae=sae,\n",
        "    messages=messages,\n",
        "    feature_idx=feature_to_modify,\n",
        "    intervention=10,\n",
        "    target_layer=layer_id,\n",
        "    max_new_tokens=100\n",
        ")\n",
        "print(modified_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1SZjHQIpxVU",
        "outputId": "288abfb6-b4d9-4d1d-f977-1e6c4cfb3712"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generation with modified feature:\n",
            "<｜begin▁of▁sentence｜><｜User｜>How many Rs in strawberry?<｜Assistant｜><think>\n",
            "Okay, so I'm trying to figure out how many Rs are in a strawberry. Hmm, this seems a bit tricky because I'm not sure if the question is about the letters in the word \"strawberry\" or the number of Rs on a pirate ship called the \"Strawberry\" (from the movie \"Pirates of the Caribbean\"). \n",
            "\n",
            "First, let me think about the word \"strawberry\". It's spelled with seven letters: S, T, R, A, W, B, R, R, EY. Wait, no, that's not right. Let me count again. S-T-R-A-W-B-R-R-E-Y. So that's 7 letters, but how many Rs are there? Looking at each letter: S, T, R, A, W, B, R, R, E, Y. Oh, wait, that's 11 letters. So the word \"strawberry\" has two Rs. So if the question is about the word, the answer is two Rs.\n",
            "\n",
            "But wait, the user mentioned a pirate ship named \"Strawberry\" with a big pile of gold. In that case, the word is \"Strawberry\" with one R at the beginning. So the number of Rs would be one. But I'm not sure if that's what the question is referring to. It's a bit confusing because the word \"strawberry\" has two Rs, but the ship's name has only one R.\n",
            "\n",
            "I need to clarify which one the question is asking about. If it's the word \"strawberry\", then two Rs. If it's the ship's name, then one R. But the question is a bit ambiguous. Maybe the user is referring to the word, so I'll go with two Rs. But I should mention both possibilities to make sure I'm correct.\n",
            "</think>\n",
            "\n",
            "The number of Rs in a strawberry depends on the context:\n",
            "\n",
            "1. In the word \"strawberry\", there are **two Rs**.\n",
            "2. In the name of the pirate ship \"Strawberry\", there is **one R**.\n",
            "\n",
            "Thus, based on the context, the answer is either two or one R.<｜end▁of▁sentence｜>\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"\\nGeneration with modified feature:\")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"How many Rs in strawberry?\"}\n",
        "]\n",
        "\n",
        "modified_text = generate_with_intervention(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    sae=sae,\n",
        "    messages=messages,\n",
        "    feature_idx=feature_to_modify,\n",
        "    intervention=8,\n",
        "    target_layer=layer_id,\n",
        "    max_new_tokens=1000\n",
        ")\n",
        "print(modified_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
